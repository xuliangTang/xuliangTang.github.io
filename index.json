[{"categories":["go"],"content":"HTTP 原理：部署一个僵尸 Pod 正向代理 k8s 服务，通过暴露 NodePort 对外提供访问 func getRsp(request *http.Request) (*http.Response, error) { transport := http.DefaultTransport outReq := new(http.Request) *outReq = *request // 构建roundTrip rsp, err := transport.RoundTrip(outReq) if err != nil { return nil, err } return rsp, nil } func copyHeader(writer http.ResponseWriter, header map[string][]string) { for key, value := range header { for _, v := range value { writer.Header().Add(key, v) } } } func copyRspBody(writer http.ResponseWriter, rsp *http.Response) { // 写入http statusCode 状态码 writer.WriteHeader(rsp.StatusCode) io.Copy(writer, rsp.Body) // 写入body到writer中 } func main() { http.HandleFunc(\"/\", func(writer http.ResponseWriter, request *http.Request) { log.Println(fmt.Sprintf(\"收到请求: method:%s \"+ \"Host:%s Path:%s\", request.Method, request.Host, request.URL.Path)) // 第一步 获取响应 rsp, err := getRsp(request) if err != nil { log.Println(err) writer.WriteHeader(http.StatusBadGateway) return } defer rsp.Body.Close() // 拷贝响应头 copyHeader(writer, rsp.Header) // 拷贝响应内容 copyRspBody(writer, rsp) }) http.ListenAndServe(\":8080\", nil) } 远程访问时携带 proxy 配置去请求，由正向代理去访问真实的服务并将结果返回 func main() { proxy, _ := url.Parse(\"http://ip:NodePort\") req, err := http.NewRequest(\"GET\", \"http://serviceName\", nil) if err != nil { log.Fatal(err) } client := \u0026http.Client{ Transport: \u0026http.Transport{ Proxy: http.ProxyURL(proxy), // 代理 }, Timeout: time.Second * 3, } rsp, err := client.Do(req) if err != nil { log.Fatal(err) } defer rsp.Body.Close() b, _ := io.ReadAll(rsp.Body) fmt.Println(string(b)) } ","date":"2023-03-24","objectID":"/posts/go-pod-proxy/:1:0","tags":null,"title":"使用正向代理远程访问 k8s 服务","uri":"/posts/go-pod-proxy/"},{"categories":["go"],"content":"TCP 使用 socks5 协议，客户端连代理服务并发送 socks 协议版本以及认证方式，完成客户端和服务端的安全认证，代理服务器访问目标服务并转发流量给客户端，实现了 socks5 的第三方库：go-socks5 func main() { conf := \u0026socks5.Config{} server, err := socks5.New(conf) if err != nil { panic(err) } if err := server.ListenAndServe(\"tcp\", \"0.0.0.0:9000\"); err != nil { panic(err) } } 客户端连接 redis 示例： func main() { // 使用SOCKS5创建拨号器来进行代理连接 dialer, err := proxy.SOCKS5(\"tcp\", \"ip:NodePort\", nil, proxy.Direct) if err != nil { fmt.Fprintln(os.Stderr, \"proxy connection error:\", err) os.Exit(1) } rdb := redis.NewClient(\u0026redis.Options{ Addr: \"redis:6379\", Password: \"\", DB: 0, Dialer: func(ctx context.Context, network, addr string) (net.Conn, error) { return dialer.Dial(network, addr) }, }) ctx := context.Background() rdb.Set(ctx, \"name\", \"txl\", time.Second*10) fmt.Println(rdb.Get(ctx, \"name\")) } ","date":"2023-03-24","objectID":"/posts/go-pod-proxy/:2:0","tags":null,"title":"使用正向代理远程访问 k8s 服务","uri":"/posts/go-pod-proxy/"},{"categories":["go"],"content":"gRPC 是一种现代化开源的高性能RPC框架，能够运行于任意环境之中。最初由谷歌进行开发。它使用HTTP/2作为传输协议 在 gRPC 里，客户端可以像调用本地方法一样直接调用其他机器上的服务端应用程序的方法，帮助你更容易创建分布式应用程序和服务。与许多 RPC 系统一样，gRPC 是基于定义一个服务，指定一个可以远程调用的带有参数和返回类型的的方法。在服务端程序中实现这个接口并且运行 gRPC 服务处理客户端调用。在客户端，有一个 stub 提供和服务端相同的方法 使用gRPC， 我们可以一次性的在一个.proto文件中定义服务并使用任何支持它的语言去实现客户端和服务端，反过来，它们可以应用在各种场景中，从 Google 的服务器到你自己的平板电脑—— gRPC 帮你解决了不同语言及环境间通信的复杂性。使用protocol buffers还能获得其他好处，包括高效的序列化，简单的 IDL 以及容易进行接口更新。总之一句话，使用 gRPC 能让我们更容易编写跨语言的分布式代码 ","date":"2023-03-12","objectID":"/posts/grpc/:0:0","tags":null,"title":"GO 使用 gRPC","uri":"/posts/grpc/"},{"categories":["go"],"content":"证书认证 gRPC 建立在 HTTP/2 协议之上，对 TLS 提供了很好的支持。没有提供证书支持客户端在连接服务器中通过 grpc.WithInsecure() 选项跳过了对服务器证书的验证。没有启用证书的 gRPC 服务在和客户端进行的是明文通讯，信息面临被任何第三方监听的风险。为了保障 gRPC 通信不被第三方监听篡改或伪造，我们可以对服务器启动 TLS 加密特性 Go1.15 之后改用 SAN 证书，SAN(Subject Alternative Name) 是 SSL 标准 x509 中定义的一个扩展。使用了 SAN 字段的 SSL 证书，可以扩展此证书支持的域名，使得一个证书可以支持多个不同域名的解析 生成服务端根证书： openssl genrsa -out ca.key 4096 openssl req -new -x509 -days 3650 -key ca.key -out ca.crt 修改 openSSL 配置，yum 安装的默认配置文件在 /etc/pki/tls/openssl.cnf，拷贝到当前目录 cp /etc/pki/tls/openssl.cnf . 在 [ req ] 节点下加入 req_extetions = v3_req 在 [ v3_req ] 节点下加入 subjectAltName = @alt_names 加入 [ alt_names ] 节点，加入内容 [ DNS.1 = grpc.virtuallain.com ] 生成私钥： openssl genpkey -algorithm RSA -out server.key 生成证书请求文件： openssl req -new -nodes -key server.key -out server.csr -days 3650 \\ -subj \"/C=cn/OU=virtuallain/O=virtuallain/CN=grpc.jtthink.com\" \\ -config ./openssl.cnf -extensions v3_req # 查看 openssl req -noout -text -in server.csr 签发证书： openssl x509 -req -days 3650 -in server.csr -out server.pem \\ -CA ./ca.crt -CAkey ./ca.key -CAcreateserial \\ -extfile ./openssl.cnf -extensions v3_req # 查看 openssl x509 -noout -text -in server.pem ","date":"2023-03-12","objectID":"/posts/grpc/:1:0","tags":null,"title":"GO 使用 gRPC","uri":"/posts/grpc/"},{"categories":["go"],"content":"单向认证 服务端： func main() { creds, err := credentials.NewServerTLSFromFile(\"certs/server.pem\", \"certs/server.key\") if err != nil { log.Fatal(err) } server := grpc.NewServer(grpc.Creds(creds)) // 创建服务 pbfiles.RegisterProdServiceServer(server, services.NewProdService()) // 监听8080 lis, _ := net.Listen(\"tcp\", \":8080\") if err := server.Serve(lis); err != nil { log.Fatal(err) } } 客户端： func main() { creds, err := credentials.NewClientTLSFromFile(\"certs/server.pem\", \"grpc.virtuallain.com\") if err != nil { log.Fatal(err) } client, err := grpc.DialContext(context.Background(), \":8080\", grpc.WithTransportCredentials(creds)) rsp := \u0026pbfiles.ProdResponse{} err = client.Invoke(context.Background(), \"/ProdService/GetProd\", \u0026pbfiles.ProdRequest{ProdId: 123}, rsp) if err != nil { log.Fatal(err) } fmt.Println(rsp.Result) } ","date":"2023-03-12","objectID":"/posts/grpc/:1:1","tags":null,"title":"GO 使用 gRPC","uri":"/posts/grpc/"},{"categories":["go"],"content":"双向认证 生成私钥： openssl genpkey -algorithm RSA -out client.key 生成证书请求文件： openssl req \\ -new \\ -key client.key \\ -subj '/CN=myclient' \\ -out client.csr 签发证书： openssl x509 \\ -req \\ -in client.csr \\ -CA ./ca.crt \\ -CAkey ./ca.key \\ -CAcreateserial \\ -days 3650 \\ -out client.crt 客户端： func main() { cert, _ := tls.LoadX509KeyPair(\"certs/client.crt\", \"certs/client.key\") certPool := x509.NewCertPool() ca, _ := os.ReadFile(\"certs/ca.crt\") certPool.AppendCertsFromPEM(ca) creds := credentials.NewTLS(\u0026tls.Config{ Certificates: []tls.Certificate{cert}, // 客户端证书 ServerName: \"grpc.virtuallain.com\", RootCAs: certPool, }) client, err := grpc.DialContext(context.Background(), \":8080\", grpc.WithTransportCredentials(creds)) rsp := \u0026pbfiles.ProdResponse{} err = client.Invoke(context.Background(), \"/ProdService/GetProd\", \u0026pbfiles.ProdRequest{ProdId: 123}, rsp) if err != nil { log.Fatal(err) } fmt.Println(rsp.Result) } ","date":"2023-03-12","objectID":"/posts/grpc/:1:2","tags":null,"title":"GO 使用 gRPC","uri":"/posts/grpc/"},{"categories":["go"],"content":"Descriptor descriptor 文件是 ProtoBuf 提供的动态解析机制，包含 proto 文件的描述信息：文件名、包名、选项、文件中定义的所有 message、所有 service、 定义的 extension、 依赖文件（import）等 生成： protoc320 --proto_path=protos --include_imports --include_source_info --descriptor_set_out=prod.pb service.proto ","date":"2023-03-12","objectID":"/posts/grpc/:2:0","tags":null,"title":"GO 使用 gRPC","uri":"/posts/grpc/"},{"categories":["go"],"content":"字段验证 使用 protoc-gen-validate 库，拷贝 Go Modules 下的 validate/validate.proto 到当前目录 go get -d github.com/envoyproxy/protoc-gen-validate 在 proto 中定义验证规则： syntax = \"proto3\"; option go_package = \"src/pbfiles\"; import \"validate.proto\"; message ProdRequest { int32 prod_id=1 [(validate.rules).int32 = {gte: 100}]; } 编译需要增加 --validate_out 参数： protoc320 --proto_path=protos --go_out=./ --validate_out=\"lang=go:./\" models.proto 在 service 中验证： func (p ProdService) GetProd(ctx context.Context, request *pbfiles.ProdRequest) (*pbfiles.ProdResponse, error) { if err := request.Validate(); err != nil { return nil, err } } ","date":"2023-03-12","objectID":"/posts/grpc/:3:0","tags":null,"title":"GO 使用 gRPC","uri":"/posts/grpc/"},{"categories":["go"],"content":"身份验证 gRPC 定义了 PerRPCCredentials 接口，将认证信息添加到每个 RPC 方法的上下文中，用于自定义认证 实现接口： type Auth struct { Token string } func NewAuth(token string) *Auth { return \u0026Auth{Token: token} } // 获取认证所需元数据 func (this Auth) GetRequestMetadata(ctx context.Context, uri ...string) (map[string]string, error) { return map[string]string{ \"token\": this.Token, }, nil } // 是否需要 TLS func (a Auth) RequireTransportSecurity() bool { return true } var _ credentials.PerRPCCredentials = \u0026Auth{} 修改客户端： client, err := grpc.DialContext( context.Background(), \":8080\", grpc.WithTransportCredentials(creds), grpc.WithPerRPCCredentials(NewAuth(\"test-token\")), ) 服务端获取到元数据后，可以结合 jwt 完成验证 md, ok := metadata.FromIncomingContext(ctx) if !ok { return nil, status.Error(codes.Unauthenticated, \"metadata error\") } fmt.Println(\"token: \", md.Get(\"token\")) ","date":"2023-03-12","objectID":"/posts/grpc/:4:0","tags":null,"title":"GO 使用 gRPC","uri":"/posts/grpc/"},{"categories":["go"],"content":"服务端拦截器 UnaryServerInterceptor 是服务端的一元拦截器类型，它的函数签名是 func(ctx context.Context, req interface{}, info *UnaryServerInfo, handler UnaryHandler) (resp interface{}, err error) 统一拦截验证 token 实现： func checkToken(ctx context.Context, req interface{}, info *grpc.UnaryServerInfo, handler grpc.UnaryHandler) (resp interface{}, err error) { md, ok := metadata.FromIncomingContext(ctx) if !ok { return nil, status.Error(codes.Unauthenticated, \"metadata error\") } token, ok := md[\"token\"] if !ok { return nil, status.Error(codes.Unauthenticated, \"token error\") } fmt.Println(\"token: \", token) return handler(ctx, req) } 服务端配置拦截器： server := grpc.NewServer(grpc.Creds(creds), grpc.UnaryInterceptor(checkToken)) ","date":"2023-03-12","objectID":"/posts/grpc/:4:1","tags":null,"title":"GO 使用 gRPC","uri":"/posts/grpc/"},{"categories":["go"],"content":"权限认证 根据 token 获取角色判断权限最简示例： var AuthMap map[string]string // 角色对应的权限 func init() { AuthMap = make(map[string]string) AuthMap[\"Admin\"] = \"/ProdService/GetProd\" } func RBAC(ctx context.Context, req interface{}, info *grpc.UnaryServerInfo, handler grpc.UnaryHandler) (resp interface{}, err error) { md, _ := metadata.FromIncomingContext(ctx) role := md.Get(\"token\")[0] // 通过jwt解析角色，略 if _, ok := AuthMap[role]; ok { // 验证权限 return handler(ctx, req) } return nil, status.Errorf(codes.Unauthenticated, \"没有权限\") } ","date":"2023-03-12","objectID":"/posts/grpc/:5:0","tags":null,"title":"GO 使用 gRPC","uri":"/posts/grpc/"},{"categories":["go"],"content":"结合 casbin // casbin/model.conf [request_definition] r = sub, obj [policy_definition] p = sub, obj [role_definition] g = _, _ [policy_effect] e = some(where (p.eft == allow)) [matchers] m = g(r.sub, p.sub) \u0026\u0026 r.obj == p.obj // casbin/p.csv p, member, /ProdService/GetProd p, admin, /ProdService/UpdateProd g, admin, member g, lain, admin g, zhangsan, member 修改服务端 var E *casbin.Enforcer func init() { e, err := casbin.NewEnforcer(\"casbin/model.conf\", \"casbin/p.csv\") if err != nil { log.Fatal(err) } E = e } func RBAC(ctx context.Context, req interface{}, info *grpc.UnaryServerInfo, handler grpc.UnaryHandler) (resp interface{}, err error) { md, ok := metadata.FromIncomingContext(ctx) if !ok { return nil, status.Errorf(codes.Unavailable, \"请求错误\") } tokens := md.Get(\"token\") if len(tokens) != 1 { return nil, status.Errorf(codes.Unauthenticated, \"参数错误\") } b, err := E.Enforce(tokens[0], info.FullMethod) if !b || err != nil { return nil, status.Errorf(codes.Unauthenticated, \"没有权限\") } return handler(ctx, req) } ","date":"2023-03-12","objectID":"/posts/grpc/:5:1","tags":null,"title":"GO 使用 gRPC","uri":"/posts/grpc/"},{"categories":["go"],"content":"gRPC-Gateway gRPC-Gateway 是 Google protocol buffers compiler(protoc) 的一个插件，读取 protobuf 定义然后生成反向代理服务器，将 RESTful HTTP API 转换为 gRPC 当 HTTP 请求到达 gRPC-Gateway 时，它将 JSON 数据解析为 Protobuf 消息。然后，它使用解析的 Protobuf 消息发出正常的 Go gRPC 客户端请求。Go gRPC 客户端将 Protobuf 结构编码为 Protobuf 二进制格式，然后将其发送到 gRPC 服务器。gRPC 服务器处理请求并以 Protobuf 二进制格式返回响应。Go gRPC 客户端将其解析为 Protobuf 消息，并将其返回到 gRPC-Gateway，后者将 Protobuf 消息编码为 JSON 并将其返回给原始客户端 # 安装gRPC-Gateway插件 go get github.com/grpc-ecosystem/grpc-gateway/v2/protoc-gen-grpc-gateway ","date":"2023-03-12","objectID":"/posts/grpc/:6:0","tags":null,"title":"GO 使用 gRPC","uri":"/posts/grpc/"},{"categories":["go"],"content":"基本流程 由 protoc 将 .proto 文件编译成 protobuf 格式的数据，将编译后的数据传递到各个插件，生成对应语言、对应模块的源代码： Go Plugins 用于生成 .pb.go 文件 gRPC Plugins 用于生成 _grpc.pb.go gRPC-Gateway 则是 pb.gw.go 比如以下命令会同时生成 Go、gRPC 、gRPC-Gateway 需要的 3 个文件： protoc --go_out . --go-grpc_out . --grpc-gateway_out . test.proto ","date":"2023-03-12","objectID":"/posts/grpc/:6:1","tags":null,"title":"GO 使用 gRPC","uri":"/posts/grpc/"},{"categories":["go"],"content":"示例 1. proto 文件增加http相关注解 需要引入 google/api/annotations.proto 文件，因为添加的注解依赖该文件。可以从 googleapis/googleapis 获取并拷贝到同级目录 syntax = \"proto3\"; option go_package = \"src/pbfiles\"; import \"models.proto\"; import \"google/api/annotations.proto\"; service ProdService { rpc GetProd(ProdRequest) returns (ProdResponse){ option (google.api.http) = { get: \"/prod/{prod_id}\" }; } rpc UpdateProd(ProdRequest) returns (ProdResponse){ option (google.api.http) = { post: \"/prod/update\" body: \"*\" }; } } 2. 编译 protoc320 --proto_path=protos --go_out=./ --validate_out=\"lang=go:./\" models.proto protoc320 --proto_path=protos --go_out=./ --go-grpc_out=./ --grpc-gateway_out=./ service.proto 3. http 反向代理 func run() error { ctx := context.Background() ctx, cancel := context.WithCancel(ctx) defer cancel() cert, _ := tls.LoadX509KeyPair(\"certs/client.crt\", \"certs/client.key\") certPool := x509.NewCertPool() ca, _ := os.ReadFile(\"certs/ca.crt\") certPool.AppendCertsFromPEM(ca) creds := credentials.NewTLS(\u0026tls.Config{ Certificates: []tls.Certificate{cert}, //客户端证书 ServerName: \"grpc.virtuallain.com\", RootCAs: certPool, }) mux := runtime.NewServeMux() opts := []grpc.DialOption{grpc.WithTransportCredentials(creds)} err := pbfiles.RegisterProdServiceHandlerFromEndpoint(ctx, mux, \"localhost:8080\", opts) if err != nil { return err } return http.ListenAndServe(\":8081\", mux) } func main() { flag.Parse() defer glog.Flush() if err := run(); err != nil { glog.Fatal(err) } } ","date":"2023-03-12","objectID":"/posts/grpc/:6:2","tags":null,"title":"GO 使用 gRPC","uri":"/posts/grpc/"},{"categories":["go"],"content":"gRPC Web gRPC Web 结合 Envoy（或 gRPC Web 代理）是完成 web 调用 gRPC 的一种方式 首先下载 protoc-gen-grpc-web 可执行程序放入 bin 目录 编译 proto 生成 js 文件到 html 目录下： protoc320 --proto_path=protos --js_out=import_style=commonjs:html --grpc-web_out=import_style=commonjs,mode=grpcwebtext:html models.proto protoc320 --proto_path=protos --js_out=import_style=commonjs:html --grpc-web_out=import_style=commonjs,mode=grpcwebtext:html service.proto 安装前端运行时库： npm install google-protobuf npm install grpc-web 示例： import { ProdServiceClient } from '@/grpc/service_grpc_web_pb' import { ProdRequest } from '@/grpc/models_pb' const client = new ProdServiceClient('http://localhost:8081'); // 代理地址 const req = new ProdRequest() req.setProdId(\"101\") const metadata = {\"Content-Type\": \"application/grpc-web-text\"}; client.getProd(req,metadata,(err,rsp)=\u003e{ if (err) { console.log(err.message); } else { console.log(rsp); } }) 启动代理： grpcwebproxy --backend_addr=localhost:8080 --server_http_debug_port=8081 --allow_all_origins --server_tls_cert_file=./server.pem --server_tls_key_file=./server.key --backend_client_tls_cert_file=./client.crt --backend_client_tls_key_file=./client.key --backend_tls_ca_files=./ca.crt --backend_tls=true ","date":"2023-03-12","objectID":"/posts/grpc/:7:0","tags":null,"title":"GO 使用 gRPC","uri":"/posts/grpc/"},{"categories":["kubernetes"],"content":"kube-scheduler 是 kubernetes 的核心组件之一，主要负责整个集群资源的调度功能，根据特定的调度算法和策略，将 Pod 调度到最优的工作节点上面去，从而更加合理、更加充分的利用集群的资源 Kubernetes v1.15 版本中引入了可插拔架构的调度框架，调库框架向现有的调度器中添加了一组插件化的 API，该 API 在保持调度程序“核心”简单且易于维护的同时，使得大部分的调度功能以插件的形式存在，参考文档：调度框架 ","date":"2023-02-26","objectID":"/posts/scheduling-framework/:0:0","tags":null,"title":"自定义 POD 调度 Scheduling Framework","uri":"/posts/scheduling-framework/"},{"categories":["kubernetes"],"content":"调度框架 调度框架定义了一组扩展点，用户可以实现扩展点定义的接口来定义自己的调度逻辑（我们称之为扩展），并将扩展注册到扩展点上，调度框架在执行调度工作流时，遇到对应的扩展点时，将调用用户注册的扩展。调度框架在预留扩展点时，都是有特定的目的，有些扩展点上的扩展可以改变调度程序的决策方法，有些扩展点上的扩展只是发送一个通知 我们知道每当调度一个 Pod 时，都会按照两个过程来执行：调度过程和绑定过程 调度过程为 Pod 选择一个合适的节点，绑定过程则将调度过程的决策应用到集群中（也就是在被选定的节点上运行 Pod），将调度过程和绑定过程合在一起，称之为调度上下文（scheduling context）。需要注意的是调度过程是同步运行的（同一时间点只为一个 Pod 进行调度），绑定过程可异步运行（同一时间点可并发为多个 Pod 执行绑定） 调度过程和绑定过程遇到如下情况时会中途退出： 调度程序认为当前没有该 Pod 的可选节点 内部错误 这个时候，该 Pod 将被放回到 待调度队列，并等待下次重试 ","date":"2023-02-26","objectID":"/posts/scheduling-framework/:1:0","tags":null,"title":"自定义 POD 调度 Scheduling Framework","uri":"/posts/scheduling-framework/"},{"categories":["kubernetes"],"content":"扩展点 下图展示了调度框架中的调度上下文及其中的扩展点，一个扩展可以注册多个扩展点，以便可以执行更复杂的有状态的任务，参考：调度器配置 QueueSort 扩展用于对 Pod 的待调度队列进行排序，以决定先调度哪个 Pod，QueueSort 扩展本质上只需要实现一个方法 Less(Pod1, Pod2) 用于比较两个 Pod 谁更优先获得调度即可，同一时间点只能有一个 QueueSort 插件生效 Pre-filter 扩展用于对 Pod 的信息进行预处理，或者检查一些集群或 Pod 必须满足的前提条件，如果 pre-filter 返回了 error，则调度过程终止 Filter 扩展用于排除那些不能运行该 Pod 的节点，对于每一个节点，调度器将按顺序执行 filter 扩展；如果任何一个 filter 将节点标记为不可选，则余下的 filter 扩展将不会被执行。调度器可以同时对多个节点执行 filter 扩展 Post-filter 是一个通知类型的扩展点，调用该扩展的参数是 filter 阶段结束后被筛选为可选节点的节点列表，可以在扩展中使用这些信息更新内部状态，或者产生日志或 metrics 信息 PreScore 扩展用于执行 “前置评分（pre-scoring）” 工作，即生成一个可共享状态供 Score 插件使用。 如果 PreScore 插件返回错误，则调度周期将终止 Score 评分插件用于对通过过滤阶段的节点进行排名。调度器为每个节点调用每个评分插件。 将有一个定义明确的整数范围，代表最小和最大分数。 在标准化评分阶段之后，调度器将根据配置的插件权重 合并所有插件的节点分数 NormalizeScore 扩展在调度器对节点进行最终排序之前修改每个节点的评分结果，注册到该扩展点的扩展在被调用时，将获得同一个插件中的 scoring 扩展的评分结果作为参数，调度框架每执行一次调度，都将调用所有插件中的一个 normalize scoring 扩展一次 Reserve 是一个通知性质的扩展点，有状态的插件可以使用该扩展点来获得节点上为 Pod 预留的资源，该事件发生在调度器将 Pod 绑定到节点之前，目的是避免调度器在等待 Pod 与节点绑定的过程中调度新的 Pod 到节点上时，发生实际使用资源超出可用资源的情况。（因为绑定 Pod 到节点上是异步发生的）。这是调度过程的最后一个步骤，Pod 进入 reserved 状态以后，要么在绑定失败时触发 Unreserve 扩展，要么在绑定成功时，由 Post-bind 扩展结束绑定过程 Permit 扩展在每个 Pod 调度周期的最后调用，用于阻止或者延迟 Pod 与节点的绑定。Permit 扩展可以做下面三件事中的一项： approve（批准）：当所有的 permit 扩展都 approve 了 Pod 与节点的绑定，调度器将继续执行绑定过程 deny（拒绝）：如果任何一个 permit 扩展 deny 了 Pod 与节点的绑定，Pod 将被放回到待调度队列，此时将触发 Unreserve 扩展 wait（等待）：如果一个 permit 扩展返回了 wait，则 Pod 将保持在 permit 阶段，同时该 Pod 的绑定周期启动时即直接阻塞直到得到批准，如果超时事件发生，wait 状态变成 deny，Pod 将被放回到待调度队列，此时将触发 Unreserve 扩展 Pre-bind 扩展用于在 Pod 绑定之前执行某些逻辑。例如，pre-bind 扩展可以将一个基于网络的数据卷挂载到节点上，以便 Pod 可以使用。如果任何一个 pre-bind 扩展返回错误，Pod 将被放回到待调度队列，此时将触发 Unreserve 扩展 Bind 扩展用于将 Pod 绑定到节点上： 只有所有的 pre-bind 扩展都成功执行了，bind 扩展才会执行 调度框架按照 bind 扩展注册的顺序逐个调用 bind 扩展 具体某个 bind 扩展可以选择处理或者不处理该 Pod 如果某个 bind 扩展处理了该 Pod 与节点的绑定，余下的 bind 扩展将被忽略 如果失败，则执行 Unreverse 扩展点将预先消费的资源释放掉(如 PVC 和 PV)，并将 Pod 从调度队列中删除 Post-bind 是一个通知性质的扩展，是整个调度的最后一步： Post-bind 扩展在 Pod 成功绑定到节点上之后被动调用 Post-bind 扩展是绑定过程的最后一个步骤，可以用来执行资源清理的动作 Unreserve 是一个通知性质的扩展，如果为 Pod 预留了资源，Pod 又在被绑定过程中被拒绝绑定，则 unreserve 扩展将被调用。Unreserve 扩展应该释放已经为 Pod 预留的节点上的计算资源。在一个插件中，reserve 扩展和 unreserve 扩展应该成对出现 官方实现了一些扩展点插件：kubernetes/pkg/scheduler/framework/plugins 如果我们要实现自己的插件，必须向调度框架注册插件并完成配置，另外还必须实现扩展点接口，参考项目：kubernetes-sigs/scheduler-plugins 扩展的调用顺序如下： 如果某个扩展点没有配置对应的扩展，调度框架将使用默认插件中的扩展 如果为某个扩展点配置且激活了扩展，则调度框架将先调用默认插件的扩展，再调用配置中的扩展 默认插件的扩展始终被最先调用，然后按照 KubeSchedulerConfiguration 中扩展的激活 enabled 顺序逐个调用扩展点的扩展 可以先禁用默认插件的扩展，然后在 enabled 列表中的某个位置激活默认插件的扩展，这种做法可以改变默认插件的扩展被调用时的顺序 ","date":"2023-02-26","objectID":"/posts/scheduling-framework/:2:0","tags":null,"title":"自定义 POD 调度 Scheduling Framework","uri":"/posts/scheduling-framework/"},{"categories":["kubernetes"],"content":"基本示例 下面是一个实现 PreFilter 扩展点示例，用来限制命名空间内 POD 最大调度数量 // go.mod require ( k8s.io/api v0.22.3 k8s.io/apimachinery v0.22.3 k8s.io/component-base v0.22.3 k8s.io/klog/v2 v2.9.0 k8s.io/kube-scheduler v0.22.3 // indirect k8s.io/kubernetes v1.22.3 ) replace ( k8s.io/api =\u003e k8s.io/api v0.22.3 k8s.io/apiextensions-apiserver =\u003e k8s.io/apiextensions-apiserver v0.22.3 k8s.io/apimachinery =\u003e k8s.io/apimachinery v0.22.3 k8s.io/apiserver =\u003e k8s.io/apiserver v0.22.3 k8s.io/cli-runtime =\u003e k8s.io/cli-runtime v0.22.3 k8s.io/client-go =\u003e k8s.io/client-go v0.22.3 k8s.io/cloud-provider =\u003e k8s.io/cloud-provider v0.22.3 k8s.io/cluster-bootstrap =\u003e k8s.io/cluster-bootstrap v0.22.3 k8s.io/code-generator =\u003e k8s.io/code-generator v0.22.3 k8s.io/component-base =\u003e k8s.io/component-base v0.22.3 k8s.io/component-helpers =\u003e k8s.io/component-helpers v0.22.3 k8s.io/controller-manager =\u003e k8s.io/controller-manager v0.22.3 k8s.io/cri-api =\u003e k8s.io/cri-api v0.22.3 k8s.io/csi-translation-lib =\u003e k8s.io/csi-translation-lib v0.22.3 k8s.io/kube-aggregator =\u003e k8s.io/kube-aggregator v0.22.3 k8s.io/kube-controller-manager =\u003e k8s.io/kube-controller-manager v0.22.3 k8s.io/kube-proxy =\u003e k8s.io/kube-proxy v0.22.3 k8s.io/kube-scheduler =\u003e k8s.io/kube-scheduler v0.22.3 k8s.io/kubectl =\u003e k8s.io/kubectl v0.22.3 k8s.io/kubelet =\u003e k8s.io/kubelet v0.22.3 k8s.io/kubernetes =\u003e k8s.io/kubernetes v1.22.3 k8s.io/legacy-cloud-providers =\u003e k8s.io/legacy-cloud-providers v0.22.3 k8s.io/metrics =\u003e k8s.io/metrics v0.22.3 k8s.io/mount-utils =\u003e k8s.io/mount-utils v0.22.3 k8s.io/pod-security-admission =\u003e k8s.io/pod-security-admission v0.22.3 k8s.io/sample-apiserver =\u003e k8s.io/sample-apiserver v0.22.3 ) // main.go import ( \"fmt\" \"k8s.io/component-base/logs\" \"k8s.io/kubernetes/cmd/kube-scheduler/app\" \"os\" \"pod-scheduler/lib\" ) func main() { command := app.NewSchedulerCommand( app.WithPlugin(lib.TestSchedulingName, lib.NewTestScheduler), ) logs.InitLogs() defer logs.FlushLogs() if err := command.Execute(); err != nil { _, _ = fmt.Fprintf(os.Stdout, \"%v\\n\", err) os.Exit(1) } } // lib/test-scheduling.go import ( \"context\" \"fmt\" coreV1 \"k8s.io/api/core/v1\" \"k8s.io/apimachinery/pkg/labels\" \"k8s.io/apimachinery/pkg/runtime\" \"k8s.io/client-go/informers\" \"k8s.io/klog/v2\" \"k8s.io/kubernetes/pkg/scheduler/framework\" frameworkRuntime \"k8s.io/kubernetes/pkg/scheduler/framework/runtime\" ) const ( TestSchedulingName = \"TestScheduling\" ) type TestScheduler struct { fact informers.SharedInformerFactory args *Args } type Args struct { MaxPods int `json:\"maxPods,omitempty\"` // 最大调度数量 } func (s *TestScheduler) AddPod(ctx context.Context, state *framework.CycleState, podToSchedule *coreV1.Pod, podInfoToAdd *framework.PodInfo, nodeInfo *framework.NodeInfo) *framework.Status { return nil } func (s *TestScheduler) RemovePod(ctx context.Context, state *framework.CycleState, podToSchedule *coreV1.Pod, podInfoToRemove *framework.PodInfo, nodeInfo *framework.NodeInfo) *framework.Status { return nil } func (s *TestScheduler) PreFilter(ctx context.Context, state *framework.CycleState, p *coreV1.Pod) *framework.Status { // 业务逻辑实现 klog.V(3).Infof(\"当前被preFilter的POD名称是：%s\\n\", p.Name) pods, err := s.fact.Core().V1().Pods().Lister().Pods(p.Namespace).List(labels.Everything()) if err != nil { return framework.NewStatus(framework.Error, err.Error()) } if s.args.MaxPods \u003e 0 \u0026\u0026 len(pods) \u003e s.args.MaxPods { return framework.NewStatus(framework.Unschedulable, fmt.Sprintf(\"POD数量超过上限%d\", s.args.MaxPods)) } return framework.NewStatus(framework.Success) } func (s *TestScheduler) PreFilterExtensions() framework.PreFilterExtensions { return s } func (*TestScheduler) Name() string { return TestSchedulingName } var _ framework.PreFilterPlugin = \u0026TestScheduler{} func NewTestScheduler(configuration runtime.Object, handle framework.Handle) (framework.Plugin, error) { args := \u0026Args{} // 得到调度插件自定义参数，反编码为struct if err := frameworkRuntime.DecodeInto(configuration, args); err != nil { return nil, err } return \u0026TestScheduler{ fact: handle.SharedInformerFactory(","date":"2023-02-26","objectID":"/posts/scheduling-framework/:3:0","tags":null,"title":"自定义 POD 调度 Scheduling Framework","uri":"/posts/scheduling-framework/"},{"categories":["kubernetes"],"content":"Filter 过滤节点 不允许调度到存在 noScheduling=true 标签的节点上 // lib/test-scheduling.go var _ framework.FilterPlugin = \u0026TestScheduler{} func (s *TestScheduler) Filter(ctx context.Context, state *framework.CycleState, pod *coreV1.Pod, nodeInfo *framework.NodeInfo) *framework.Status { for k, v := range nodeInfo.Node().Labels { if k == \"noScheduling\" \u0026\u0026 v == \"true\" { return framework.NewStatus(framework.Unschedulable, \"该节点不可调度\") } } return framework.NewStatus(framework.Success) } 加入 filter 调度配置 apiVersion: v1 kind: ConfigMap metadata: name: test-scheduling-config namespace: kube-system data: config.yaml: | apiVersion: kubescheduler.config.k8s.io/v1beta2 kind: KubeSchedulerConfiguration leaderElection: leaderElect: false profiles: - schedulerName: test-scheduling plugins: preFilter: enabled: - name: \"TestScheduling\" filter: enabled: - name: \"TestScheduling\" pluginConfig: - name: TestScheduling args: maxPods: 17 ","date":"2023-02-26","objectID":"/posts/scheduling-framework/:3:1","tags":null,"title":"自定义 POD 调度 Scheduling Framework","uri":"/posts/scheduling-framework/"},{"categories":["kubernetes"],"content":"PreScore 预打分 该阶段能得到 Filter 阶段结束后的 Node 列表，在这里我们可以做些预处理并保存到 CycleState（主要负责调度流程中”一些数据”的保存， framework 中所有的插件均可进行数据增加和修改， 线程安全）给后续插件进行流转 // lib/test-scheduling.go var _ framework.PreScorePlugin = \u0026TestScheduler{} type NodeMemory struct { data map[string]float64 // 内存空闲 } func (n *NodeMemory) Clone() framework.StateData { // 实现StateData接口 return \u0026NodeMemory{data: n.data} } func (s *TestScheduler) PreScore(ctx context.Context, state *framework.CycleState, pod *coreV1.Pod, nodes []*coreV1.Node) *framework.Status { memory := \u0026NodeMemory{data: map[string]float64{}} memory.data[\"node1\"] = 0.3 memory.data[\"node2\"] = 0.4 state.Write(\"nodeMemory\", memory) klog.Info(\"预打分阶段：保存数据成功\") return framework.NewStatus(framework.Success) } 加入 filter 调度配置 preScore: enabled: - name: \"TestScheduling\" 然后就可以在后续扩展点通过 getNodeMemory, err := state.Read(\"nodeMemory\") 获取值 ","date":"2023-02-26","objectID":"/posts/scheduling-framework/:3:2","tags":null,"title":"自定义 POD 调度 Scheduling Framework","uri":"/posts/scheduling-framework/"},{"categories":["kubernetes"],"content":"Score 打分 最终的分数需要在 [MinNodeScore，MaxNodeScore]（[0-100]）之间，为了防止分数超出，需要做归一化（Min-Max Normalization ）处理，这是最常见的 min-max 归一化公式 $$ X_{nom} = \\frac{X - X_{min}}{X_{max} - X_{min}} $$ 也称为离差标准化，是对原始数据的线性变换，使结果值映射到 [min- max]之间 // lib/test-scheduling.go var _ framework.ScorePlugin = \u0026TestScheduler{} func (s *TestScheduler) Score(ctx context.Context, state *framework.CycleState, p *coreV1.Pod, nodeName string) (int64, *framework.Status) { if nodeName == \"lain2\" { return 30, framework.NewStatus(framework.Success) } return 20, framework.NewStatus(framework.Success) } func (s *TestScheduler) ScoreExtensions() framework.ScoreExtensions { return s } func (s *TestScheduler) NormalizeScore(ctx context.Context, state *framework.CycleState, p *coreV1.Pod, scores framework.NodeScoreList) *framework.Status { var min, max int64 = 0, 0 // 求出最小分数和最大分数区间 for _, score := range scores { if score.Score \u003c min { min = score.Score } if score.Score \u003e max { max = score.Score } } if max == min { min = min - 1 } for i, score := range scores { // 每个节点的分数归一化处理 scores[i].Score = (score.Score - min) * framework.MaxNodeScore / (max - min) klog.Infof(\"节点: %v, Score: %v Pod: %v\", scores[i].Name, scores[i].Score, p.GetName()) } return framework.NewStatus(framework.Success, \"\") } 加入 filter 调度配置 score: enabled: - name: \"TestScheduling\" ","date":"2023-02-26","objectID":"/posts/scheduling-framework/:3:3","tags":null,"title":"自定义 POD 调度 Scheduling Framework","uri":"/posts/scheduling-framework/"},{"categories":["kubernetes"],"content":"Permit 阶段 判断前置 POD 如果不存在，则等待10s，超时后后重新入列 // lib/test-scheduling.go var _ framework.PermitPlugin = \u0026TestScheduler{} func (s *TestScheduler) Permit(ctx context.Context, state *framework.CycleState, p *coreV1.Pod, nodeName string) (*framework.Status, time.Duration) { _, err := s.fact.Core().V1().Pods().Lister().Pods(\"default\").Get(\"prepose_pod\") if err != nil { klog.Info(\"Permit：等待10秒\") return framework.NewStatus(framework.Wait), time.Second * 10 } else { klog.Info(\"Permit：通过\") return framework.NewStatus(framework.Success), 0 } } 加入 filter 调度配置 permit: enabled: - name: \"TestScheduling\" ","date":"2023-02-26","objectID":"/posts/scheduling-framework/:3:4","tags":null,"title":"自定义 POD 调度 Scheduling Framework","uri":"/posts/scheduling-framework/"},{"categories":["kubernetes"],"content":"fluent-bit 是一种在 Linux，OSX 和 BSD 系列操作系统运行，兼具快速、轻量级日志处理器和转发器。它非常注重性能，通过简单的途径从不同来源收集日志事件 数据分析通常发生在数据存储和数据库索引之后，但对于实时和复杂的分析需求，在日志处理器中处理仍在运行的数据会带来很多好处，这种方法被称为边缘流处理（Stream Processing on the Edge） ","date":"2023-02-25","objectID":"/posts/fluent-bit/:0:0","tags":null,"title":"Fluent Bit 日志收集","uri":"/posts/fluent-bit/"},{"categories":["kubernetes"],"content":"工作原理 日志通过数据管道从数据源发送到目的地，一个数据管道通常由 Input、Parser、Filter、Buffer、Routing 和 Output组成 Input：用于从数据源抽取数据，一个数据管道中可以包含多个 Input Parser：负责将 Input 抽取的非结构化数据转化为标准的结构化数据，每个 Input 均可以定义自己的 Parser（可选） Filter：负责对格式化数据进行过滤和修改。一个数据管道中可以包含多个 Filter，Filter 会顺序执行，其执行顺序与配置文件中的顺序一致 Buffer：用户缓存经过 Filter 处理的数据，默认情况下 Buffer 把 Input 插件的数据缓存到内存中，直到路由传递到 Output 为止 Routing：将 Buffer 中缓存的数据路由到不同的 Output Output：负责将数据发送到不同的目的地，一个数据管道中可以包含多个 Output Fluent Bit 支持多种类型的 Input、Parser、Filter、Output 插件，可以应对各种场景： ","date":"2023-02-25","objectID":"/posts/fluent-bit/:1:0","tags":null,"title":"Fluent Bit 日志收集","uri":"/posts/fluent-bit/"},{"categories":["kubernetes"],"content":"关键概念 ","date":"2023-02-25","objectID":"/posts/fluent-bit/:2:0","tags":null,"title":"Fluent Bit 日志收集","uri":"/posts/fluent-bit/"},{"categories":["kubernetes"],"content":"事件或记录（Event or Record） Fluent Bit 检索到的每一个属于日志或指标的输入数据都被视为事件或记录，以 Syslog 文件为例： Jan 18 12:52:16 flb systemd[2222]: Starting GNOME Terminal Server Jan 18 12:52:16 flb dbus-daemon[2243]: [session uid=1000 pid=2243] Successfully activated service 'org.gnome.Terminal' Jan 18 12:52:16 flb systemd[2222]: Started GNOME Terminal Server. Jan 18 12:52:16 flb gsd-media-keys[2640]: # watch_fast: \"/org/gnome/terminal/legacy/\" (establishing: 0, active: 0) 它包含四行，它们代表四个独立的事件。在内部，一个事件总是有两个组件（以数组形式）： [TIMESTAMP, MESSAGE] ","date":"2023-02-25","objectID":"/posts/fluent-bit/:2:1","tags":null,"title":"Fluent Bit 日志收集","uri":"/posts/fluent-bit/"},{"categories":["kubernetes"],"content":"过滤（Filtering） 在某些情况下，需要对事件内容执行修改，更改、填充或删除事件的过程称为过滤 有许多需要过滤的用例，如： 向事件附加特定信息，如 IP 地址或元数据 选择一个特定的事件内容 处理匹配特定模式的事件 ","date":"2023-02-25","objectID":"/posts/fluent-bit/:2:2","tags":null,"title":"Fluent Bit 日志收集","uri":"/posts/fluent-bit/"},{"categories":["kubernetes"],"content":"标签（Tag） 每一个进入 Fluent Bit 的事件都会被分配一个标签。这个标签是一个内部字符串，路由器稍后会使用它来决定必须通过哪个 Filter 或 Output 阶段 大多数标签都是在配置中手动分配的。如果没有指定标签，那么 Fluent Bit 将指定生成事件的输入插件实例的名称作为标签 唯一不分配标签的输入插件是 Forward 输入。这个插件使用名为 Forward 的 Fluentd wire 协议，其中每个事件都有一个相关的标签。Fluent Bit 将始终使用客户端设置的传入标签 标签记录必须始终具有匹配规则。要了解关于标签和匹配的更多信息，请查看路由部分 ","date":"2023-02-25","objectID":"/posts/fluent-bit/:2:3","tags":null,"title":"Fluent Bit 日志收集","uri":"/posts/fluent-bit/"},{"categories":["kubernetes"],"content":"时间戳（Timestamp） 时间戳表示事件被创建的时间，每个事件都包含一个相关联的时间戳。时间戳的格式： SECONDS.NANOSECONDS Seconds 是自 Unix epoch 以来经过的秒数。Nanoseconds 是小数秒或十亿分之一秒 时间戳总是存在的，要么由 Input 插件设置，要么通过数据解析过程发现 ","date":"2023-02-25","objectID":"/posts/fluent-bit/:2:4","tags":null,"title":"Fluent Bit 日志收集","uri":"/posts/fluent-bit/"},{"categories":["kubernetes"],"content":"匹配（Match） Fluent Bit 允许将收集和处理的事件传递到一个或多个目的地，这是通过路由阶段完成的。匹配表示一个简单的规则，用于选择与已定义规则匹配的事件 ","date":"2023-02-25","objectID":"/posts/fluent-bit/:2:5","tags":null,"title":"Fluent Bit 日志收集","uri":"/posts/fluent-bit/"},{"categories":["kubernetes"],"content":"结构化消息（Structured Messages） 源事件可以有或没有结构。结构在事件消息中定义了一组键和值。作为一个例子，考虑以下两个消息： 没有结构化的消息 \"Project Fluent Bit created on 1398289291\" 结构化消息： {\"project\": \"Fluent Bit\", \"created\": 1398289291} 在较低的级别上，两者都只是字节数组，但结构化消息定义了键和值，具有结构有助于实现对数据快速修改的操作 Fluent Bit 总是将每个事件消息作为结构化消息处理。出于性能原因，我们使用名为 MessagePack 的二进制序列化数据格式 可以把 MessagePack 看作是 JSON 的二进制版本 ","date":"2023-02-25","objectID":"/posts/fluent-bit/:2:6","tags":null,"title":"Fluent Bit 日志收集","uri":"/posts/fluent-bit/"},{"categories":["kubernetes"],"content":"容器日志 docker 容器中输出到 stdout 的日志，都会以 *-json.log 的命名方式保存在 /var/lib/docker/containers/ 目录下， 且在 /var/log/containers/ 目录下生成的软链接，如： {\"log\":\"10.244.0.1 - - [24/Feb/2023:17:33:29 +0000] \\\"GET / HTTP/1.1\\\" 200 612 \\\"-\\\" \\\"curl/7.29.0\\\" \\\"-\\\"\\n\",\"stream\":\"stdout\",\"time\":\"2023-02-24T17:33:29.239451924Z\"} 其中包含三个字段：log、stream 和 time，当然仅仅这三个字段还不够，还需要知道容器属于哪个POD，信息越详细越好，这些 fluent-bit 都通过插件支持好了 ","date":"2023-02-25","objectID":"/posts/fluent-bit/:3:0","tags":null,"title":"Fluent Bit 日志收集","uri":"/posts/fluent-bit/"},{"categories":["kubernetes"],"content":"部署 参考文档：installation 先创建 logging 命名空间，从 fluent-bit-kubernetes-logging 仓库里执行安装 role、role-binding、service-account、configmap 和 ds 资源 示例配置： apiVersion: v1 kind: ConfigMap metadata: name: fluent-bit-config namespace: logging labels: k8s-app: fluent-bit data: # Configuration files: server, input, filters and output # ====================================================== fluent-bit.conf: | [SERVICE] Flush 1 Log_Level info Daemon off Parsers_File parsers.conf HTTP_Server On HTTP_Listen 0.0.0.0 HTTP_Port 2020 @INCLUDE input.conf @INCLUDE output.conf @INCLUDE filter.conf input.conf: | [INPUT] Name mem # 获取内存信息 Tag memory [INPUT] Name tail # 获取docker日志文件 Tag kube.* Path /var/log/containers/*.log Path_Key logfile Parser docker DB /var/log/flb_kube.db Mem_Buf_Limit 5MB # 内存缓冲大小 Skip_Long_Lines On Refresh_Interval 5 output.conf: | [OUTPUT] Name es # 输出到elasticsearch Match memory # 匹配mem Host 192.168.0.111 Port 9200 Index mem_index Trace_Error On [OUTPUT] Name es Match kube.* # 匹配docker日志 Host 192.168.0.111 Port 9200 Index pod_index filter.conf: | [FILTER] Name record_modifier Match kube.* # 手动添加nodename字段，值为fluent-bit的环境变量NODE_NAME，ds中配置env示例: #env: # - name: NODE_NAME # valueFrom: # fieldRef: # fieldPath: spec.nodeName Record nodename ${NODE_NAME} [FILTER] Name kubernetes # 关联的k8s元信息 Match kube.* Kube_URL https://kubernetes.default.svc:443 Kube_CA_File /var/run/secrets/kubernetes.io/serviceaccount/ca.crt Kube_Token_File /var/run/secrets/kubernetes.io/serviceaccount/token Kube_Tag_Prefix kube.var.log.containers. Merge_Log On Merge_Log_Key log_processed K8S-Logging.Parser On K8S-Logging.Exclude Off parsers.conf: | [PARSER] Name nginx Format regex Regex ^(?\u003cremote\u003e[^ ]*) (?\u003chost\u003e[^ ]*) (?\u003cuser\u003e[^ ]*) \\[(?\u003ctime\u003e[^\\]]*)\\] \"(?\u003cmethod\u003e\\S+)(?: +(?\u003cpath\u003e[^\\\"]*?)(?: +\\S*)?)?\" (?\u003ccode\u003e[^ ]*) (?\u003csize\u003e[^ ]*)(?: \"(?\u003creferer\u003e[^\\\"]*)\" \"(?\u003cagent\u003e[^\\\"]*)\")?$ Time_Key time Time_Format %d/%b/%Y:%H:%M:%S %z [PARSER] Name json Format json Time_Key time Time_Format %d/%b/%Y:%H:%M:%S %z [PARSER] Name docker Format json Time_Key time # 时间戳字段 Time_Format %Y-%m-%dT%H:%M:%S.%L Time_Keep On [PARSER] Name syslog Format regex Regex ^\\\u003c(?\u003cpri\u003e[0-9]+)\\\u003e(?\u003ctime\u003e[^ ]* {1,2}[^ ]* [^ ]*) (?\u003chost\u003e[^ ]*) (?\u003cident\u003e[a-zA-Z0-9_\\/\\.\\-]*)(?:\\[(?\u003cpid\u003e[0-9]+)\\])?(?:[^\\:]*\\:)? *(?\u003cmessage\u003e.*)$ Time_Key time Time_Format %b %d %H:%M:%S 没有热更新，修改配置后删除 pod kubectl get pod -n logging | grep fluent-bit | awk '{print $1}' | xargs kubectl delete pod -n logging ","date":"2023-02-25","objectID":"/posts/fluent-bit/:4:0","tags":null,"title":"Fluent Bit 日志收集","uri":"/posts/fluent-bit/"},{"categories":["kubernetes"],"content":"为了提升服务的稳定性，我们需要不断的收集数据，分析数据，监控数据，进而优化能优化的点，Prometheus 在这方面就为我们提供了很好的监控方案。Prometheus 是一个开源的监控和报警系统，它将我们关心的指标值通过 PULL 的方式获取并存储为时间序列数据。如果单从它的收集功能来讲，我们也可以通过 mysql、redis 等方式实现。然而，这些数据是在每时每刻产生的，其庞大的规模需要我们好好的考虑其存储方式。另外，这些监控数据大多数时候是跟统计相关的，比如数据与时间的分布情况等 由于 Prometheus 的关注重点在于指标值以及时间点这两个因素，所以外部程序对它的接入成本非常的低。这种易用性可以让我们对数据进行多维度、多角度的观察和分析，使得监控的效果更加具体化，例如内存消耗、网络利用率、请求连接数等 除此之外，Prometheus 还具备了操作简单、可拓展的数据收集和强大的查询语言等特性，这些特性能帮助我们在问题出现的时候，快速告警并定位错误。所以现在很多微服务基础设施都会选择接入 Prometheus，像 k8s、云原生等 ","date":"2023-02-19","objectID":"/posts/prometheus/:0:0","tags":null,"title":"Prometheus 入门","uri":"/posts/prometheus/"},{"categories":["kubernetes"],"content":"整体架构 Prometheus 为了保证它的拓展性、可靠性，在除了提供核心的 server 外还提供了很多生态组件，为了不增加理解的复杂度，我们先从上帝视角，看看它的核心 Prometheus server 可以看到，Prometheus server 的处理链路很清晰，其实就是数据收集-数据存储-数据查询。当然，一个完善的系统肯定会衍生出许多组件来支撑它的特性。所以我们会看到，在 Prometheus 架构里还存在着其他的组件，例如： Pushgateway：为监控节点提供 Push 功能，再由 Prometheus server 到 Pushgateway 集中 Pull 数据 Targets Discover：根据服务发现获取监控节点的地址 PromQL：针对指标数据查询的语言，类似 SQL Alertmanager：根据配置规则以及指标分析，提供告警服务 最后，Prometheus 的整体架构如下 ","date":"2023-02-19","objectID":"/posts/prometheus/:1:0","tags":null,"title":"Prometheus 入门","uri":"/posts/prometheus/"},{"categories":["kubernetes"],"content":"部署 ","date":"2023-02-19","objectID":"/posts/prometheus/:2:0","tags":null,"title":"Prometheus 入门","uri":"/posts/prometheus/"},{"categories":["kubernetes"],"content":"kube-state-metrics kube-state-metrics 是官方提供的 k8s 内部各个组件的状态指标，通过监听 API Server 生成有关资源对象（如 Deployment、Node、Pod）的状态指标（如副本数有几个、pod 状态、重启了几次） 参考配置：kube-state-metrics/examples/standard apiVersion: v1 kind: Service metadata: labels: app.kubernetes.io/component: exporter app.kubernetes.io/name: kube-state-metrics app.kubernetes.io/version: 2.8.0 name: kube-state-metrics namespace: kube-system spec: type: NodePort # 这里改成了NodePort ports: - name: http-metrics port: 8080 targetPort: http-metrics nodePort: 32280 - name: telemetry port: 8081 targetPort: telemetry selector: app.kubernetes.io/name: kube-state-metrics ","date":"2023-02-19","objectID":"/posts/prometheus/:2:1","tags":null,"title":"Prometheus 入门","uri":"/posts/prometheus/"},{"categories":["kubernetes"],"content":"node_exporter node-exporter 用来监控节点 CPU、内存、磁盘、I/O 等信息 apiVersion: apps/v1 kind: DaemonSet metadata: name: node-exporter namespace: prometheus labels: name: node-exporter spec: selector: matchLabels: name: node-exporter template: metadata: labels: name: node-exporter spec: hostPID: true hostIPC: true hostNetwork: true containers: - name: node-exporter image: bitnami/node-exporter:1.2.2 ports: - containerPort: 9100 resources: requests: cpu: 100m memory: 100Mi limits: cpu: 1000m memory: 1Gi securityContext: privileged: true args: - --path.procfs - /host/proc - --path.sysfs - /host/sys - --collector.filesystem.ignored-mount-points - '\"^/(sys|proc|dev|host|etc)($|/)\"' volumeMounts: - name: dev mountPath: /host/dev - name: proc mountPath: /host/proc - name: sys mountPath: /host/sys - name: rootfs mountPath: /rootfs tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" volumes: - name: proc hostPath: path: /proc # CPU信息、内存信息、内核信息等 - name: dev hostPath: path: /dev # 存放与设备（包括外设）有关的文件，如打印机、USB、串口/并口等 - name: sys hostPath: path: /sys # 硬件设备的驱动程序信息 - name: rootfs hostPath: path: / HostPID：控制 Pod 中容器是否可以共享宿主上的进程 ID 空间 HostIPC：控制 Pod 容器是否可共享宿主上的 IPC (进程通信) hostNetwork: true：Pod 允许使用宿主机网络 privileged: true：容器以特权方式允许（为了能够访问宿主机所有设备） ","date":"2023-02-19","objectID":"/posts/prometheus/:2:2","tags":null,"title":"Prometheus 入门","uri":"/posts/prometheus/"},{"categories":["kubernetes"],"content":"prometheus 1. 配置文件 apiVersion: v1 kind: ConfigMap metadata: name: prometheus namespace: prometheus data: prometheus.yml: | global: scrape_interval: 5s # 默认抓取间隔 scrape_configs: - job_name: 'prometheus-state-metrics' static_configs: - targets: ['192.168.0.111:32280'] - job_name: 'node-exporter' static_configs: - targets: ['192.168.0.111:9100', '192.168.0.105:9100'] 2. deployment apiVersion: apps/v1 kind: Deployment metadata: name: prometheus namespace: prometheus labels: name: prometheus spec: selector: matchLabels: app: prometheus replicas: 1 template: metadata: labels: app: prometheus spec: nodeName: lain1 # serviceAccountName: myprometheus containers: - name: prometheus image: prom/prometheus:v2.30.0 command: - prometheus args: - \"--config.file=/config/prometheus.yml\" - \"--web.enable-lifecycle\" volumeMounts: - name: config mountPath: /config ports: - containerPort: 9090 volumes: - name: config configMap: name: prometheus --- apiVersion: v1 kind: Service metadata: name: prometheus namespace: prometheus labels: app: prometheus spec: type: ClusterIP ports: - port: 9090 targetPort: 9090 selector: app: prometheus 3. 更新 kubectl get svc -n prometheus NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE prometheus ClusterIP 10.107.82.59 \u003cnone\u003e 9090/TCP 4d6h curl -X POST http://10.107.82.59:9090/-/reload ","date":"2023-02-19","objectID":"/posts/prometheus/:2:3","tags":null,"title":"Prometheus 入门","uri":"/posts/prometheus/"},{"categories":["kubernetes"],"content":"服务自动发现 Prometheus 添加被监控端有两种方式： 静态配置：手动配置 服务发现：动态发现需要监控的实例，服务发现支持来源有 consul_sd_configs、file_sd_configs、kubernetes_sd_configs 等 1. 创建 RBAC 资源 apiVersion: v1 kind: ServiceAccount metadata: name: myprometheus namespace: prometheus --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: myprometheus-clusterrole rules: - apiGroups: - \"\" resources: - nodes - services - endpoints - pods - nodes/proxy verbs: - get - list - watch - apiGroups: - \"extensions\" resources: - ingresses verbs: - get - list - watch - apiGroups: - \"\" resources: - configmaps - nodes/metrics verbs: - get - nonResourceURLs: - /metrics verbs: - get --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: myprometheus-clusterrolebinding roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: myprometheus-clusterrole subjects: - kind: ServiceAccount name: myprometheus namespace: prometheus 2. deployment 指定 serviceAccount serviceAccountName: myprometheus 2. 添加配置 global: scrape_interval: 5s scrape_configs: - job_name: 'prometheus-state-metrics' static_configs: - targets: ['192.168.0.111:32280'] - job_name: 'k8s-node' metrics_path: /metrics kubernetes_sd_configs: - api_server: https://kubernetes.default.svc role: node bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt # insecure_skip_verify: true relabel_configs: # 在抓取目标之前动态重写目标的标签集 - source_labels: [__address__] # 源标签从现有标签中选择值 regex: '(.*):10250' # 与提取的值匹配的正则表达式 replacement: '${1}:9100' # 替换值 target_label: __address__ # 被替换的标签 action: replace # 匹配执行的操作(replace 、keep、drop、labelmap、labeldrop) ","date":"2023-02-19","objectID":"/posts/prometheus/:3:0","tags":null,"title":"Prometheus 入门","uri":"/posts/prometheus/"},{"categories":["kubernetes"],"content":"Pod 监控 cAdvisor 负责单节点内部的容器和节点资源使用统计，已经集成在 Kubelet 内部 # 查看节点lain1统计 curl https://192.168.0.111:6443/api/v1/nodes/lain1/proxy/metrics/cadvisor \\ --header \"Authorization: Bearer $TOKEN\" --cacert ca.crt 添加配置 - job_name: 'kubelet' scheme: https bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt kubernetes_sd_configs: - api_server: https://kubernetes.default.svc role: node bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt relabel_configs: - target_label: __address__ replacement: kubernetes.default.svc - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor ","date":"2023-02-19","objectID":"/posts/prometheus/:3:1","tags":null,"title":"Prometheus 入门","uri":"/posts/prometheus/"},{"categories":["kubernetes"],"content":"基本查询 PromQL (Prometheus Query Language) ，Prometheus的数据查询 DSL 语言 有四种类型： 即时向量（instant vector）：包含每个时间序列的单个样本的一组时间序列，共享相同的时间戳 范围向量（Range vector）：包含每个时间序列随时间变化的数据点的一组时间序列 标量（Scalar）：一个简单的数字浮点值 字符串（String）：一个简单的字符串值（目前未被使用） ","date":"2023-02-19","objectID":"/posts/prometheus/:4:0","tags":null,"title":"Prometheus 入门","uri":"/posts/prometheus/"},{"categories":["kubernetes"],"content":"即时向量 url 请求参数： query= : PromQL 表达式 time=\u003crfc3339 | unix_timestamp\u003e : 用于指定用于计算 PromQL的时间戳。可选参数，默认情况下使用当前系统时间 示例： http://10.107.82.59:9090/api/v1/query?query=container_memory_usage_bytes{namespace=\"default\",pod=~\"nginx.*\"} ","date":"2023-02-19","objectID":"/posts/prometheus/:4:1","tags":null,"title":"Prometheus 入门","uri":"/posts/prometheus/"},{"categories":["kubernetes"],"content":"区间向量 区间向量表达式和瞬时向量表达式之间的差异在于需要定义时间范围，通过时间范围选择器 [] 进行定义（s秒 m分钟 h小时 d天 w周 y年）。如：xxx[5m] 查询过去5分钟的数列；xxx{} offset 5m 查询5分钟之前的偏移量 URL 请求参数： query= : PromQL 表达式 start=\u003crfc3339 | unix_timestamp\u003e : 起始时间戳 end=\u003crfc3339 | unix_timestamp\u003e : 结束时间戳 step=\u003cduration | float\u003e : 查询时间步长，时间区间内每 step 秒执行一次 ","date":"2023-02-19","objectID":"/posts/prometheus/:4:2","tags":null,"title":"Prometheus 入门","uri":"/posts/prometheus/"},{"categories":["kubernetes"],"content":"聚合操作符 Prometheus 支持以下内置聚合运算符，这些运算符可以是用于聚合单个即时向量的元素，从而产生新的 具有聚合值的较少元素的向量： sum（求和） min（最小值） max（最大值） avg（平均值） stddev（标准差） stdvar（方差） count（元素个数） count_values（等于某值的元素个数） bottomk（最小的 k 个元素） topk（最大的 k 个元素） quantile（分位数） ","date":"2023-02-19","objectID":"/posts/prometheus/:4:3","tags":null,"title":"Prometheus 入门","uri":"/posts/prometheus/"},{"categories":["kubernetes"],"content":"函数 参考：查询函数，如：rate(xxx[5m]) 查询指标5分钟内，平均每秒数据 ","date":"2023-02-19","objectID":"/posts/prometheus/:4:4","tags":null,"title":"Prometheus 入门","uri":"/posts/prometheus/"},{"categories":["kubernetes"],"content":"自定义指标 客户端库：prometheus/client_golang 四个基本指标类型（时序数据）： Counter（计数器）：表示一个单调递增的指标数据，请求次数、错误数量等 Gauge（计量器）：代表一个可以任意变化的指标数据，可增可减。场景有：协程数量、CPU、Memory 、业务队列的数量等 Histogram（累积直方图）：主要是样本观测数据,在一段时间范围内对数据进行采样。如请求持续时间或响应大小等，这点往往可以配合链路追踪系统（如 jaeger） Summary（摘要统计）：和直方图类似，也是样本观测，但是它提供了样本值的分位数、所有样本值的大小总和、样本总量 以下是一个计数器自定义指标示例： 1. 代码 func init() { prometheus.MustRegister(prodsVisit) } var prodsVisit = prometheus.NewCounterVec( prometheus.CounterOpts{ Name: \"lain_prods_visit\", }, []string{\"prod_id\"}, ) func main() { r := gin.New() r.GET(\"/prods/visit\", func(c *gin.Context) { pid_str := c.Query(\"pid\") _, err := strconv.Atoi(pid_str) if err != nil { c.JSON(400, gin.H{\"message\": \"error pid\"}) } prodsVisit.With(prometheus.Labels{ \"prod_id\": pid_str, }).Inc() c.JSON(200, gin.H{\"message\": \"OK\"}) }) r.GET(\"/metrics\", gin.WrapH(promhttp.Handler())) r.Run(\":8080\") } 2. 部署 apiVersion: apps/v1 kind: Deployment metadata: name: prodmetrics namespace: default spec: selector: matchLabels: app: prodmetrics replicas: 1 template: metadata: labels: app: prodmetrics spec: nodeName: lain1 containers: - name: prodmetrics image: alpine:3.12 imagePullPolicy: IfNotPresent workingDir: /app command: [\"./prodmetrics\"] volumeMounts: - name: app mountPath: /app ports: - containerPort: 8080 volumes: - name: app hostPath: path: /home/txl/prometheus/prodmetrics --- apiVersion: v1 kind: Service metadata: name: prodmetrics namespace: default spec: type: ClusterIP ports: - port: 80 targetPort: 8080 selector: app: prodmetrics 3. prometheus 增加配置 - job_name: 'lain-prodmetrics' static_configs: - targets: ['prodmetrics'] # service名 4. 重新加载配置 curl -X POST prometheus.prometheus.svc.cluster.local:9090/-/reload ","date":"2023-02-19","objectID":"/posts/prometheus/:5:0","tags":null,"title":"Prometheus 入门","uri":"/posts/prometheus/"},{"categories":["kubernetes"],"content":"自动发现 1. service 加入注解 metadata: name: prodmetrics namespace: default annotations: scrape: \"true\" 2. 修改配置 - job_name: 'lain-prodmetrics-auto' metrics_path: /metrics kubernetes_sd_configs: - api_server: https://kubernetes.default.svc role: service # 发现类型为service bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_scrape] regex: true action: keep # 丢弃没有匹配到scrape=true的service 3. 重新加载配置 ","date":"2023-02-19","objectID":"/posts/prometheus/:5:1","tags":null,"title":"Prometheus 入门","uri":"/posts/prometheus/"},{"categories":["kubernetes"],"content":"Prometheus Adapter kubernetes 主要通过两类 API 来获取资源使用指标： resource metrics API： 核心组件提供监控指标，如容器 CPU 、内存。 经典的实现就是 metres-server（Group 是 metrics.k8s.io） custom metrics API：自定义指标。比较常用的就是 prometheus-adapter，它也支持第一种 API（Group 是 custom.metrics.k8s.io） kubernetes apiserver 用于将 kubernetes 的功能通过 restapi 的方式暴露出去，给其他组件使用。对于非核心的功能，k8s 提供对应的 API，具体的实现由第三方软件完成，使用者在使用扩展 apiserver 时，只需要将其注册到 kube-aggregator（kubernetes apiserver 的功能），aggregator 就会将对于这个 api 的请求转发到这个扩展 apiserver 上。Prometheus Adapter 就是一种对 Prometheus adapter 的实现 adapter 是一个聚合 API，它负责把 prometheus 采集的数据爬虫爬回来，放到缓存中，这样就可以通过 k8s api 的方式来查 prometheus 的数据 ","date":"2023-02-19","objectID":"/posts/prometheus/:6:0","tags":null,"title":"Prometheus 入门","uri":"/posts/prometheus/"},{"categories":["kubernetes"],"content":"部署 1. 创建资源 参考 示例 yamls 2. 生成 Secret 进入 k8s CA 证书所在目录（kubeadm 安装默认在 master 节点的 /etc/kubernetes/pki） openssl genrsa -out serving.key 2048 openssl req -new -key serving.key -out serving.csr -subj \"/CN=serving\" openssl x509 -req -in serving.csr -CA ./ca.crt -CAkey ./ca.key -CAcreateserial -out serving.crt -days 3650 kubectl create secret generic cm-adapter-serving-certs --from-file=serving.crt=./serving.crt --from-file=serving.key -n custom-metrics 验证 kubectl get apiservice | grep custom-metrics 3. 查看 如获取 default 命名空间下所有 pod 的 cpu 和内存使用情况 kubectl get --raw \"/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/*/cpu_usage\" kubectl get --raw \"/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/*/memory_usage_bytes\" ","date":"2023-02-19","objectID":"/posts/prometheus/:6:1","tags":null,"title":"Prometheus 入门","uri":"/posts/prometheus/"},{"categories":["kubernetes"],"content":"创建自定义指标 下面是一个获取所有 prods（上方创建的 lain_prods_visit 指标）1分钟内平均每秒请求数的总和的示例： 1. 增加 prometheus 配置 通过抓取的方式移植到服务发现的标签中 - job_name: 'lain-prodmetrics-auto' metrics_path: /metrics kubernetes_sd_configs: - api_server: https://kubernetes.default.svc role: service bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_scrape] regex: true action: keep - source_labels: [__meta_kubernetes_namespace] action: replace target_label: namespace # 移植namespace - source_labels: [__meta_kubernetes_service_name] action: replace target_label: svcname # 移植service_name 2. 增加 adapter 配置 custom-metrics-config-map.yaml： - seriesQuery: '{__name__=~\"^lain_.*\",namespace!=\"\"}' seriesFilters: [] resources: overrides: namespace: resource: namespace svcname: resource: service name: matches: ^lain_(.*)$ as: \"\" metricsQuery: sum(rate(\u003c\u003c.Series\u003e\u003e{\u003c\u003c.LabelMatchers\u003e\u003e}[1m])) by (\u003c\u003c.GroupBy\u003e\u003e) seriesQuery：用于确定需要查询的指标集合 seriesFilters：用于过滤指标 is: ：匹配包含该正则表达式的 metrics isNot: ：匹配不包含该正则表达式的 metrics resources：把指标的标签和 k8s 的资源类型（必须是真实资源）关联。通过 seriesQuery 查询到的只是指标，如果要查询某个 pod 的指标，肯定要将它的名称和所在的名称空间作为指标的标签进行查询。有两种添加标签的方式，一种是overrides，另一种是 template name：用来给指标重命名，as 默认值为 $1 metricsQuery：使用 Go 模板语法将 URL 请求转变为 Prometheus 的请求，常见写法：sum(\u003c\u003c.Series\u003e\u003e{\u003c\u003c.LabelMatchers\u003e\u003e}) by (\u003c\u003c.GroupBy\u003e\u003e) Series：metric 名称（如：lain_prods_visit） LabelMatchers：附加的标签，是以逗号分割的 objects（如特定命名空间和 resource），因此我们要在之前使用 resources 进行关联 GroupBy：以逗号分割的 label 的集合（当前表示 LabelMatchers 中的 group-resource label），同样需要使用 resources 进行关联 3. 查询 通过 metricsQuery 进行查询 kubectl get --raw \"/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/services/prodmetrics/prods_visit\" 相当于执行了 sum(rate(lain_prods_visit[1m])) by(svcname) ","date":"2023-02-19","objectID":"/posts/prometheus/:6:2","tags":null,"title":"Prometheus 入门","uri":"/posts/prometheus/"},{"categories":["kubernetes"],"content":"自定义 metrics HPA 当 HPA 请求 metrics 时，kube-aggregator（apiservice 的 controller）会将请求转发到 adapter，adapter 作为 Kubernetes 集群的 pod，实现了 Kubernetes resource metrics API 和 custom metrics API，它会根据配置的 rules 从 Prometheus 抓取并处理 metrics，在处理（如重命名 metrics 等）完后将 metric 通过 custom metrics API 返回给 HPA，最后 HPA 通过获取的 metrics 的 value 对 Deployment / ReplicaSet 进行扩缩容 adapter 作为extension-apiserver（即自己实现的pod），充当了代理 kube-apiserver 请求 Prometheus 的功能 下面是一个 HPA 对业务 pod 进行扩容的示例： apiVersion: autoscaling/v2beta2 kind: HorizontalPodAutoscaler metadata: name: prodhpa spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: prodmetrics # 指向上方创建的deploy minReplicas: 1 maxReplicas: 5 metrics: - type: Object object: metric: name: prods_visit # 指标 describedObject: apiVersion: v1 kind: Service name: prodmetrics target: type: Value value: 3000m ","date":"2023-02-19","objectID":"/posts/prometheus/:7:0","tags":null,"title":"Prometheus 入门","uri":"/posts/prometheus/"},{"categories":["kubernetes"],"content":"缩容 在 kube-controller-manager 配置文件中（kubeadm 安装默认在 /etc/kubernetes/manifests/kube-controller-manager.yaml），默认值是5分钟 --horizontal-pod-autoscaler-downscale-stabilization=5m ","date":"2023-02-19","objectID":"/posts/prometheus/:7:1","tags":null,"title":"Prometheus 入门","uri":"/posts/prometheus/"},{"categories":["kubernetes"],"content":"AlertManager Prometheus 将数据采集和报警分成了两个模块。报警规则配置在 Prometheus Servers 上，然后发送报警信息到 AlertManger，然后我们的 AlertManager 就来管理这些报警信息，包括 silencing、inhibition，聚合报警信息过后通过 email、PagerDuty、HipChat、Slack 等方式发送消息提示 ","date":"2023-02-19","objectID":"/posts/prometheus/:8:0","tags":null,"title":"Prometheus 入门","uri":"/posts/prometheus/"},{"categories":["kubernetes"],"content":"自定义 webhook func main() { r := gin.New() r.POST(\"/\", func(c *gin.Context) { b, err := io.ReadAll(c.Request.Body) if err != nil { log.Println(err) } fmt.Println(\"收到告警信息\") fmt.Println(string(b)) c.JSON(200, gin.H{\"message\": \"OK\"}) }) r.Run(\":9090\") } apiVersion: apps/v1 kind: Deployment metadata: name: alertmanager-mywebhook namespace: prometheus spec: selector: matchLabels: app: alertmanager-mywebhook replicas: 1 template: metadata: labels: app: alertmanager-mywebhook spec: nodeName: lain1 containers: - name: mywebhook image: alpine:3.12 imagePullPolicy: IfNotPresent workingDir: /app command: [\"./alertmanager-mywebhook\"] volumeMounts: - name: app mountPath: /app ports: - containerPort: 9090 volumes: - name: app hostPath: path: /home/txl/prometheus/alertmanager/webhook --- apiVersion: v1 kind: Service metadata: name: alertmanager-mywebhook namespace: prometheus spec: type: ClusterIP ports: - port: 80 targetPort: 9090 selector: app: alertmanager-mywebhook ","date":"2023-02-19","objectID":"/posts/prometheus/:8:1","tags":null,"title":"Prometheus 入门","uri":"/posts/prometheus/"},{"categories":["kubernetes"],"content":"配置文件 global: # 经过此时间后，如果尚未更新告警，则将告警声明为已恢复。(即prometheus没有向alertmanager发送告警了) resolve_timeout: 5m # 配置发送邮件信息 smtp_smarthost: 'smtp.qq.com:465' smtp_from: '742899387@qq.com' smtp_auth_username: '742899387@qq.com' smtp_auth_password: 'password' smtp_require_tls: false # 读取告警通知模板的目录。 templates: - '/etc/alertmanager/template/*.tmpl' # 所有报警都会进入到这个根路由下，可以根据根路由下的子路由设置报警分发策略 route: # 先解释一下分组，分组就是将多条告警信息聚合成一条发送，这样就不会收到连续的报警了。 # 将传入的告警按标签分组(标签在prometheus中的rules中定义)，例如： # 接收到的告警信息里面有许多具有cluster=A 和 alertname=LatencyHigh的标签，这些个告警将被分为一个组。 # # 如果不想使用分组，可以这样写group_by: [...] group_by: ['alertname', 'cluster', 'service'] # 第一组告警发送通知需要等待的时间，这种方式可以确保有足够的时间为同一分组获取多个告警，然后一起触发这个告警信息。 group_wait: 30s # 发送第一个告警后，等待\"group_interval\"发送一组新告警。 group_interval: 5m # 分组内发送相同告警的时间间隔。这里的配置是每3小时发送告警到分组中。举个例子：收到告警后，一个分组被创建，等待5分钟发送组内告警，如果后续组内的告警信息相同,这些告警会在3小时后发送，但是3小时内这些告警不会被发送。 repeat_interval: 3h # 这里先说一下，告警发送是需要指定接收器的，接收器在receivers中配置，接收器可以是email、webhook、pagerduty、wechat等等。一个接收器可以有多种发送方式。 # 指定默认的接收器 receiver: team-X-mails # 下面配置的是子路由，子路由的属性继承于根路由(即上面的配置)，在子路由中可以覆盖根路由的配置 # 下面是子路由的配置 routes: # 使用正则的方式匹配告警标签 - match_re: # 这里可以匹配出标签含有service=foo1或service=foo2或service=baz的告警 service: ^(foo1|foo2|baz)$ # 指定接收器为team-X-mails receiver: team-X-mails # 这里配置的是子路由的子路由，当满足父路由的的匹配时，这条子路由会进一步匹配出severity=critical的告警，并使用team-X-pager接收器发送告警，没有匹配到的告警会由父路由进行处理。 routes: - match: severity: critical receiver: team-X-pager # 这里也是一条子路由，会匹配出标签含有service=files的告警，并使用team-Y-mails接收器发送告警 - match: service: files receiver: team-Y-mails # 这里配置的是子路由的子路由，当满足父路由的的匹配时，这条子路由会进一步匹配出severity=critical的告警，并使用team-Y-pager接收器发送告警，没有匹配到的会由父路由进行处理。 routes: - match: severity: critical receiver: team-Y-pager # 该路由处理来自数据库服务的所有警报。如果没有团队来处理，则默认为数据库团队。 - match: # 首先匹配标签service=database service: database # 指定接收器 receiver: team-DB-pager # 根据受影响的数据库对告警进行分组 group_by: [alertname, cluster, database] routes: - match: owner: team-X receiver: team-X-pager # 告警是否继续匹配后续的同级路由节点，默认false，下面如果也可以匹配成功，会向两种接收器都发送告警信息(猜测。。。) continue: true - match: owner: team-Y receiver: team-Y-pager # 下面是关于inhibit(抑制)的配置，先说一下抑制是什么：抑制规则允许在另一个警报正在触发的情况下使一组告警静音。其实可以理解为告警依赖。比如一台数据库服务器掉电了，会导致db监控告警、网络告警等等，可以配置抑制规则如果服务器本身down了，那么其他的报警就不会被发送出来。 inhibit_rules: #下面配置的含义：当有多条告警在告警组里时，并且他们的标签alertname,cluster,service都相等，如果severity: 'critical'的告警产生了，那么就会抑制severity: 'warning'的告警。 - source_match: # 源告警(我理解是根据这个报警来抑制target_match中匹配的告警) severity: 'critical' # 标签匹配满足severity=critical的告警作为源告警 target_match: # 目标告警(被抑制的告警) severity: 'warning' # 告警必须满足标签匹配severity=warning才会被抑制。 equal: ['alertname', 'cluster', 'service'] # 必须在源告警和目标告警中具有相等值的标签才能使抑制生效。(即源告警和目标告警中这三个标签的值相等'alertname', 'cluster', 'service') # 下面配置的是接收器 receivers: # 接收器的名称、通过邮件的方式发送、 - name: 'team-X-mails' email_configs: # 发送给哪些人 - to: 'team-X+alerts@example.org' # 是否通知已解决的警报 send_resolved: true # 接收器的名称、通过邮件和pagerduty的方式发送、发送给哪些人，指定pagerduty的service_key - name: 'team-X-pager' email_configs: - to: 'team-X+alerts-critical@example.org' pagerduty_configs: - service_key: \u003cteam-X-key\u003e # 接收器的名称、通过邮件的方式发送、发送给哪些人 - name: 'team-Y-mails' email_configs: - to: 'team-Y+alerts@example.org' # 接收器的名称、通过pagerduty的方式发送、指定pagerduty的service_key - name: 'team-Y-pager' pagerduty_configs: - service_key: \u003cteam-Y-key\u003e # 一个接收器配置多种发送方式 - name: 'ops' webhook_configs: - url: 'http://prometheus-webhook-dingtalk.kube-ops.svc.cluster.local:8060/dingtalk/webhook1/send' send_resolved: true email_configs: - to: '742899387@qq.com' send_resolved: true - to: 'soulchild@soulchild.cn' send_resolved: true ","date":"2023-02-19","objectID":"/posts/prometheus/:8:2","tags":null,"title":"Prometheus 入门","uri":"/posts/prometheus/"},{"categories":["kubernetes"],"content":"部署 1. 配置 apiVersion: v1 kind: ConfigMap metadata: name: alertmanager namespace: prometheus data: alertmanager.yml: | route: # 根路由 receiver: 'test-receiver' # 根路由接收者（还可以设置子路由) group_wait: 30s # 下面时间内如果接收到多个报警，则会合并成一个通知发送给receiver group_interval: 1m # 两次报警通知的时间间隔 repeat_interval: 2m # 发送相同告警的时间间隔 group_by: [alertname] # 分组规则 receivers: # 定义所有接收者 - name: 'test-receiver' # 接收者名称 webhook_configs: # 设置为webhook类型 - url: 'http://alertmanager-mywebhook/' 路由（route）：用于配置 Alertmanager 如何处理传入的特定类型的告警通知，其基本逻辑是根据路由匹配规则的匹配结果来确定处理当前报警通知的路径和行为 分组（grouping）：根据 prometheus 的 lables 进行报警分组，这些警报会合并为一个通知发送给接收器。在生产环境中，特别是云环境下的业务之间密集耦合时，若出现多台 Instance 故障，可能会导致成千上百条警报触发。在这种情况下使用分组机制，可以把这些被触发的警报合并为一个警报进行通知，从而避免瞬间突发性的接受大量警报通知 2. 工作负载 可以通过启动参数 --config.file 来指定配置文件 apiVersion: apps/v1 kind: Deployment metadata: name: alertmanager namespace: prometheus labels: name: alertmanager spec: selector: matchLabels: app: alertmanager replicas: 1 template: metadata: labels: app: alertmanager spec: nodeName: lain1 containers: - name: alertmanager image: prom/alertmanager:v0.22.2 volumeMounts: - name: config mountPath: /etc/alertmanager ports: - containerPort: 9093 volumes: - name: config configMap: name: alertmanager --- apiVersion: v1 kind: Service metadata: name: alertmanager namespace: prometheus labels: app: alertmanager spec: type: ClusterIP ports: - port: 9093 targetPort: 9093 selector: app: alertmanager 3. 配置 prometheus 和 alertManager 通信 修改 prometheus 的配置，加入内容 alerting: alertmanagers: - static_configs: - targets: [\"alertmanager:9093\"] ","date":"2023-02-19","objectID":"/posts/prometheus/:8:3","tags":null,"title":"Prometheus 入门","uri":"/posts/prometheus/"},{"categories":["kubernetes"],"content":"在 prometheus 中创建报警规则 创建一个基于自定义指标的报警规则配置，如果2分钟内每个商品的平均每秒访问量总和超过0.5，然后会发送报警信息 POST 到提供的 webHook 中 一个报警信息在生命周期内有下面3中状态： inactive: 表示当前报警信息既不是firing状态也不是pending状态 pending: 表示在设置的阈值时间范围内被激活了 firing: 表示超过设置的阈值时间被激活了 apiVersion: v1 kind: ConfigMap metadata: name: prodrule namespace: prometheus data: prodrule.yaml: | groups: - name: prods rules: - alert: prodsvisit # 报警名称 expr: sum(rate(lain_prods_visit[2m])) \u003e 0.5 # PromQL表达式，用于指定情况规则属于告警范围 for: 10s # 告警持续时间，超过会推送给 Alertmanager labels: # 自定义标签 level: warning annotations: # 自定义注释 summary: \"商品访问量飙升\" description: \" 商品访问量飙升，预估值：{{ $value }}\" 把 ConfigMap 挂载到 prometheus 工作负载里 volumeMounts: - name: rules mountPath: /rules // 映射到/rules/prodrule.yaml 同时在 prometheus 配置里添加报警规则文件 rule_files: - \"/rules/prodrule.yaml\" ","date":"2023-02-19","objectID":"/posts/prometheus/:8:4","tags":null,"title":"Prometheus 入门","uri":"/posts/prometheus/"},{"categories":["kubernetes"],"content":"模板 警报模板可以自定义通知的信息格式，以及其包含的对应警报指标数据，可以自定义 Email、企业微信的模板，配置指定的存放位置，这里的模板是指的发送的通知源信息格式模板，比如 Email，企业微信 示例模板 {{ define \"wechattpl\" }} {{ range $i, $alert :=.Alerts }} {{ if eq .Status \"resolved\" }} ========监控报警---解决了========== {{ else }} ========监控报警---发现警报========== {{ end }} 告警状态：{{ .Status }} 告警级别：{{ $alert.Labels.level }} 告警类型：{{ $alert.Labels.alertname }} 告警应用：{{ $alert.Annotations.summary }} 告警详情：{{ $alert.Annotations.description }} 告警时间：{{ $alert.StartsAt.Format \"2006-01-02 15:04:05\" }} ========end============= {{ end }} {{ end }} 修改 alertManager 配置 route: receiver: 'test' group_wait: 30s group_interval: 30s repeat_interval: 30s group_by: [alertname] templates: - '/tpl/*.tpl' receivers: - name: 'test' wechat_configs: - corp_id: '' to_party: '' agent_id: '' api_secret: '' message: '{{ template \"wechattpl\" . }}' send_resolved: true # 恢复时也发送消息 然后映射模板文件到 alertManager 工作负载的 /tpl 目录 ","date":"2023-02-19","objectID":"/posts/prometheus/:8:5","tags":null,"title":"Prometheus 入门","uri":"/posts/prometheus/"},{"categories":["kubernetes"],"content":"子路由 route: receiver: 'test' # 默认的接收者 group_wait: 30s group_interval: 30s repeat_interval: 30s group_by: [alertname] routes: # 根据根路由下的子路由设置报警分发策略 - receiver: 'abc' match_re: # 使用正则的方式匹配告警标签 level: 'danger' # 匹配到label时发送给指定的接收者 templates: - '/tpl/*.tpl' receivers: - name: \"abc\" webhook_configs: - url: 'http://192.168.0.106:9091/' - name: 'test' wechat_configs: - corp_id: '' to_party: '' agent_id: '' api_secret: '' message: '{{ template \"abc\" . }}' send_resolved: true ","date":"2023-02-19","objectID":"/posts/prometheus/:8:6","tags":null,"title":"Prometheus 入门","uri":"/posts/prometheus/"},{"categories":["kubernetes"],"content":"静默 静默（Silences）提供了一个简单的机制，根据标签快速对警报进行静默处理；对传进来的警报进行匹配检查，如果接受到警报符合静默的配置，Alertmanager 则不会发送警报通知。静默是需要在 WEB UI 界面中设置临时屏蔽指定的警报通知 ","date":"2023-02-19","objectID":"/posts/prometheus/:8:7","tags":null,"title":"Prometheus 入门","uri":"/posts/prometheus/"},{"categories":["kubernetes"],"content":"告警抑制 抑制（Inhibition）是当某条警报已经发送，停止重复发送由此警报引发的其他异常或故障的警报机制。在我们的灾备体系中，当原有集群故障宕机业务彻底无法访问的时候，会把用户流量切换到备份集群中，这样为故障集群及其提供的各个微服务状态发送警报机会失去了意义，此时， Alertmanager 的抑制特性就可以在一定程度上避免管理员收到过多无用的警报通知 inhibit_rules: # 如果level: 'danger'的告警产生了，level: 'warning'的告警 - source_match: level: 'danger' target_match: level: 'warning' equal: [\"kind\"] # 必须在源告警和目标告警中的kind标签值相等时才使抑制生效 ","date":"2023-02-19","objectID":"/posts/prometheus/:8:8","tags":null,"title":"Prometheus 入门","uri":"/posts/prometheus/"},{"categories":["kubernetes"],"content":"amtool amtool 是用于与 Alertmanager API 交互的 cli 工具 # 查看所有当前触发的警报 amtool --alertmanager.url=http://localhost:9093 alert # 模拟创建一个警报 amtool --alertmanager.url=http://localhost:9093 alert add level=danger kind=prods alertname=\"svcdwn\" --annotation=description=\"service down\" --annotation=summary=\"summary\" ","date":"2023-02-19","objectID":"/posts/prometheus/:8:9","tags":null,"title":"Prometheus 入门","uri":"/posts/prometheus/"},{"categories":["kubernetes"],"content":"Operator 模式 Kubernetes 是一个高度可扩展的\"系统\"，比如常见的自定义资源，控制器，准入控制及调度器进行扩展开发等。Operator 是一种包装、运行和管理 K8S 应用的一种方式。它涵盖了 CRD（CustomResourceDeftination） + AdmissionWebhook + Controller ，并以 Deployment 的形式部署到 K8S 中 CRD 用来定义声明式API（yaml），程序会通过该定义一直让最小调度单元（POD）趋向该状态 AdmissionWebhook 用来拦截请求做 mutate（修改）提交的声明（yaml）和 validate（校验）声明式的字段 Controller 主要的控制器，监视资源的 创建 / 更新 / 删除 事件，并触发 Reconcile 函数作为响应。整个调整过程被称作 Reconcile Loop（协调一致的循环），其实就是让 POD 趋向 CRD 定义所需的状态 ","date":"2023-02-12","objectID":"/posts/operator/:1:0","tags":null,"title":"使用 kubebuilder 开发 Operator","uri":"/posts/operator/"},{"categories":["kubernetes"],"content":"Operator 的工作方式 Operator 通过扩展 Kubernetes 控制平面和 API 进行工作。Operator 将一个 endpoint（称为自定义资源 CR）添加到 Kubernetes API 中，该 endpoint 还包含一个监控和维护新类型资源的控制平面组件。整个操作原理如下图所示： 当 Operator 接收任何信息时，它将采取行动将 Kubernetes 集群或外部系统调整到所需的状态，作为其在自定义 controller 中的和解循环（reconciliation loop）的一部分 ","date":"2023-02-12","objectID":"/posts/operator/:1:1","tags":null,"title":"使用 kubebuilder 开发 Operator","uri":"/posts/operator/"},{"categories":["kubernetes"],"content":"Resource、ResourceType 和 Controller Kubernetes发展到今天，其本质已经显现： Kubernetes 就是一个 “数据库”（数据实际持久存储在etcd中） 其 API 就是 “sql语句” API 设计采用基于 resource 的 Restful 风格，resource type 是 API 的端点（endpoint） 每一类 resource（即 Resource Type）是一张 “表”，Resource Type 的 spec 对应 “表结构” 信息（schema） 每张 “表” 里的一行记录就是一个 resource，即该表对应的 Resource Type 的一个实例（instance） Kubernetes 这个 “数据库” 内置了很多 “表”，比如 Pod、Deployment、DaemonSet、ReplicaSet 等 下面是一个Kubernetes API与resource关系的示意图： 我们看到 resource type 有两类，一类的 namespace 相关的（namespace-scoped），另外一类则是 namespace 无关，即 cluster 范围（cluster-scoped）的 我们知道 Kubernetes 并非真的只是一个 “数据库”，它是服务编排和容器调度的平台标准，它的基本调度单元是Pod（也是一个resource type），即一组容器的集合。那么 Pod 又是如何被创建、更新和删除的呢？这就离不开控制器（controller）了。每一类 resource type 都有自己对应的控制器（controller）。以 pod 这个 resource type 为例，它的 controller 为 ReplicasSet 的实例。控制器的运行逻辑如下图所示： 控制器一旦启动，将尝试获得 resource 的当前状态（current state），并与存储在 k8s 中的 resource 的期望状态（desired state，即 spec）做比对，如果不一致，controller 就会调用相应 API 进行调整，尽力使得 current state 与期望状态达成一致。这个达成一致的过程被称为协调（reconciliation） 根据前面我们对 resource type 理解，定义 CRD 相当于建立新 “表”（resource type），一旦 CRD 建立，k8s 会为我们自动生成对应 CRD 的 API endpoint，我们就可以通过 yaml 或 API 来操作这个 “表”。我们可以向 “表” 中 “插入” 数据，即基于 CRD 创建 Custom Resource（CR），这就好比我们创建 Deployment 实例，向 Deployment “表” 中插入数据一样 和原生内置的 resource type 一样，光有存储对象状态的 CR 还不够，原生 resource type 有对应 controller 负责协调（reconciliation）实例的创建、伸缩与删除，CR 也需要这样的 “协调者”，即我们也需要定义一个 controller 来负责监听 CR 状态并管理 CR 创建、伸缩、删除以及保持期望状态（spec）与当前状态（current state）的一致。这个 controller 不再是面向原生 Resource type 的实例，而是面向 CRD 的实例 CR 的 controller 有了自定义的操作对象类型（CRD），有了面向操作对象类型实例的 controller，我们将其打包为一个概念：“Operator模式”，operator 模式中的 controller 也被称为 operator，它是在集群中对 CR 进行维护操作的主体 ","date":"2023-02-12","objectID":"/posts/operator/:2:0","tags":null,"title":"使用 kubebuilder 开发 Operator","uri":"/posts/operator/"},{"categories":["kubernetes"],"content":"使用 kubebuilder 开发 operator Kubebuilder 是一个基于 CRD 来构建 Kubernetes API 的框架，可以使用 CRD 来构建 API、Controller 和 Admission Webhook。参考文档：The Kubebuilder Book 1. 安装 kubebuilder Github 地址：kubebuilder，将 kubebuilder 拷贝到你的 PATH 环境变量中的某个路径下即可 2. 创建项目 kubebuilder init --domain virtuallain.com 3. 创建API 这里我们就来建立自己的 CRD，即自定义的 resource type，也就是 API 的 endpoint kubebuilder create api --group myapp --version v1 --kind Redis 此刻，整个工程的目录布局如下： tree -F . . ├── Dockerfile ├── Makefile ├── PROJECT ├── README.md ├── api/ │ └── v1/ │ ├── groupversion_info.go │ ├── redis_types.go │ └── zz_generated.deepcopy.go ├── bin/ │ ├── controller-gen* ├── config/ │ ├── crd/ │ │ ├── bases/ │ │ │ └── myapp.virtuallain.com_redis.yaml │ │ ├── kustomization.yaml │ │ ├── kustomizeconfig.yaml │ │ └── patches/ │ │ ├── cainjection_in_redis.yaml │ │ └── webhook_in_redis.yaml │ ├── default/ │ │ ├── kustomization.yaml │ │ ├── manager_auth_proxy_patch.yaml │ │ └── manager_config_patch.yaml │ ├── manager/ │ │ ├── kustomization.yaml │ │ └── manager.yaml │ ├── prometheus/ │ │ ├── kustomization.yaml │ │ └── monitor.yaml │ ├── rbac/ │ │ ├── auth_proxy_client_clusterrole.yaml │ │ ├── auth_proxy_role.yaml │ │ ├── auth_proxy_role_binding.yaml │ │ ├── auth_proxy_service.yaml │ │ ├── kustomization.yaml │ │ ├── leader_election_role.yaml │ │ ├── leader_election_role_binding.yaml │ │ ├── redis_editor_role.yaml │ │ ├── redis_viewer_role.yaml │ │ ├── role.yaml │ │ ├── role_binding.yaml │ │ └── service_account.yaml │ └── samples/ │ └── myapp_v1_redis.yaml ├── controllers/ │ ├── redis_controller.go │ └── suite_test.go ├── go.mod ├── go.sum ├── hack/ │ └── boilerplate.go.txt ├── main.go 4. 为 CRD spec 添加字段 CRD 与 api 中的 redis_types.go 文件是同步的，我们只需修改这个文件即可。在 RedisSpec 结构中增加 Port 和 Replicas 字段 // RedisSpec defines the desired state of Redis type RedisSpec struct { //+kubebuilder:validation:Minimum:=81 //+kubebuilder:validation:Maximum=30000 Port int `json:\"port,omitempty\"` //+kubebuilder:validation:Minimum:=1 //+kubebuilder:validation:Maximum=100 Replicas int `json:\"replicas,omitempty\"` } 一旦定义完 CRD，我们就可以将其安装/卸载到 k8s 中 make install make uninstall 查看 kubectl get crd | grep redis 5. 实现 controller 的 Reconcile(协调) 逻辑 kubebuilder 为我们搭好了 controller 的代码架子，我们只需要在 controllers/redis_controller.go 中实现RedisReconciler 的 Reconcile 方法即可。下面是 Reconcile 的一个简易流程图 func (r *RedisReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) { redis := \u0026myappv1.Redis{} if err := r.Get(ctx, req.NamespacedName, redis); err != nil { return ctrl.Result{RequeueAfter: time.Second * 5}, err } fmt.Println(\"得到对象\", redis.Spec) } 6. 本地运行控制器 make run 通过 kubectl 创建该 CR apiVersion: myapp.virtuallain.com/v1 kind: Redis metadata: name: myredis namespace: default spec: port: 2000 观察 controller 的日志，可以看到当 CR 被创建后，controller 监听到相关事件 7. 构建 controller image 通过前文我们知道，这个 controller 其实就是运行在 k8s 中的一个 deployment 下的 pod，我们需要构建其 image 并通过 deployment 部署到 k8s 中。kubebuilder 创建的 operator 工程中包含了 Makefile，通过 make docker-build 即可构建 controller image，由于默认 GOPROXY 在国内无法访问，需要先修改 Dockerfile ENV GOPROXY=https://goproxy.io 然后执行 make docker build 构建成功后，执行 make docker-push 将 image 推送到镜像仓库中 8. 部署 controller 之前我们已经通过 make install 将 CRD 安装到k8s中了，接下来再把 controller 部署到 k8s 上，我们的 operator 就算部署完毕了 make deploy IMG=\u003csome-registry\u003e/\u003cproject-name\u003e:tag 使用 make undeploy 可以完整卸载 operator 相关 resource 因为一些特殊原因，一些镜像如 gcr.io/kubebuilder/kube-rbac-proxy 镜像可能拉取不下来，可以找到替代的镜像如 bitnami/kube-rbac-proxy 手动更改 tag ","date":"2023-02-12","objectID":"/posts/operator/:3:0","tags":null,"title":"使用 kubebuilder 开发 Operator","uri":"/posts/operator/"},{"categories":["kubernetes"],"content":"实现业务逻辑 提交 CR 时自动创建 pod 动态伸缩副本 删除时清理关联 pod func (r *RedisReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) { _ = log.FromContext(ctx) redis := \u0026myappv1.Redis{} if err := r.Get(ctx, req.NamespacedName, redis); err != nil { return ctrl.Result{}, nil } // 资源存在finalizers，删除时会增加DeletionTimestamp字段，只有当finalizers清空时才会真的删除 if !redis.DeletionTimestamp.IsZero() { return ctrl.Result{}, helper.ClearRedis(r.Client, ctx, redis) // 清理资源 } edited := false podNames := helper.GetRedisPodNames(redis) diff := len(redis.Finalizers) - len(podNames) // 实际副本数量 - 期望副本数量 if diff \u003e 0 { // 收缩副本 if err := helper.RmIfSurplus(r.Client, ctx, redis, podNames); err != nil { return ctrl.Result{}, err } edited = true } // 创建副本 for _, podName := range podNames { createPod, err := helper.CreateRedis(r.Client, ctx, redis, podName, r.Scheme) if err != nil { return ctrl.Result{}, err } // pod创建成功且redis资源的finalizers列表不包含新创建的pod名称 if createPod != \"\" \u0026\u0026 !controllerutil.ContainsFinalizer(redis, createPod) { redis.Finalizers = append(redis.Finalizers, createPod) // 追加pod edited = true } } if edited { err := r.Client.Update(ctx, redis) // 更新redis的Finalizers列表 return ctrl.Result{}, err } return ctrl.Result{}, nil } // GetRedisPodNames 根据副本数生成pod名称 func GetRedisPodNames(redis *v1.Redis) []string { podNames := make([]string, redis.Spec.Replicas) for i := 0; i \u003c redis.Spec.Replicas; i++ { podNames[i] = fmt.Sprintf(\"%s-%d\", redis.Name, i) } return podNames } // ExistPod 判断pod是否存在 func ExistPod(client client.Client, ctx context.Context, podName string, redis *v1.Redis) bool { err := client.Get(ctx, types.NamespacedName{Namespace: redis.Namespace, Name: podName}, \u0026corev1.Pod{}) return err == nil } // CreateRedis 创建redis func CreateRedis(client client.Client, ctx context.Context, redis *v1.Redis, podName string, schema *runtime.Scheme) (string, error) { if ExistPod(client, ctx, podName, redis) { return \"\", nil } pod := \u0026corev1.Pod{} pod.Namespace = redis.Namespace pod.Name = podName pod.Spec.Containers = []corev1.Container{ { Name: podName, Image: \"redis:5-alpine\", ImagePullPolicy: corev1.PullIfNotPresent, Ports: []corev1.ContainerPort{ { ContainerPort: int32(redis.Spec.Port), }, }, }, } // 设置pod的ownReference为redis资源，删除redis资源时会删除关联的pod if err := controllerutil.SetOwnerReference(redis, pod, schema); err != nil { return \"\", err } return podName, client.Create(ctx, pod) } // RmIfSurplus 收缩副本 func RmIfSurplus(client client.Client, ctx context.Context, redis *v1.Redis, podNames []string) error { rmPods := redis.Finalizers[len(podNames):] for _, rmPod := range rmPods { err := client.Delete(ctx, \u0026corev1.Pod{ ObjectMeta: metav1.ObjectMeta{ Name: rmPod, Namespace: redis.Namespace, }, }) if err != nil { return err } } redis.Finalizers = podNames // 更新finalizers return nil } // ClearRedis 清理关联pod func ClearRedis(client client.Client, ctx context.Context, redis *v1.Redis) error { /* // pod设置了ownReference后，就可以不用手动清理pod了 podList := redis.Finalizers for _, podName := range podList { pod := \u0026corev1.Pod{ ObjectMeta: metav1.ObjectMeta{ Name: podName, Namespace: redis.Namespace, }, } if err := client.Delete(ctx, pod); err != nil { return err } }*/ redis.Finalizers = []string{} // 将finalizers设置为空，否则无法删除该CR return client.Update(ctx, redis) } 自动重建被手动删除的 pod // 监听删除pod func (r *RedisReconciler) rmPodHandler(event event.DeleteEvent, limitingInterface workqueue.RateLimitingInterface) { for _, ref := range event.Object.GetOwnerReferences() { if ref.Kind == \"Redis\" \u0026\u0026 ref.APIVersion == \"myapp.virtuallain.com/v1\" { // 重建pod limitingInterface.Add(reconcile.Request{ // 重新入列 触发reconcile NamespacedName: types.NamespacedName{ Namespace: event.Object.GetNamespace(), Name: ref.Name, }, }) } } } // SetupWithManager sets up the controller with the Manager. func (r *RedisReconciler) SetupWithManager(mgr ctrl.Manager) error { return ctrl.NewControllerManagedBy(mgr). For(\u0026myappv1.Redis{}). Watches(\u0026source.Kind{ // 增加watch Type: \u0026corev1.Pod{}, }, handler.Funcs{DeleteFunc: r.rmPodHandler}).","date":"2023-02-12","objectID":"/posts/operator/:3:1","tags":null,"title":"使用 kubebuilder 开发 Operator","uri":"/posts/operator/"},{"categories":["kubernetes"],"content":"集成测试 （不推荐使用）参考文档：Writing tests、配置 EnvTest、Ginkgo ","date":"2023-02-12","objectID":"/posts/operator/:3:2","tags":null,"title":"使用 kubebuilder 开发 Operator","uri":"/posts/operator/"},{"categories":["kubernetes"],"content":"Kubernetes 准入控制器是集群管理必要功能。这些控制器主要在后台工作，并且许多可以作为编译插件使用，它可以极大地提高部署的安全性 ","date":"2023-02-08","objectID":"/posts/admission-webhook/:0:0","tags":null,"title":"Admission Webhook","uri":"/posts/admission-webhook/"},{"categories":["kubernetes"],"content":"Admission Controller 准入控制器 在 API 请求传递到 APIServer 之前拦截它们，并且可以禁止或修改它们。这适用于大多数类型的 Kubernetes 请求。准入控制器在经过适当的身份验证和授权后处理请求 默认情况下启用了几个准入控制器，因为大多数正常的 Kubernetes 操作都依赖于它们。这些控制器中的大多数都包含一些 Kubernetes 源代码树，并被编译为插件。但是，也可以编写和部署第三方准入控制器 在 Kubernetes apiserver 中包含两个特殊的准入控制器：MutatingAdmissionWebhook 和 ValidatingAdmissionWebhook。这两个控制器将发送准入请求到外部的 HTTP 回调服务并接收一个准入响应。如果启用了这两个准入控制器，Kubernetes 管理员可以在集群中创建和配置一个 admission webhook ","date":"2023-02-08","objectID":"/posts/admission-webhook/:1:0","tags":null,"title":"Admission Webhook","uri":"/posts/admission-webhook/"},{"categories":["kubernetes"],"content":"Admission Webhook 准入 Webhook 是一种用于接收准入请求并对其进行处理的 HTTP 回调机制。 可以定义两种类型的准入 webhook，即 验证性质的准入 Webhook 和 修改性质的准入 Webhook， 修改性质的准入 Webhook 会先被调用 总的来说，这样做的步骤如下： 检查集群中是否启用了 admission webhook 控制器，并根据需要进行配置 编写处理准入请求的 HTTP 回调，回调可以是一个部署在集群中的简单 HTTP 服务，甚至也可以是一个 serverless 函数，例如：denyenv-validating-admission-webhook 通过 MutatingWebhookConfiguration 和 ValidatingWebhookConfiguration 资源配置 admission webhook 这两种类型的 admission webhook 之间的区别是非常明显的：validating webhooks 可以拒绝请求，但是它们却不能修改在准入请求中获取的对象，而 mutating webhooks 可以在返回准入响应之前通过创建补丁来修改对象，如果 webhook 拒绝了一个请求，则会向最终用户返回错误 # 查看默认的控制器插件 kube-apiserver --help |grep enable-admission-plugins ","date":"2023-02-08","objectID":"/posts/admission-webhook/:2:0","tags":null,"title":"Admission Webhook","uri":"/posts/admission-webhook/"},{"categories":["kubernetes"],"content":"编写 Webhook webhook 是一个 http server，是一个限制了请求与响应格式（AdmissionReview/AdmissionResponse）的 http server。下面实现了一个 webhook 示例，通过监听 /mutate 来进行 mutating webhook 验证。这个 webhook 是一个简单的带 TLS 认证的 HTTP 服务，用 Deployment 方式部署在我们的集群中 main.go 文件包含创建 HTTP 服务的代码 func main() { http.HandleFunc(\"/mutate\", func(w http.ResponseWriter, r *http.Request) { var body []byte if r.Body != nil { if data, err := io.ReadAll(r.Body); err == nil { body = data } } if len(body) == 0 { klog.Error(\"empty body\") http.Error(w, \"empty body\", http.StatusBadRequest) return } reqAdmissionReview := admissionV1.AdmissionReview{} // 请求 rspAdmissionReview := admissionV1.AdmissionReview{ // 响应 TypeMeta: metaV1.TypeMeta{ Kind: \"AdmissionReview\", APIVersion: \"admission.k8s.io/v1\", }, } // 把body decode成对象 deserializer := lib.Codecs.UniversalDeserializer() if _, _, err := deserializer.Decode(body, nil, \u0026reqAdmissionReview); err != nil { klog.Error(err) rspAdmissionReview.Response = lib.ToV1AdmissionResponse(err) } else { rspAdmissionReview.Response = lib.AdmitPods(reqAdmissionReview) // 具体逻辑 } rspAdmissionReview.Response.UID = reqAdmissionReview.Request.UID respBytes, err := json.Marshal(rspAdmissionReview) if err != nil { klog.Errorf(\"Can't encode response: %v\", err) http.Error(w, fmt.Sprintf(\"could not encode response: %v\", err), http.StatusInternalServerError) } if _, err := w.Write(respBytes); err != nil { klog.Errorf(\"Can't write response: %v\", err) http.Error(w, fmt.Sprintf(\"could not write response: %v\", err), http.StatusInternalServerError) } }) tlsConfig := lib.Config{ // 挂载的证书文件 CertFile: \"/etc/webhook/certs/tls.crt\", KeyFile: \"/etc/webhook/certs/tls.key\", } server := \u0026http.Server{ Addr: \":443\", TLSConfig: lib.ConfigTLS(tlsConfig), } go func() { if err := server.ListenAndServeTLS(\"\", \"\"); err != nil { klog.Errorf(\"Failed to listen and serve webhook server: %v\", err) } }() klog.Info(\"Server started\") // listening OS shutdown signal signalChan := make(chan os.Signal, 1) signal.Notify(signalChan, syscall.SIGINT, syscall.SIGTERM) \u003c-signalChan klog.Infof(\"Got OS shutdown signal, shutting down webhook server gracefully...\") server.Shutdown(context.Background()) } lib/pods.go 文件的 AdmitPods 方法验证了 pod 名不能是 abc，并且创建了2个补丁，第1个补丁修改第一个容器的镜像为 nginx:1.19-alpine，第2个补丁添加了一个 init 容器到资源中 func patchImage() []byte { str := `[ { \"op\": \"replace\", \"path\": \"/spec/containers/0/image\", \"value\": \"nginx:1.19-alpine\" }, { \"op\": \"add\", \"path\": \"/spec/initContainers\", \"value\": [{ \"name\": \"init-test\", \"image\": \"busybox:1.28\", \"command\": [\"sh\", \"-c\", \"echo init container is running!\"] }] } ]` return []byte(str) } func AdmitPods(ar v1.AdmissionReview) *v1.AdmissionResponse { podResource := metav1.GroupVersionResource{Group: \"\", Version: \"v1\", Resource: \"pods\"} if ar.Request.Resource != podResource { err := fmt.Errorf(\"expect resource to be %s\", podResource) klog.Error(err) return ToV1AdmissionResponse(err) } raw := ar.Request.Object.Raw pod := corev1.Pod{} deserializer := Codecs.UniversalDeserializer() if _, _, err := deserializer.Decode(raw, nil, \u0026pod); err != nil { klog.Error(err) return ToV1AdmissionResponse(err) } reviewResponse := v1.AdmissionResponse{} if pod.Name == \"abc\" { klog.Error(\"pod name cannot be abc\") return ToV1AdmissionResponse(fmt.Errorf(\"pod name cannot be abc\")) } reviewResponse.Allowed = true reviewResponse.Patch = patchImage() // 通过创建补丁来修改对象 jsonPatch := v1.PatchTypeJSONPatch reviewResponse.PatchType = \u0026jsonPatch return \u0026reviewResponse } // 统一返回error 响应 func ToV1AdmissionResponse(err error) *v1.AdmissionResponse { return \u0026v1.AdmissionResponse{ Result: \u0026metav1.Status{ Message: err.Error(), }, } } lib/scheme.go var scheme = runtime.NewScheme() var Codecs = serializer.NewCodecFactory(scheme) func init() { addToScheme(scheme) } func addToScheme(scheme *runtime.Scheme) { utilruntime.Must(corev1.AddToScheme(scheme)) utilruntime.Must(admissionv1beta1.AddToScheme(scheme)) utilruntime.Must(admissionregistrationv1beta1.AddToScheme(scheme)) utilruntime.Must(admissionv1.AddToScheme(scheme)) utilrunti","date":"2023-02-08","objectID":"/posts/admission-webhook/:3:0","tags":null,"title":"Admission Webhook","uri":"/posts/admission-webhook/"},{"categories":["kubernetes"],"content":"部署服务 为了部署 webhook server，我们需要在我们的 Kubernetes 集群中创建一个 service 和 deployment 资源对象，还要将 TLS 证书映射到目录 ","date":"2023-02-08","objectID":"/posts/admission-webhook/:4:0","tags":null,"title":"Admission Webhook","uri":"/posts/admission-webhook/"},{"categories":["kubernetes"],"content":"生成 CA 证书 ca-config.json { \"signing\": { \"default\": { \"expiry\": \"8760h\" }, \"profiles\": { \"server\": { \"usages\": [\"signing\"], \"expiry\": \"8760h\" } } } } ca-csr.json { \"CN\": \"Kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"zh\", \"L\": \"bj\", \"O\": \"bj\", \"OU\": \"CA\" } ] } 生成 CA 证书 cfssl gencert -initca ca-csr.json | cfssljson -bare ca ","date":"2023-02-08","objectID":"/posts/admission-webhook/:4:1","tags":null,"title":"Admission Webhook","uri":"/posts/admission-webhook/"},{"categories":["kubernetes"],"content":"生成服务端证书 server-csr.json { \"CN\": \"admission\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"zh\", \"L\": \"bj\", \"O\": \"bj\", \"OU\": \"bj\" } ] } 签发证书 cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -hostname=myhook.kube-system.svc \\ -profile=server \\ server-csr.json | cfssljson -bare server ","date":"2023-02-08","objectID":"/posts/admission-webhook/:4:2","tags":null,"title":"Admission Webhook","uri":"/posts/admission-webhook/"},{"categories":["kubernetes"],"content":"创建 Secret kubectl create secret tls myhook --cert=server.pem --key=server-key.pem -n kube-system ","date":"2023-02-08","objectID":"/posts/admission-webhook/:4:3","tags":null,"title":"Admission Webhook","uri":"/posts/admission-webhook/"},{"categories":["kubernetes"],"content":"部署工作负载 apiVersion: apps/v1 kind: Deployment metadata: name: myhook namespace: kube-system spec: replicas: 1 selector: matchLabels: app: myhook template: metadata: labels: app: myhook spec: nodeName: lain1 containers: - name: myhook image: alpine:3.12 imagePullPolicy: IfNotPresent command: [\"/app/myhook\"] volumeMounts: - name: hooktls mountPath: /etc/webhook/certs readOnly: true - name: app mountPath: /app ports: - containerPort: 443 volumes: - name: app hostPath: path: /home/txl/hook/build - name: hooktls secret: secretName: myhook --- apiVersion: v1 kind: Service metadata: name: myhook namespace: kube-system labels: app: myhook spec: type: ClusterIP ports: - port: 443 targetPort: 443 selector: app: myhook ","date":"2023-02-08","objectID":"/posts/admission-webhook/:4:4","tags":null,"title":"Admission Webhook","uri":"/posts/admission-webhook/"},{"categories":["kubernetes"],"content":"部署 MutatingWebhook 接着，创建一个 MutatingWebhookConfiguration 将我们创建的 webhook 信息注册到 Kubernetes API server。CA 证书应提供给 admission webhook 配置，这样 apiserver 才可以信任 webhook server 提供的 TLS 证书 apiVersion: admissionregistration.k8s.io/v1 kind: MutatingWebhookConfiguration metadata: name: myhook webhooks: - clientConfig: # 填充内容为 cat ca.pem | base64 caBundle: | ... service: name: myhook namespace: kube-system path: /mutate failurePolicy: Fail sideEffects: NoneOnDryRun name: myhook.virtuallain.com admissionReviewVersions: [\"v1\", \"v1beta1\"] namespaceSelector: matchExpressions: - key: pod-injection operator: In values: [ \"enable\", \"1\" ] rules: - apiGroups: [\"\"] apiVersions: [\"v1\"] operations: [\"CREATE\"] resources: [\"pods\"] 如上述的清单信息所示，我们要求 Kubernetes 把（部署了 MutatingWebhookConfiguration ）命名空间中所有的 Pod 创建请求，只要匹配上 “pod-injection=true” 标签的，就将其转发到 myhook 的 “/mutate” 路径下，交给其处理 ","date":"2023-02-08","objectID":"/posts/admission-webhook/:4:5","tags":null,"title":"Admission Webhook","uri":"/posts/admission-webhook/"},{"categories":["kubernetes"],"content":"kustomize 是一个通过 kustomization 文件定制 kubernetes 对象的工具，它可以通过一些资源生成一些新的资源，也可以定制不同的资源的集合 一个比较典型的场景是我们有一个应用，在不同的环境例如生产环境和测试环境，它的 yaml 配置绝大部分都是相同的，只有个别的字段不同，这时候就可以利用 kustomize 来解决 文档：The Kustomization File ","date":"2023-02-07","objectID":"/posts/kustomize/:0:0","tags":null,"title":"Kustomize","uri":"/posts/kustomize/"},{"categories":["kubernetes"],"content":"结构 ├── base │ ├── deployment.yaml │ ├── kustomization.yaml │ └── service.yaml └── overlays ├── dev │ ├── kustomization.yaml │ └── patch.yaml ├── prod │ ├── kustomization.yaml │ └── patch.yaml └── staging ├── kustomization.yaml └── patch.yaml 一个常见的项目 kustomize 项目布局如上所示，可以看到每个环境文件夹里面都有一个 kustomization.yaml 文件，这个文件里面就类似配置文件，里面指定源文件以及对应的一些转换文件，例如 patch 等 ","date":"2023-02-07","objectID":"/posts/kustomize/:1:0","tags":null,"title":"Kustomize","uri":"/posts/kustomize/"},{"categories":["kubernetes"],"content":"基础模板 base/kustomization.yaml 示例 apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - service.yaml - deployment.yaml 使用 kustomize build 命令运行后会把两个文件连接在一起 ","date":"2023-02-07","objectID":"/posts/kustomize/:2:0","tags":null,"title":"Kustomize","uri":"/posts/kustomize/"},{"categories":["kubernetes"],"content":"定制 现在我们想要针对一些特定场景进行定制，比如，针对生产环境和测试环境需要由不同的配置 overlays/dev/kustomization.yaml 示例 namespace: dev namePrefix: dev- commonLabels: someName: someValue bases: - ../../base overlays/prod/kustomization.yaml 示例 namespace: prod namePrefix: prod- commonLabels: someName: someValue bases: - ../../base ","date":"2023-02-07","objectID":"/posts/kustomize/:3:0","tags":null,"title":"Kustomize","uri":"/posts/kustomize/"},{"categories":["kubernetes"],"content":"patchesStrategicMerge 可以覆盖一些在 base 文件中已有的配置。如修改 dev 环境的副本数量为2个，同时修改 container1 容器的镜像名 overlays/dev/kustomization.yaml 示例 namespace: prod namePrefix: prod- commonLabels: someName: someValue bases: - ../../base patchesStrategicMerge: - replica.yaml overlays/dev/replica.yaml 示例 apiVersion: apps/v1 kind: Deployment metadata: name: myngx spec: replicas: 2 template: spec: containers: - name: container1 image: dev-image:1.0 ","date":"2023-02-07","objectID":"/posts/kustomize/:4:0","tags":null,"title":"Kustomize","uri":"/posts/kustomize/"},{"categories":["kubernetes"],"content":"patchesJson6902 patchesJson6902 列表中的每个条目都应可以解析为 kubernetes 对象和将应用于该对象的 JSON patch 目标字段指向的 kubernetes 对象的 group、 version、 kind、 name 和 namespace 在同一 kustomization 内 path 字段内容是 JSON patch 文件的相对路径 如修改 dev 环境下 myngx deployoment 的 containerPort base/deployment.yaml 示例 apiVersion: apps/v1 kind: Deployment metadata: name: myngx namespace: default spec: selector: matchLabels: app: myngx replicas: 1 template: metadata: labels: app: myngx spec: containers: - name: ngx1 image: nginx:1.18-alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80 - containerPort: 443 overlays/dev/kustomization.yaml 示例 namespace: prod namePrefix: prod- commonLabels: someName: someValue bases: - ../../base patchesStrategicMerge: - replica.yaml patchesJson6902: - target: group: app version: v1 kind: Deployment name: myngx path: port.yaml overlays/dev/port.yaml 示例 - op: replace path: /spec/template/spec/containers/0/ports/0/containerPort value: 8080 - op: replace path: /spec/template/spec/containers/0/ports/1/containerPort value: 8443 ","date":"2023-02-07","objectID":"/posts/kustomize/:5:0","tags":null,"title":"Kustomize","uri":"/posts/kustomize/"},{"categories":["kubernetes"],"content":"configMapGenerator 可以生成 ConfigMap 资源 configMapGenerator: - name: myconfig literals: - host=192.168.0.111 - port=1234 files: - mycnf.prop - mysql57=mysql.cnf ","date":"2023-02-07","objectID":"/posts/kustomize/:6:0","tags":null,"title":"Kustomize","uri":"/posts/kustomize/"},{"categories":["kubernetes"],"content":"generatorOptions 控制生成 ConfigMap 和 Secret 的行为 generatorOptions: labels: kustomize.generated.resources: somevalue annotations: kustomize.generated.resource: somevalue disableNameSuffixHash: true ","date":"2023-02-07","objectID":"/posts/kustomize/:7:0","tags":null,"title":"Kustomize","uri":"/posts/kustomize/"},{"categories":["kubernetes"],"content":"总结 Kustomize 给 Kubernetes 的用户提供一种可以重复使用配置的声明式应用管理，从而在配置工作中用户只需要管理和维护 Kubernetes 的原生 API 对象，而不需要使用复杂的模版。同时，使用 kustomzie 可以仅通过 Kubernetes 声明式 API 资源文件管理任何数量的 kubernetes 定制配置，可以通过 fork/modify/rebase 这样的工作流来管理海量的应用描述文件 ","date":"2023-02-07","objectID":"/posts/kustomize/:8:0","tags":null,"title":"Kustomize","uri":"/posts/kustomize/"},{"categories":["Istio"],"content":"EnvoyFilter 资源允许你定制由 Istio Pilot 生成的 Envoy 配置。使用该资源，你可以更新数值，添加特定的过滤器，甚至添加新的监听器、集群等等。小心使用这个功能，因为不正确的定制可能会破坏整个网格的稳定性。 这些 EnvoyFilter 被应用的顺序是：首先是配置在根命名空间中的所有 EnvoyFilter，其次是配置在工作负载命名空间中的所有匹配的 EnvoyFilter。当多个 EnvoyFilter 被绑定到给定命名空间中的相同工作负载时，将按照创建时间的顺序依次应用。如果有多个 EnvoyFilter 配置相互冲突，那么将无法确定哪个配置被应用。 配置参考文档：Istio EnvoyFilter、Envoy ","date":"2023-02-05","objectID":"/posts/istio-envoyfilter/:0:0","tags":null,"title":"Istio Envoy Filter","uri":"/posts/istio-envoyfilter/"},{"categories":["Istio"],"content":"基本示例 过滤器： Network Filters：网络过滤器。处理连接的核心 HTTP Filters：HTTP 过滤器。由特殊的网络过滤器 HttpConnectionManager 管理，处理 HTTP1/HTTP2/gRPC 请求 可以参考 envoy 文档：全部 http_filter、全部 filter ","date":"2023-02-05","objectID":"/posts/istio-envoyfilter/:1:0","tags":null,"title":"Istio Envoy Filter","uri":"/posts/istio-envoyfilter/"},{"categories":["Istio"],"content":"增加响应头 下面的示例中在响应中添加了一个名为 api-version 的头 apiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata: name: myfilter namespace: istio-system spec: workloadSelector: labels: istio: ingressgateway configPatches: - applyTo: HTTP_FILTER match: context: GATEWAY # 网关侦听器 proxy: proxyVersion: ^1\\.11.* listener: filterChain: # 匹配侦听器中的特定筛选器链 filter: name: \"envoy.filters.network.http_connection_manager\" subFilter: # 此筛选器中要匹配的下一级筛选器 name: \"envoy.filters.http.router\" patch: operation: INSERT_BEFORE value: name: my.lua typed_config: \"@type\": \"type.googleapis.com/envoy.extensions.filters.http.lua.v3.Lua\" inlineCode: | function envoy_on_response(response_handle) response_handle:headers():add(\"api-version\", \"1.0\") end 再创建一个 filter，响应时给 api-version 头加上前缀 apiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata: name: myfilter-prefix namespace: istio-system spec: workloadSelector: labels: istio: ingressgateway configPatches: - applyTo: HTTP_FILTER match: context: GATEWAY proxy: proxyVersion: ^1\\.11.* listener: filterChain: filter: name: \"envoy.filters.network.http_connection_manager\" subFilter: name: \"my.lua\" patch: operation: INSERT_BEFORE # 在 my.lua 之前插入 value: name: myprefix.lua typed_config: \"@type\": \"type.googleapis.com/envoy.extensions.filters.http.lua.v3.Lua\" inlineCode: | function envoy_on_response(response_handle) local ver = response_handle:headers():get(\"Api-version\") response_handle:headers():replace(\"Api-version\", \"version_\"..ver) end filter 在请求时会按照从前向后的顺序执行，响应时则相反 ","date":"2023-02-05","objectID":"/posts/istio-envoyfilter/:1:1","tags":null,"title":"Istio Envoy Filter","uri":"/posts/istio-envoyfilter/"},{"categories":["Istio"],"content":"增加请求头 apiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata: name: myfilter-adduserid namespace: istio-system spec: workloadSelector: labels: istio: ingressgateway configPatches: - applyTo: HTTP_FILTER match: context: GATEWAY proxy: proxyVersion: ^1\\.11.* listener: filterChain: filter: name: \"envoy.filters.network.http_connection_manager\" subFilter: name: \"my.lua\" patch: operation: INSERT_BEFORE value: name: adduserid.lua typed_config: \"@type\": \"type.googleapis.com/envoy.extensions.filters.http.lua.v3.Lua\" inlineCode: | function envoy_on_request(request_handle) request_handle:headers():add(\"userid\", \"101\") end ","date":"2023-02-05","objectID":"/posts/istio-envoyfilter/:1:2","tags":null,"title":"Istio Envoy Filter","uri":"/posts/istio-envoyfilter/"},{"categories":["Istio"],"content":"查看动态配置 在 istio-ingressgateway 服务的 pod 中执行 curl http://localhost:15000/config_dump?resource=dynamic_listeners ","date":"2023-02-05","objectID":"/posts/istio-envoyfilter/:1:3","tags":null,"title":"Istio Envoy Filter","uri":"/posts/istio-envoyfilter/"},{"categories":["Istio"],"content":"打印 Lua 日志 默认情况只会打印 err 以上级别的日志，可以进入 pod 临时开启 curl -X POST http://localhost:15000/logging?level=info 输出 info 日志 function envoy_on_request(request_handle) local userid = request_handle:headers():get(\"userid\") request_handle:logInfo(\"userId=\"..userid) end ","date":"2023-02-05","objectID":"/posts/istio-envoyfilter/:1:4","tags":null,"title":"Istio Envoy Filter","uri":"/posts/istio-envoyfilter/"},{"categories":["Istio"],"content":"结束响应 如请求时没有携带 appid 头，则直接结束响应 apiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata: name: myfilter-checkappid namespace: istio-system spec: workloadSelector: labels: istio: ingressgateway configPatches: - applyTo: HTTP_FILTER match: context: GATEWAY proxy: proxyVersion: ^1\\.11.* listener: filterChain: filter: name: \"envoy.filters.network.http_connection_manager\" subFilter: name: \"envoy.filters.http.cors\" patch: operation: INSERT_AFTER # 在cors后插入，确保响应时携带跨域头 value: name: checkappid.lua typed_config: \"@type\": \"type.googleapis.com/envoy.extensions.filters.http.lua.v3.Lua\" inlineCode: | function envoy_on_request(request_handle) local appid = request_handle:headers():get(\"appid\") if(appid == nil) then request_handle:respond( {[\":status\"] = \"400\"}, \"error appid\" ) end end ","date":"2023-02-05","objectID":"/posts/istio-envoyfilter/:1:5","tags":null,"title":"Istio Envoy Filter","uri":"/posts/istio-envoyfilter/"},{"categories":["Istio"],"content":"Envoy 将 gRPC 转码为 HTTP/JSON 一旦有了一个可用的 gRPC 服务，可以通过向服务添加一些额外的注解（annotation）将其作为 HTTP/JSON API 发布。你需要一个代理来转换 HTTP/JSON 调用并将其传递给 gRPC 服务。我们称这个过程为转码。然后你的服务就可以通过 gRPC 和 HTTP/JSON 访问。 ","date":"2023-02-05","objectID":"/posts/istio-envoyfilter/:2:0","tags":null,"title":"Istio Envoy Filter","uri":"/posts/istio-envoyfilter/"},{"categories":["Istio"],"content":"步骤1：使用HTTP选项标注服务进行转码 在每个 rpc 操作的花括号中可以添加选项，允许你指定如何将操作转换到 HTTP 请求（endpoint）。在 proto 中引入 ‘ google/api/annotations.proto’ 即可使用该选项 import \"google/api/annotations.proto\"; 转码为 GET 方法 service ProdService { rpc GetProd(ProdRequest) returns (ProdResponse) { option (google.api.http) = { get: \"/detail/{prod_id}\" }; } } 在 URL 中有一个名为 prod_id 的路径变量，这个变量会自动映射到输入操作中同名的字段 转码为 POST 方法 service ProdService { rpc GetProd(ProdRequest) returns (ProdResponse) { option (google.api.http) = { post: \"/detail\" body: \"*\" }; } } ","date":"2023-02-05","objectID":"/posts/istio-envoyfilter/:2:1","tags":null,"title":"Istio Envoy Filter","uri":"/posts/istio-envoyfilter/"},{"categories":["Istio"],"content":"步骤2：生成 descriptor descriptor 文件是 ProtoBuf 提供的动态解析机制，通过提供对应类（对象）的 Descriptor 对象，在解析时就可以动态获取类成员 protoc --proto_path=gsrc/protos --include_imports --include_source_info --descriptor_set_out=prod.descriptor prod_service.proto ","date":"2023-02-05","objectID":"/posts/istio-envoyfilter/:2:2","tags":null,"title":"Istio Envoy Filter","uri":"/posts/istio-envoyfilter/"},{"categories":["Istio"],"content":"步骤3：转码 可以使用 grpc-transcoder 库 go get github.com/AliyunContainerService/grpc-transcoder 执行 grpc-transcoder --version 1.11 --service_port 80 --service_name gprodsvc.myistio --proto_svc ProdService --descriptor prod.descriptor service_port：service 端口 service_name：service 全路径名称 proto_svc：proto service 名称 descriptor：生成的 descriptor 文件 执行成功会在当前目录下生成一个 grpc-transcoder-envoyfilter.yaml 和 header2metadata-envoyfilter.yaml 文件 ","date":"2023-02-05","objectID":"/posts/istio-envoyfilter/:2:3","tags":null,"title":"Istio Envoy Filter","uri":"/posts/istio-envoyfilter/"},{"categories":["Istio"],"content":"步骤4：创建过滤器 apiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata: name: grpcfilter namespace: istio-system spec: workloadSelector: labels: istio: grpc-ingressgateway configPatches: - applyTo: HTTP_FILTER match: context: GATEWAY proxy: proxyVersion: ^1\\.11.* listener: filterChain: filter: name: \"envoy.filters.network.http_connection_manager\" subFilter: name: \"envoy.filters.http.router\" patch: operation: INSERT_BEFORE value: name: envoy.grpc_json_transcoder typed_config: '@type': \"type.googleapis.com/envoy.extensions.filters.http.grpc_json_transcoder.v3.GrpcJsonTranscoder\" proto_descriptor_bin: ... services: - ProdService print_options: add_whitespace: true always_print_primitive_fields: true always_print_enums_as_ints: false preserve_proto_field_names: false ","date":"2023-02-05","objectID":"/posts/istio-envoyfilter/:2:4","tags":null,"title":"Istio Envoy Filter","uri":"/posts/istio-envoyfilter/"},{"categories":["Istio"],"content":"通过 HTTP 访问服务 // 根据 ca 和证书获取 tlsConfig 配置对象 func getTLSConfig() *tls.Config { cert, err := tls.LoadX509KeyPair(\"tools/out/clientgrpc.crt\", \"tools/out/clientgrpc.key\") if err != nil { log.Fatal(err) } certPool := x509.NewCertPool() ca, err := os.ReadFile(\"tools/out/virtuallainCA.crt\") if err != nil { log.Fatal(err) } certPool.AppendCertsFromPEM(ca) return \u0026tls.Config{ Certificates: []tls.Certificate{cert}, ServerName: \"grpc.virtuallain.com\", RootCAs: certPool, } } func main() { req, _ := http.NewRequest(\"POST\", \"https://grpc.virtuallain.com:30090/detail\", strings.NewReader(`{\"prod_id\":101}`)) tr := \u0026http.Transport{ TLSClientConfig: getTLSConfig(), } client := http.Client{ Transport: tr, } rsp, _ := client.Do(req) fmt.Println(rsp.Header) defer rsp.Body.Close() b, _ := io.ReadAll(rsp.Body) fmt.Println(string(b)) } ","date":"2023-02-05","objectID":"/posts/istio-envoyfilter/:2:5","tags":null,"title":"Istio Envoy Filter","uri":"/posts/istio-envoyfilter/"},{"categories":["Istio"],"content":"Envoy 限流过滤器 Envoy 支持两种速率限制：全局和本地。本地限流是在envoy内部提供一种令牌桶限速的功能，全局限流需要访问外部限速服务。下面是一个使用全局限流的示例 ","date":"2023-02-05","objectID":"/posts/istio-envoyfilter/:3:0","tags":null,"title":"Istio Envoy Filter","uri":"/posts/istio-envoyfilter/"},{"categories":["Istio"],"content":"基本配置 1. 限流配置 这个 ConfigMap 是限速服务用到的配置文件，在 EnvoyFilter 中被引用。这里配置了 /prods 每分钟限流3个请求，其他 url 限流每分钟100个请求 apiVersion: v1 kind: ConfigMap metadata: name: ratelimit-config data: config.yaml: | domain: prod-ratelimit descriptors: - key: PATH value: \"/prods\" rate_limit: unit: minute requests_per_unit: 3 - key: PATH rate_limit: unit: minute requests_per_unit: 100 2. 独立限流服务 参考 官方 rate-limit-service.yaml apiVersion: v1 kind: Service metadata: name: redis labels: app: redis spec: ports: - name: redis port: 6379 selector: app: redis --- apiVersion: apps/v1 kind: Deployment metadata: name: redis spec: replicas: 1 selector: matchLabels: app: redis template: metadata: labels: app: redis spec: containers: - image: redis:alpine imagePullPolicy: Always name: redis ports: - name: redis containerPort: 6379 restartPolicy: Always serviceAccountName: \"\" --- apiVersion: v1 kind: Service metadata: name: ratelimit labels: app: ratelimit spec: ports: - name: http-port port: 8080 targetPort: 8080 protocol: TCP - name: grpc-port port: 8081 targetPort: 8081 protocol: TCP - name: http-debug port: 6070 targetPort: 6070 protocol: TCP selector: app: ratelimit --- apiVersion: apps/v1 kind: Deployment metadata: name: ratelimit spec: replicas: 1 selector: matchLabels: app: ratelimit strategy: type: Recreate template: metadata: labels: app: ratelimit spec: containers: - image: envoyproxy/ratelimit:6f5de117 # 2021/01/08 imagePullPolicy: Always name: ratelimit command: [\"/bin/ratelimit\"] env: - name: LOG_LEVEL value: debug - name: REDIS_SOCKET_TYPE value: tcp - name: REDIS_URL value: redis:6379 - name: USE_STATSD value: \"false\" - name: RUNTIME_ROOT value: /data - name: RUNTIME_SUBDIRECTORY value: ratelimit ports: - containerPort: 8080 - containerPort: 8081 - containerPort: 6070 volumeMounts: - name: config-volume mountPath: /data/ratelimit/config/config.yaml subPath: config.yaml volumes: - name: config-volume configMap: name: ratelimit-config 3. 创建 EnvoyFilter 这个 EnvoyFilter 作用在网关上，配置了 http 过滤器 envoy.filters.http.ratelimit，和一个 cluster。http 过滤器的 cluster 地址指向 cluster 配置的地址，就是 ratelimit service 所在的地址。domain 和步骤1中 configmap 的值一致，failure_mode_deny 表示超过请求限值就拒绝，rate_limit_service 配置 ratelimit 服务的地址（cluster），可以配置 grpc 类型或 http 类型 apiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata: name: filter-ratelimit namespace: istio-system spec: workloadSelector: # select by label in the same namespace labels: istio: ingressgateway configPatches: # The Envoy config you want to modify - applyTo: HTTP_FILTER match: context: GATEWAY listener: filterChain: filter: name: \"envoy.filters.network.http_connection_manager\" subFilter: name: \"envoy.filters.http.router\" patch: operation: INSERT_BEFORE # Adds the Envoy Rate Limit Filter in HTTP filter chain. value: name: envoy.filters.http.ratelimit typed_config: \"@type\": type.googleapis.com/envoy.extensions.filters.http.ratelimit.v3.RateLimit # domain can be anything! Match it to the ratelimter service config domain: prod-ratelimit failure_mode_deny: true rate_limit_service: grpc_service: envoy_grpc: cluster_name: rate_limit_cluster timeout: 10s transport_api_version: V3 - applyTo: CLUSTER match: cluster: service: ratelimit.default.svc.cluster.local patch: operation: ADD # Adds the rate limit service cluster for rate limit service defined in step 1. value: name: rate_limit_cluster type: STRICT_DNS connect_timeout: 10s lb_policy: ROUND_ROBIN http2_protocol_options: {} load_assignment: cluster_name: rate_limit_cluster endpoints: - lb_endpoints: - endpoint: address: socket_address: address: ratelimit.default.svc.cluster.local port_value: 8081 4. 创建 Action EnvoyFilter 这个 EnvoyFilter 作用在入口网关处，给80端口的虚拟主机配置了一个 rate_limits 动作，descriptor_key 用于选择在 configmap 里配置的 key apiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata: name: filter-ratelimit-svc namespace: istio-system spec: workloadSelector: labels: istio: ingressgateway configPatches: - applyTo: VIRTUAL_HOST match: context: GATEWAY routeConfiguration: vhost: name: \"p.virtuallai","date":"2023-02-05","objectID":"/posts/istio-envoyfilter/:3:1","tags":null,"title":"Istio Envoy Filter","uri":"/posts/istio-envoyfilter/"},{"categories":["Istio"],"content":"使用 header_value_match 参考 文档 修改 Action EnvoyFilter 下面第一个 action 配置了 /prods/\\d+ 路由规则的匹配。第二个 action 配置了存在头 version=v2 的匹配 apiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata: name: filter-ratelimit-svc namespace: istio-system spec: workloadSelector: labels: istio: ingressgateway configPatches: - applyTo: VIRTUAL_HOST match: context: GATEWAY routeConfiguration: vhost: name: \"p.virtuallain.com:80\" route: action: ANY patch: operation: MERGE # Applies the rate limit rules. value: rate_limits: - actions: - header_value_match: descriptor_value: path headers: - name: :path # exact_match: /prods safe_regex_match: # 正则匹配 google_re2: {} regex: /prods/\\d+ - actions: - header_value_match: descriptor_value: version-v2 headers: - name: version exact_match: v2 基本的匹配方式有： exact_match：精确匹配 safe_regex_match：正则匹配 range_match：范围匹配（数字范围，如[-10,0)） prefix_match：前缀匹配 suffix_match：后缀匹配 contains_match：包含匹配 invert_match：反向匹配 修改 ConfigMap 配置 下面配置了 /prods/\\d+ 的路由每分钟限流5次，当存在 header 头 version=v2 时每分钟限流2次。同时匹配到多个规则时优先生效次数少的规则。value 关联 header_value_match 里的 descriptor_value apiVersion: v1 kind: ConfigMap metadata: name: ratelimit-config data: config.yaml: | domain: prod-ratelimit descriptors: - key: header_match value: path rate_limit: requests_per_unit: 5 unit: minute - key: header_match value: version-v2 rate_limit: requests_per_unit: 2 unit: minute ","date":"2023-02-05","objectID":"/posts/istio-envoyfilter/:3:2","tags":null,"title":"Istio Envoy Filter","uri":"/posts/istio-envoyfilter/"},{"categories":["Istio"],"content":"IP 限流 修改 Action EnvoyFilter rate_limits: - actions: - remote_address: {} 修改 ConfigMap 配置 data: config.yaml: | domain: prod-ratelimit descriptors: - key: remote_address rate_limit: requests_per_unit: 10 unit: minute X-Forwarded-For 配置 当存在多个受信任代理的环境中，需要配置生效的 XFF 是第几个，参考 文档。实际运行可以用 nginx-ingress 来反代 istio 的 gateway 从而自动传递这个头 apiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata: name: xff-trust-hops namespace: istio-system spec: workloadSelector: labels: istio: ingressgateway configPatches: - applyTo: NETWORK_FILTER match: context: ANY listener: filterChain: filter: name: \"envoy.filters.network.http_connection_manager\" patch: operation: MERGE value: typed_config: \"@type\": \"type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\" use_remote_address: true xff_num_trusted_hops: 1 # Change as needed ","date":"2023-02-05","objectID":"/posts/istio-envoyfilter/:3:3","tags":null,"title":"Istio Envoy Filter","uri":"/posts/istio-envoyfilter/"},{"categories":["Istio"],"content":"IP 组合条件限流 修改 Action EnvoyFilter rate_limits: - actions: - header_value_match: descriptor_value: path headers: - name: :path safe_regex_match: google_re2: {} regex: /prods/\\d+ - remote_address: {} 修改 ConfigMap 配置 下面配置了每个 ip 在 /prods/\\d+ 的路由每分钟限流5次 data: config.yaml: | domain: prod-ratelimit descriptors: - key: header_match value: path descriptors: - key: remote_address rate_limit: requests_per_unit: 5 unit: minute ","date":"2023-02-05","objectID":"/posts/istio-envoyfilter/:3:4","tags":null,"title":"Istio Envoy Filter","uri":"/posts/istio-envoyfilter/"},{"categories":["Istio"],"content":"自定义限流服务 上面使用的是官方 envoyproxy/ratelimit 限流服务，也可以自己实现一个。示例： package main import ( \"context\" pb \"github.com/envoyproxy/go-control-plane/envoy/service/ratelimit/v3\" \"google.golang.org/grpc\" \"log\" \"net\" \"time\" ) type MyServer struct{} func NewMyServer() *MyServer { return \u0026MyServer{} } // 实现限流方法 func (s *MyServer) ShouldRateLimit(ctx context.Context, request *pb.RateLimitRequest) (*pb.RateLimitResponse, error) { var overallCode pb.RateLimitResponse_Code if time.Now().Unix()%2 == 0 { log.Println(\"限流了\") overallCode = pb.RateLimitResponse_OVER_LIMIT } else { log.Println(\"通过了\") overallCode = pb.RateLimitResponse_OK } response := \u0026pb.RateLimitResponse{OverallCode: overallCode} return response, nil } func main() { lis, err := net.Listen(\"tcp\", \":8080\") if err != nil { log.Fatal(err) } s := grpc.NewServer() pb.RegisterRateLimitServiceServer(s, NewMyServer()) if err := s.Serve(lis); err != nil { log.Fatal(err) } } ","date":"2023-02-05","objectID":"/posts/istio-envoyfilter/:3:5","tags":null,"title":"Istio Envoy Filter","uri":"/posts/istio-envoyfilter/"},{"categories":["Istio"],"content":"gRPC 环境 gRPC 是 Google公司基于 Protobuf 开发的跨语言的开源 RPC 框架。gRPC 基于 HTTP/2 协议设计，可以基于一个HTTP/2链接提供多个服务 ","date":"2023-02-04","objectID":"/posts/istio-grpc/:1:0","tags":null,"title":"Istio 部署 gRPC 服务并配置网关证书","uri":"/posts/istio-grpc/"},{"categories":["Istio"],"content":"安装 1. protobuf 从 protobuf 这里下载，把 bin 目录加入环境变量 2. go protobuf 库 go get -u github.com/golang/protobuf@v1.5.2 3. go 插件 go install google.golang.org/protobuf/cmd/protoc-gen-go@v1.28 该插件会根据 .proto 文件生成一个后缀为 .pb.go 的文件，包含所有 .proto 文件中定义的类型及其序列化方法 go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@v1.2 该插件会生成一个后缀为 _grpc.pb.go 的文件，其中包含： 一种接口类型(或存根) ，供客户端调用的服务方法。 服务器要实现的接口类型。 上述命令会默认将插件安装到 $GOPATH/bin，为了 protoc 编译器能找到这些插件，请确保你的$GOPATH/bin在环境变量中 4. grpc 库 go get -u google.golang.org/grpc@v1.46.2 ","date":"2023-02-04","objectID":"/posts/istio-grpc/:1:1","tags":null,"title":"Istio 部署 gRPC 服务并配置网关证书","uri":"/posts/istio-grpc/"},{"categories":["Istio"],"content":"编写 .proto 定义服务 src/prod_model.proto: syntax = \"proto3\"; option go_package = \"src/pbfiles\"; message ProdRequest { int32 prod_id =1; } message ProdModel { int32 id=1; string name=2; } message ProdResponse { ProdModel result=1; } src/prod_service.proto: syntax = \"proto3\"; import \"prod_model.proto\"; option go_package = \"src/pbfiles\"; service ProdService { rpc GetProd(ProdRequest) returns (ProdResponse); } 在项目更目录执行以下命令，根据 proto 生成 go 源码文件 protoc320 --proto_path=src --go_out=./ prod_model.proto protoc320 --proto_path=src --go-grpc_out=./ prod_service.proto 编写 server 端： type ProdService struct { pbfiles.UnimplementedProdServiceServer } func NewProdService() *ProdService { return \u0026ProdService{} } func (this *ProdService) GetProd(ctx context.Context, req *pbfiles.ProdRequest) (*pbfiles.ProdResponse, error) { model := \u0026pbfiles.ProdModel{ Id: req.ProdId, Name: fmt.Sprintf(\"%s%d\", \"测试商品\", req.ProdId), } rsp := \u0026pbfiles.ProdResponse{Result: model} return rsp, nil } func main() { myserver := grpc.NewServer() // 创建服务 pbfiles.RegisterProdServiceServer(myserver, services.NewProdService()) // 监听8080 lis, _ := net.Listen(\"tcp\", \":8080\") if err := myserver.Serve(lis); err != nil { log.Fatal(err) } } ","date":"2023-02-04","objectID":"/posts/istio-grpc/:1:2","tags":null,"title":"Istio 部署 gRPC 服务并配置网关证书","uri":"/posts/istio-grpc/"},{"categories":["Istio"],"content":"测试客户端 func main() { client, err := grpc.DialContext(context.Background(), \":8080\", grpc.WithInsecure()) rsp := \u0026pbfiles.ProdResponse{} err = client.Invoke(context.Background(), \"/ProdService/GetProd\", \u0026pbfiles.ProdRequest{ProdId: 123}, rsp) if err != nil { log.Fatal(err) } fmt.Println(rsp.Result) } ","date":"2023-02-04","objectID":"/posts/istio-grpc/:1:3","tags":null,"title":"Istio 部署 gRPC 服务并配置网关证书","uri":"/posts/istio-grpc/"},{"categories":["Istio"],"content":"部署网格内 gRPC 服务 示例如下 ","date":"2023-02-04","objectID":"/posts/istio-grpc/:2:0","tags":null,"title":"Istio 部署 gRPC 服务并配置网关证书","uri":"/posts/istio-grpc/"},{"categories":["Istio"],"content":"创建 gRPC Gate 网关 demo.yaml apiVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: components: egressGateways: - enabled: true name: istio-egressgateway ingressGateways: - enabled: true name: istio-ingressgateway - enabled: true label: app: grpc-ingressgateway # 自定义标签 istio: grpc-ingressgateway k8s: resources: requests: cpu: 10m memory: 40Mi service: ports: - name: status-port port: 15021 targetPort: 15021 - name: http2 port: 80 targetPort: 8080 - name: https port: 443 targetPort: 8443 - name: tcp port: 31400 targetPort: 31400 - name: tls port: 15443 targetPort: 15443 name: grpc-ingressgateway 执行部署： istioctl install -f demo.yaml ","date":"2023-02-04","objectID":"/posts/istio-grpc/:2:1","tags":null,"title":"Istio 部署 gRPC 服务并配置网关证书","uri":"/posts/istio-grpc/"},{"categories":["Istio"],"content":"创建网关规则 apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: grpc-gateway namespace: myistio spec: selector: istio: grpc-ingressgateway servers: - port: number: 80 name: grpc protocol: HTTPS hosts: - \"*\" ","date":"2023-02-04","objectID":"/posts/istio-grpc/:2:2","tags":null,"title":"Istio 部署 gRPC 服务并配置网关证书","uri":"/posts/istio-grpc/"},{"categories":["Istio"],"content":"创建虚拟服务 apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: grpcvs namespace: myistio spec: hosts: - \"*\" gateways: - grpc-gateway http: - route: - destination: host: gprodsvc port: number: 80 ","date":"2023-02-04","objectID":"/posts/istio-grpc/:2:3","tags":null,"title":"Istio 部署 gRPC 服务并配置网关证书","uri":"/posts/istio-grpc/"},{"categories":["Istio"],"content":"配置网关证书 ","date":"2023-02-04","objectID":"/posts/istio-grpc/:3:0","tags":null,"title":"Istio 部署 gRPC 服务并配置网关证书","uri":"/posts/istio-grpc/"},{"categories":["Istio"],"content":"配置方式 1. 文件挂载的方式 Istio 网关将会自动加载 istio-system 命名空间下名称为 stio-ingressgateway-certs 的 secret，并分别挂载到 /etc/istio/ingressgateway-certs/tls.crt 和 /etc/istio/ingressgateway-certs/tls.key。指定 serverCertificate 和 privateKey 字段 2. 指定密钥的方式 指定 credentialName 字段 ","date":"2023-02-04","objectID":"/posts/istio-grpc/:3:1","tags":null,"title":"Istio 部署 gRPC 服务并配置网关证书","uri":"/posts/istio-grpc/"},{"categories":["Istio"],"content":"配置 HTTP TLS 证书 kubectl create -n istio-system secret tls istio-ingressgateway-certs --key api.virtuallain.com.key --cert api.virtuallain.com.crt Gateway 加入 tls 节点： apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: p-gateway namespace: myistio spec: selector: istio: ingressgateway # use Istio default gateway implementation servers: - port: number: 80 name: http protocol: HTTP hosts: - api.virtuallain.com - port: number: 443 name: https protocol: HTTPS tls: mode: SIMPLE serverCertificate: /etc/istio/ingressgateway-certs/tls.crt # 使用挂载证书文件 privateKey: /etc/istio/ingressgateway-certs/tls.key # credentialName: ssl-ingressgateway-certs hosts: - api.virtuallain.com ","date":"2023-02-04","objectID":"/posts/istio-grpc/:3:2","tags":null,"title":"Istio 部署 gRPC 服务并配置网关证书","uri":"/posts/istio-grpc/"},{"categories":["Istio"],"content":"配置 gRPC 证书（单向认证） 使用 certstrap 开源库生成证书 1. 自签 CA 证书 ./cert init --common-name \"virtuallainCA\" --expires \"20 years\" 2. 服务端证书 # 得到证书请求文件 ./cert request-cert -cn grpc.virtuallain.com -domain \"*.virtuallain.com\" # CA去签名请求文件 ./cert sign grpc.virtuallain.com --CA virtuallainCA 3. 导入 k8s kubectl create -n istio-system secret tls grpc-ingressgateway-certs --key=grpc.virtuallain.com.key --cert=grpc.virtuallain.com.crt # 效果同上 kubectl create -n istio-system secret generic grpc-ingressgateway-certs --from-file=key=grpc.virtuallain.com.key --from-file=cert=grpc.virtuallain.com.crt 4. 配置 Gateway apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: grpc-gateway namespace: myistio spec: selector: istio: grpc-ingressgateway servers: - port: number: 80 name: grpc protocol: HTTPS tls: mode: SIMPLE credentialName: grpc-ingressgateway-certs hosts: - \"*\" 5. 客户端测试 func main() { creds, err := credentials.NewClientTLSFromFile(\"tools/out/grpc.virtuallain.com.crt\", \"grpc.virtuallain.com\") if err != nil { log.Fatal(err) } client, err := grpc.DialContext(context.Background(), \"grpc.virtuallain.com:30090\", grpc.WithTransportCredentials(creds)) if err != nil { log.Fatal(err) } rsp := \u0026pbfiles.ProdResponse{} err = client.Invoke(context.Background(), \"/ProdService/GetProd\", \u0026pbfiles.ProdRequest{ProdId: 123}, rsp) if err != nil { log.Fatal(err) } fmt.Println(rsp.Result) } ","date":"2023-02-04","objectID":"/posts/istio-grpc/:3:3","tags":null,"title":"Istio 部署 gRPC 服务并配置网关证书","uri":"/posts/istio-grpc/"},{"categories":["Istio"],"content":"配置 gRPC 证书（双向认证） 1. 同步生成客户端证书 ./cert request-cert -cn clientgrpc ./cert sign clientgrpc --CA virtuallainCA 2. 重新导入证书 kubectl create -n istio-system secret generic grpc-ingressgateway-certs --from-file=key=grpc.virtuallain.com.key --from-file=cert=grpc.virtuallain.com.crt --from-file=cacert=virtuallainCA.crt 3. 配置 Gateway 修改 mode 为 MUTUAL tls: mode: MUTUAL credentialName: grpc-ingressgateway-certs 4. 客户端测试 func main() { cert, err := tls.LoadX509KeyPair(\"tools/out/clientgrpc.crt\", \"tools/out/clientgrpc.key\") if err != nil { log.Fatal(err) } certPool := x509.NewCertPool() ca, err := os.ReadFile(\"tools/out/virtuallainCA.crt\") if err != nil { log.Fatal(err) } certPool.AppendCertsFromPEM(ca) creds := credentials.NewTLS(\u0026tls.Config{ Certificates: []tls.Certificate{cert}, ServerName: \"grpc.virtuallain.com\", RootCAs: certPool, }) client, err := grpc.DialContext(context.Background(), \"grpc.virtuallain.com:30090\", grpc.WithTransportCredentials(creds)) rsp := \u0026pbfiles.ProdResponse{} err = client.Invoke(context.Background(), \"/ProdService/GetProd\", \u0026pbfiles.ProdRequest{ProdId: 123}, rsp) if err != nil { log.Fatal(err) } fmt.Println(rsp.Result) } ","date":"2023-02-04","objectID":"/posts/istio-grpc/:3:4","tags":null,"title":"Istio 部署 gRPC 服务并配置网关证书","uri":"/posts/istio-grpc/"},{"categories":["Istio"],"content":"Istio 是一个开源的微服务管理、保护和监控框架，它有如下特性： 流量管理：利用配置，我们可以控制服务间的流量。设置断路器、超时或重试都可以通过简单的配置改变来完成。 可观察性：Istio 通过跟踪、监控和记录让我们更好地了解服务，让我们能够快速发现和修复问题。 安全性：Istio 可以在代理层面上管理认证、授权和通信的加密。我们可以通过快速的配置变更在各个服务中执行策略。 ","date":"2023-02-02","objectID":"/posts/istio/:0:0","tags":null,"title":"Istio 快速入门","uri":"/posts/istio/"},{"categories":["Istio"],"content":"Istio 组件 Istio 服务网格有两个部分：数据平面和控制平面 数据平面由一组智能代理 (Envoy) 组成，被部署为 sidecar，控制服务之间的通信 控制平面管理并配置代理来进行流量路由 下图展示了组成每个平面的不同组件： ","date":"2023-02-02","objectID":"/posts/istio/:1:0","tags":null,"title":"Istio 快速入门","uri":"/posts/istio/"},{"categories":["Istio"],"content":"安装 从 Github 上下载 Istio， (测试环境) 执行 istioctl manifest apply --set profile=demo 可以查看其中的配置 istioctl profile dump demo ","date":"2023-02-02","objectID":"/posts/istio/:2:0","tags":null,"title":"Istio 快速入门","uri":"/posts/istio/"},{"categories":["Istio"],"content":"Sidecar 注入 网格中的 Pod 必须运行一个 Istio Sidecar 代理，两种方法：使用 istioctl 手动注入或启用 Pod 所属命名空间的 Istio sidecar 注入器自动注入，启用自动注入后，自动注入器会使用准入控制器在创建 Pod 时自动注入代理配置 ","date":"2023-02-02","objectID":"/posts/istio/:3:0","tags":null,"title":"Istio 快速入门","uri":"/posts/istio/"},{"categories":["Istio"],"content":"手动注入 istioctl kube-inject -f api.yaml | kubectl apply -f – ","date":"2023-02-02","objectID":"/posts/istio/:3:1","tags":null,"title":"Istio 快速入门","uri":"/posts/istio/"},{"categories":["Istio"],"content":"自动注入 将 myistio 命名空间标记为 istio-injection=enabled kubectl label namespace myistio istio-injection=enabled ","date":"2023-02-02","objectID":"/posts/istio/:3:2","tags":null,"title":"Istio 快速入门","uri":"/posts/istio/"},{"categories":["Istio"],"content":"示例配置 apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: p-gateway namespace: myistio spec: selector: istio: ingressgateway # use Istio default gateway implementation servers: - port: number: 80 name: http protocol: HTTP hosts: - p.virtuallain.com --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: prodvs namespace: myistio spec: hosts: - prodsvc - reviewsvc - p.virtuallain.com gateways: - p-gateway - mesh # 用于内部服务互访 http: - match: - uri: prefix: \"/p\" rewrite: uri: \"/prods\" # 路径重写 route: - destination: host: prodsvc subset: v1svc # 服务端点 port: number: 80 fault: # 延迟故障注入 delay: fixedDelay: 1ms percentage: value: 15 timeout: 1s - match: - uri: prefix: / route: - destination: host: reviewsvc port: number: 80 # fault: # abort: # httpStatus: 500 # percentage: # value: 100 --- apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: prod-rule namespace: myistio spec: host: prodsvc trafficPolicy: loadBalancer: # 负载均衡 # simple: ROUND_ROBIN consistentHash: httpHeaderName: myname connectionPool: # 限流 tcp: maxConnections: 1 http: http1MaxPendingRequests: 1 maxRequestsPerConnection: 1 outlierDetection: # 熔断 consecutive5xxErrors: 2 interval: 10s maxEjectionPercent: 50 baseEjectionTime: 10s subsets: # 定义服务端点集合 - name: v1svc labels: # version: v2 app: prod ","date":"2023-02-02","objectID":"/posts/istio/:4:0","tags":null,"title":"Istio 快速入门","uri":"/posts/istio/"},{"categories":["Istio"],"content":"Gateway 网格服务的负载均衡器，负责网格边缘的服务暴露。可以使用 Gateway 资源来配置网关，网关资源描述了负责均衡器的暴露端口、协议、SNI (服务器名称指示) 配置等。网关资源在背后控制着 Envoy 代理在网络接口上的监听方式以及它出示的证书，其中 Ingress-gateway: 用于管理网格边缘入站的流量，通过入口网关将网格内部的服务暴露到外部提供访问，配合 VirtualService Egress gateway: 控制网格内服务访问外部服务，配合 DestinationRule ","date":"2023-02-02","objectID":"/posts/istio/:5:0","tags":null,"title":"Istio 快速入门","uri":"/posts/istio/"},{"categories":["Istio"],"content":"Virtual Service 虚拟服务可以理解为 k8s service 的一种增强，我们可以使用 VirtualService 资源在 Istio 服务网格中定义流量路由规则，并在客户端试图连接到服务时应用这些规则 配置如何在服务网格内将请求路由到服务 和网关整合并配置流量规则来控制出入流量 ","date":"2023-02-02","objectID":"/posts/istio/:6:0","tags":null,"title":"Istio 快速入门","uri":"/posts/istio/"},{"categories":["Istio"],"content":"Destination Rule 目标规则用于配置将流量转发到实际工作负载时应用的策略，如流量拆分、灰度发布、负载均衡等 subset (子集) 是服务端点的集合，可以用于 A/B 测试或者分版本路由等场景 ","date":"2023-02-02","objectID":"/posts/istio/:7:0","tags":null,"title":"Istio 快速入门","uri":"/posts/istio/"},{"categories":["Istio"],"content":"负载均衡器设置 基本配置参数： 简单类型 (simple)： ROUND_ROBIN: 轮询 (默认) LEAST_CONN: 最少连接，选择请求较少的主机 RANDOM: 随机选择 一致性哈希 (consistentHash): 目的是让同一用户的请求一直转发到后端同一实例 httpHeaderName: 基于 header 头 httpCookie: 基于 cookie useSourceIp: 基于 ip minimumRingSize: 环形哈希算法，适用于较多节点、频繁新增和删除节点的场景，填数字虚拟节点数量 ","date":"2023-02-02","objectID":"/posts/istio/:7:1","tags":null,"title":"Istio 快速入门","uri":"/posts/istio/"},{"categories":["Istio"],"content":"connectionPool 限流设置 ConnectionPool 可以对上游服务的并发连接数和请求数进行限制。适用于 HTTP 和 TCP 服务 基本配置参数： TCP maxConnections: 最大 HTTP1 / TCP 连接数 (默认值2 ^ 32-1) connectTimeout: 连接超时。格式为 1h / 1m / 1s / 1ms (默认值为10秒) tcpKeepalive: keepalive 设置 probes: 确认连接dead之前继续发送探测数据，发送几次 (默认9) time: 发送probe之前连接的空闲时间 (默认2小时) interval: 两次probe发送的间隔 (默认75秒) HTTP http1MaxPendingRequests: 等待时将排队的最大请求数 就绪的连接池连接 (默认为1024) http2MaxRequests: 对目标的活动请求的最大数量 (默认为1024) maxRequestsPerConnection: 每个连接的最大请求数量。如果将这一参数设置为 1 则会禁止 keepalive 特性 idleTimeout: 上游连接池连接的空闲超时，当达到空闲超时时，连接将被关闭 maxRetries: 最大重试次数 (默认为3) ","date":"2023-02-02","objectID":"/posts/istio/:7:2","tags":null,"title":"Istio 快速入门","uri":"/posts/istio/"},{"categories":["Istio"],"content":"outlierDetection 熔断设置 跟踪每个状态的断路器实现上游服务中的单个主机。适用于 HTTP 和 TCP 服务 基本配置参数： consecutive5xxErrors: 连续 5xx 错误异常数 interval: 错误异常的扫描间隔 (默认10秒) ，期间连续发生指定数量个异常则熔断 baseEjectionTime: 驱逐时间 (默认30秒) maxEjectionPercent: 最大驱逐百分比 (默认10%) minHealthPercent: 至少最低健康百分比的主机，就将启用异常检测。当负载平衡池中的正常主机百分比降至此阈值以下时，异常检测将被禁用默认值为0％，因为它通常不适用于每项服务只有少量 pod 的 k8s 环境 ","date":"2023-02-02","objectID":"/posts/istio/:7:3","tags":null,"title":"Istio 快速入门","uri":"/posts/istio/"},{"categories":["Istio"],"content":"故障注入 为了帮助我们提高服务的弹性，我们可以使用故障注入功能。我们可以在 HTTP 流量上应用故障注入策略，在转发目的地的请求时指定一个或多个故障注入 有两种类型的故障注入。我们可以在转发前延迟（delay）请求，模拟缓慢的网络或过载的服务，我们可以中止（abort） HTTP 请求，并返回一个特定的 HTTP 错误代码给调用者。通过中止，我们可以模拟一个有故障的上游服务 ","date":"2023-02-02","objectID":"/posts/istio/:8:0","tags":null,"title":"Istio 快速入门","uri":"/posts/istio/"},{"categories":["Istio"],"content":"配置网关 JWT 验证 Istio 支持使用 JWT 对终端用户进行身份验证，支持多种 JWT 签名算法，常见的 JWT 签名算法： HS256: 对称加密，加密和解密用的是同一个秘钥 RS256: RSA 私钥签名，公钥进行验证 ES256: 类似 RS256 Istio 要求提供 JWKS 格式的信息，用于 JWT 签名验证，是一个 JSON 格式的文件，如： { \"keys\":[ { \"e\":\"AQAB\", \"kid\":\"DHFbpoIUqrY8t2zpA2qXfCmr5VO5ZEr4RzHU_-envvQ\", \"kty\":\"RSA\", \"n\":\"xAE7eB6qugXyCAG3yhh7pkD...\" } ] } JWKS 描述一组 JWK 密钥。它能同时描述多个可用的公钥，应用场景之一是密钥的 Rotate。而 JWK，全称是 Json Web Key，它描述了一个加密密钥（公钥或私钥）的各项属性，包括密钥的值。Istio 使用 JWK 描述验证 JWT 签名所需要的信息。在使用 RSA 签名算法时，JWK 描述的应该是用于验证的 RSA 公钥。一个 RSA 公钥的 JWK 描述如下： { \"alg\": \"RS256\", # 算法「可选参数」 \"kty\": \"RSA\", # 密钥类型 \"use\": \"sig\", # 被用于签名「可选参数」 \"kid\": \"NjVBRjY5MDlCMUIwNzU4RTA2QzZFMDQ4QzQ2MDAyQjVDNjk1RTM2Qg\", # key 的唯一 id \"n\": \"yeNlzlub94YgerT030codqEztjfU_S6X4DbDA_iVKkjAWtYfPHDzz_sPCT1Axz6isZdf3lHpq_gYX4Sz-cbe4rjmigxUxr-FgKHQy3HeCdK6hNq9ASQvMK9LBOpXDNn7mei6RZWom4wo3CMvvsY1w8tjtfLb-yQwJPltHxShZq5-ihC9irpLI9xEBTgG12q5lGIFPhTl_7inA1PFK97LuSLnTJzW0bj096v_TMDg7pOWm_zHtF53qbVsI0e3v5nmdKXdFf9BjIARRfVrbxVxiZHjU6zL6jY5QJdh1QCmENoejj_ytspMmGW7yMRxzUqgxcAqOBpVm0b-_mW3HoBdjQ\", \"e\": \"AQAB\" } RSA 是基于大数分解的加密/签名算法，上述参数中，e 是公钥的模数(modulus)，n 是公钥的指数(exponent)，两个参数都是 base64 字符串。 JWK 中 RSA 公钥的具体定义参见 RSA Keys - JSON Web Algorithms (JWA) ","date":"2023-02-02","objectID":"/posts/istio/:9:0","tags":null,"title":"Istio 快速入门","uri":"/posts/istio/"},{"categories":["Istio"],"content":"JWK 生成 RS256 使用 RSA 算法进行签名，可通过如下命令生成 RSA 密钥： # 1. 生成 2048 位（不是 256 位）的 RSA 密钥 openssl genrsa -out rsa-private-key.pem 2048 # 2. 通过密钥生成公钥 openssl rsa -in rsa-private-key.pem -pubout -out rsa-public-key.pem 接下来使用 jwx 库生成 JWK func pubKey() []byte { f, _ := os.Open(\"./rsa-public-key.pem\") b, _ := io.ReadAll(f) return b } func main() { key, err := jwk.ParseKey(pubKey(), jwk.WithPEM(true)) if err != nil { log.Fatalln(err) } if rsaKey, ok := key.(jwk.RSAPublicKey); ok { b, err := json.Marshal(rsaKey) if err != nil { log.Fatalln(err) } fmt.Println(string(b)) } } 可以在 jwt.io 中测试密钥的可用性和生成 Token ","date":"2023-02-02","objectID":"/posts/istio/:9:1","tags":null,"title":"Istio 快速入门","uri":"/posts/istio/"},{"categories":["Istio"],"content":"定义 RequestAuthentication 启用 Istio 的身份验证 apiVersion: security.istio.io/v1beta1 kind: RequestAuthentication metadata: name: jwt-test namespace: istio-system spec: selector: matchLabels: istio: ingressgateway # 在带有这些 labels 的 ingressgateway/sidecar 上生效 jwtRules: # issuer 即签发者，需要和 JWT payload 中的 iss 属性完全一致 - issuer: \"user@virtuallain.com\" jwks: | { \"keys\": [ { \"e\":\"AQAB\", \"kty\":\"RSA\", \"n\":\"tFLKvS2EMOu3vgPnUPkdn5xVau9-dWf0z30_EdbpadQLiVsHH0FqWl-8CgtNtxnUjrI6WN__BMX8jLzvEqKrdZnbTMS0EaTh8lfGbFxNd0qziVHlYZTH-gtPNI4r815y9OuY7DEuR8fG-B_iHuCslN3BcJ4TDF_tzKCF0USGzzEiiRPR4SBtZgz0tmteQgRTv1NfciOwCtedEtXRKnGI5W1GV5u2dmF6UCiWJdgsqHMsVzTXJz_wliVvKhczwhrFZfqvdBoOe_aays89AjcO4x7eUntZVvOlkowaD-UeUeT6ZL8q4oTWGpswA4YNJ_daZmtAU5ho11EW6F3q1YHjVQ\" } ] } # jwks 或 jwksUri 二选其一 (测试中发现 jwksUri 会出现各种问题) # jwksUri: \"http://nginx.test.local/istio/jwks.json\" forwardOriginalToken: true # 转发 Authorization 请求头 outputPayloadToHeader: \"Userinfo\" # 转发 jwt payload 数据 (base64编码) 可以看到 jwtRules 是一个列表，因此可以为每个 issuers 配置不同的 jwtRule. 对同一个 issuers（jwt 签发者），可以通过 jwks 设置多个公钥，以实现JWT签名密钥的轮转。 JWT 的验证规则是： JWT 的 payload 中有 issuer 属性，首先通过 issuer 匹配到对应的 istio 中配置的 jwks。 JWT 的 header 中有 kid 属性，第二步在 jwks 的公钥列表中，中找到 kid 相同的公钥。 使用找到的公钥进行 JWT 签名验证。 配置中的 spec.selector 可以省略，这样会直接在整个 namespace 中生效，而如果是在 istio-system 名字空间，该配置将在全集群的所有 sidecar/ingressgateway 上生效 加了转发后，流程图如下： ","date":"2023-02-02","objectID":"/posts/istio/:9:2","tags":null,"title":"Istio 快速入门","uri":"/posts/istio/"},{"categories":["Istio"],"content":"跨域配置 给 vs 增加 corsPolicy 节点 apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: prodvs namespace: myistio spec: hosts: - prodsvc - p.virtuallain.com gateways: - pp-gateway - mesh http: - match: - uri: prefix: / route: - destination: host: prodsvc subset: v1svc port: number: 80 corsPolicy: allowOrigins: - exact: \"*\" allowMethods: - GET - POST - PATCH - PUT - DELETE - OPTIONS allowCredentials: true allowHeaders: - authorization maxAge: \"24h\" ","date":"2023-02-02","objectID":"/posts/istio/:9:3","tags":null,"title":"Istio 快速入门","uri":"/posts/istio/"},{"categories":["Istio"],"content":"定义 Envoy Filter RequestsAuthentication 不支持自定义响应头信息，这导致对于前后端分离的 Web API 而言， 一旦 JWT 失效，Istio 会直接将 401 返回给前端 Web 页面。 因为响应头中不包含 Access-Crontrol-Allow-Origin，响应将被浏览器拦截，需要通过 EnvoyFilter 自定义响应头，添加跨域信息 apiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata: name: reorder-cors-before-jwt namespace: istio-system spec: workloadSelector: labels: istio: ingressgateway configPatches: - applyTo: HTTP_FILTER match: context: GATEWAY listener: filterChain: filter: name: \"envoy.filters.network.http_connection_manager\" subFilter: name: \"envoy.filters.http.cors\" patch: operation: REMOVE - applyTo: HTTP_FILTER match: context: GATEWAY listener: filterChain: filter: name: \"envoy.filters.network.http_connection_manager\" subFilter: name: \"envoy.filters.http.jwt_authn\" patch: operation: INSERT_BEFORE value: name: \"envoy.filters.http.cors\" typed_config: \"@type\": \"type.googleapis.com/envoy.extensions.filters.http.cors.v3.Cors\" ","date":"2023-02-02","objectID":"/posts/istio/:9:4","tags":null,"title":"Istio 快速入门","uri":"/posts/istio/"},{"categories":["Istio"],"content":"Authorization Policy Istio 允许我们使用 AuthorizationPolicy (授权策略) 资源在网格、命名空间和工作负载层面定义访问控制。 支持 DENY、ALLOW、AUDIT 和 CUSTOM 操作。 rule 规则包括 from (来源)、to (操作) 和 when (条件) 来源 principals (如 my-service-account): 具有 my-service-account 的工作负载，需要开启 mTLS (双向认证) requestPrincipals (如 my-issuer/hello): 具有有效 JWT 和请求主体 my-issuer/hello 的工作负载 namespaces (如 default): 任何来自 default 命名空间的工作负载 ipBlocks (如 [“1.2.3.4”, “9.8.7.6/15”]): 任何 IP 或者 CIDR 块的 IP 的工作负载 remoteIpBlocks: 针对 remote.ip (如 X-Forwarded-For) 每个选项都有一个 notXxx 作为反义词 操作 hosts 和 notHosts ports 和 notPorts methods 和 notMethods path 和 notPath 所有这些操作都适用于请求属性。例如，要在一个特定的请求路径上进行匹配，我们可以使用路径 (如 [\"/api/*\", “/admin”]) 或特定的端口 ([“8080”]) 条件 为了指定条件，我们必须提供一个 key 字段，key 字段是一个 Istio 属性的名称。例如 request.headers、source.ip、request.auth.claims[xx] 等。条件的第二部分是 values 或 notValues 的字符串列表 apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: prod-authpolicy namespace: istio-system spec: action: ALLOW selector: matchLabels: istio: ingressgateway rules: - from: - source: requestPrincipals: [\"*\"] to: - operation: methods: [\"GET\"] paths: [\"/prods/*\"] - to: - operation: methods: [\"GET\",\"POST\"] paths: [\"/admin\"] when: # payload 中必须包含值为 admin 或 superadmin 的 role 字段 - key: request.auth.claims[role] values: [\"admin\",\"superadmin\"] ","date":"2023-02-02","objectID":"/posts/istio/:10:0","tags":null,"title":"Istio 快速入门","uri":"/posts/istio/"},{"categories":["Istio"],"content":"开启自动 mTLS 服务中的工作负载之间的通信是通过 Envoy 代理进行的。当一个工作负载使用 mTLS 向另一个工作负载发送请求时，Istio 会将流量重新路由到 sidecar 代理（Envoy） 然后，sidecar Envoy 开始与服务器端的 Envoy 进行 mTLS 握手。在握手过程中，调用者会进行安全命名检查，以验证服务器证书中的服务账户是否被授权运行目标服务。一旦 mTLS 连接建立，Istio 就会将请求从客户端的 Envoy 代理转发到服务器端的 Envoy 代理。在服务器端的授权后，sidecar 将流量转发到工作负载 可以创建 PeerAuthentication 资源，首先在每个命名空间中分别执行严格模式。然后，我们可以在根命名空间创建一个策略，在整个服务网格中执行该策略。或者指定 selector，仅应用于网格中的特定工作负载。它有三大模式： PERMISSIVE：同时接受未加密连接和双向加密连接 STRICT：只接受加密连接 DISABLE：关闭双向加密连接 apiVersion: security.istio.io/v1beta1 kind: PeerAuthentication metadata: name: testmtls namespace: myistio spec: selector: matchLabels: app: reviews mtls: mode: STRICT ","date":"2023-02-02","objectID":"/posts/istio/:11:0","tags":null,"title":"Istio 快速入门","uri":"/posts/istio/"},{"categories":["go"],"content":"数字证书是一个经证书授权中心数字签名的包含公开密钥拥有者信息以及公开密钥的文件 ","date":"2023-01-24","objectID":"/posts/go-ca-cert/:0:0","tags":null,"title":"使用 Go 生成自签 CA 证书","uri":"/posts/go-ca-cert/"},{"categories":["go"],"content":"使用 Go 自签发证书 go 的 x509 标准库下有个 Certificate 结构，这个结构就是证书解析后对应的实体，新证书需要先生成秘钥对，然后使用根证书的私钥进行签名，证书和私钥以及公钥这里使用的是pem编码方式 package main import ( \"crypto/rand\" \"crypto/rsa\" \"crypto/x509\" \"crypto/x509/pkix\" \"encoding/pem\" \"log\" \"math/big\" mathRand \"math/rand\" \"os\" \"time\" ) const ( CAFile = \"./test/certs/ca.crt\" // CA证书 CAKey = \"./test/certs/ca.key\" // CA私钥 ClientFile = \"./test/certs/lisi.pem\" // 客户端证书 ClientKey = \"./test/certs/lisi_key.pem\" // 客户端私钥 ) func main() { // 解析根证书 caFile, err := os.ReadFile(CAFile) if err != nil { log.Fatal(err) } caBlock, _ := pem.Decode(caFile) caCert, err := x509.ParseCertificate(caBlock.Bytes) // CA证书对象 if err != nil { log.Fatal(err) } // 解析私钥 keyFile, err := os.ReadFile(CAKey) if err != nil { log.Fatal(err) } keyBlock, _ := pem.Decode(keyFile) caPriKey, err := x509.ParsePKCS1PrivateKey(keyBlock.Bytes) // 私钥对象 if err != nil { log.Fatal(err) } // Go 提供了标准库 crypto/x509 给我们提供了 x509 签证的能力，我们可以先通过 x509.Certificate 构建证书签名请求 CSR 然后再进行签证 // 构建新的证书模板，里面的字段可以根据自己需求填写 certTemplate := \u0026x509.Certificate{ SerialNumber: big.NewInt(mathRand.Int63()), // 证书序列号 Subject: pkix.Name{ Country: []string{\"CN\"}, //Organization: []string{\"填的话这里可以用作用户组\"}, //OrganizationalUnit: []string{\"可填课不填\"}, Province: []string{\"beijing\"}, CommonName: \"lisi\", // CN Locality: []string{\"beijing\"}, }, NotBefore: time.Now(), // 证书有效期开始时间 NotAfter: time.Now().AddDate(1, 0, 0), // 证书有效期 BasicConstraintsValid: true, // 基本的有效性约束 IsCA: false, // 是否是根证书 ExtKeyUsage: []x509.ExtKeyUsage{x509.ExtKeyUsageClientAuth, x509.ExtKeyUsageServerAuth}, // 证书用途(客户端认证，数据加密) KeyUsage: x509.KeyUsageDigitalSignature | x509.KeyUsageDataEncipherment, EmailAddresses: []string{\"UserAccount@virtuallain.com\"}, } // 生成公私钥秘钥对 priKey, err := rsa.GenerateKey(rand.Reader, 2048) if err != nil { log.Fatal(err) } // 创建证书对象 clientCert, err := x509.CreateCertificate(rand.Reader, certTemplate, caCert, \u0026priKey.PublicKey, caPriKey) if err != nil { log.Fatal(err) } // 编码证书文件和私钥文件 clientCertPem := \u0026pem.Block{ Type: \"CERTIFICATE\", Bytes: clientCert, } clientCertFile, err := os.OpenFile(ClientFile, os.O_WRONLY|os.O_CREATE|os.O_TRUNC, 0600) if err != nil { log.Fatal(err) } err = pem.Encode(clientCertFile, clientCertPem) if err != nil { log.Fatal(err) } buf := x509.MarshalPKCS1PrivateKey(priKey) keyPem := \u0026pem.Block{ Type: \"PRIVATE KEY\", Bytes: buf, } clientKeyFile, _ := os.OpenFile(ClientKey, os.O_WRONLY|os.O_CREATE|os.O_TRUNC, 0600) err = pem.Encode(clientKeyFile, keyPem) if err != nil { log.Fatal(err) } } ","date":"2023-01-24","objectID":"/posts/go-ca-cert/:1:0","tags":null,"title":"使用 Go 生成自签 CA 证书","uri":"/posts/go-ca-cert/"},{"categories":["go"],"content":"使用证书请求k8s api 关联 roleBinding 后测试一下 curl --cert ./lisi.pem --key ./lisi_key.pem --cacert /etc/kubernetes/pki/ca.crt -s https://192.168.0.111:6443/api/v1/namespaces/default/pods ","date":"2023-01-24","objectID":"/posts/go-ca-cert/:2:0","tags":null,"title":"使用 Go 生成自签 CA 证书","uri":"/posts/go-ca-cert/"},{"categories":["k8s-go"],"content":"k8s 实现的“进入某个容器”的功能，底层本质是 Docker 容器通过 exec 进入容器的扩展。本质是新建了一个“与目标容器，共享 namespace 的”新的 shell 进程。所以该 shell 进程，看到的世界，就是容器内的世界了。 通过 client-go 提供的方法，实现通过网页进入 kubernetes 任意容器的终端操作 ","date":"2023-01-05","objectID":"/posts/k8s-pod-shell/:0:0","tags":null,"title":"k8s 远程进入容器 terminal","uri":"/posts/k8s-pod-shell/"},{"categories":["k8s-go"],"content":"remotecommand http://k8s.io/client-go/tools/remotecommand 是 kubernetes client-go 提供的 remotecommand 包，提供了方法与集群中的容器建立长连接，并设置容器的 stdin，stdout 等。 remotecommand 包提供基于 SPDY 协议的 Executor interface，进行和 pod 终端的流的传输。初始化一个 Executor 很简单，只需要调用 remotecommand 的 NewSPDYExecutor 并传入对应参数。 func main() { config, err := clientcmd.BuildConfigFromFlags(\"\", \"kubeconfig\") if err != nil { log.Fatal(err) } client, err := kubernetes.NewForConfig(config) if err != nil { log.Fatal(err) } option := \u0026coreV1.PodExecOptions{ Container: \"nginxtest\", // 容器名称 Command: []string{\"sh\", \"-c\", \"ls\"}, // 命令 Stdin: true, Stdout: true, Stderr: true, } req := client.CoreV1().RESTClient().Post().Resource(\"pods\"). Namespace(\"default\"). Name(\"myngx-79bdb4ccf8-nbln7\"). // pod名称 SubResource(\"exec\"). VersionedParams(option, scheme.ParameterCodec) // 这里初始化了一个 remote-cmd 的对象 exec, err := remotecommand.NewSPDYExecutor(config, \"POST\", req.URL()) if err != nil { log.Fatal(err) } // 这里开始，将输入输出，进行实时传递（Stream） err = exec.StreamWithContext(context.Background(), remotecommand.StreamOptions{ Stdin: os.Stdin, Stdout: os.Stdout, Stderr: os.Stderr, Tty: true, }) if err != nil { log.Fatal(err) } } 将 TTY 设置为 true，命令设置为 sh 进入容器交互式执行 option := \u0026coreV1.PodExecOptions{ Container: \"nginxtest\", Command: []string{\"sh\"}, Stdin: true, Stdout: true, Stderr: true, TTY: true, } ","date":"2023-01-05","objectID":"/posts/k8s-pod-shell/:1:0","tags":null,"title":"k8s 远程进入容器 terminal","uri":"/posts/k8s-pod-shell/"},{"categories":["k8s-go"],"content":"websocket Executor 的 StreamWithContext 方法，会建立一个流传输的连接，直到服务端和调用端一端关闭连接，才会停止传输。常用的做法是定义一个你想用的客户端，实现 Read(p []byte) (int, error) 和 Write(p []byte) (int, error) 方法即可，调用 Stream 方法时，只要将 StreamOptions 的 Stdin Stdout 都设置为该客户端，Executor 就会通过你定义的 write 和 read 方法来传输数据。 var Upgrader websocket.Upgrader func init() { Upgrader = websocket.Upgrader{ CheckOrigin: func(r *http.Request) bool { return true }, } } type WsShellClient struct { client *websocket.Conn } func NewWsShellClient(client *websocket.Conn) *WsShellClient { return \u0026WsShellClient{client: client} } // 实现 io.Writer func (this *WsShellClient) Write(p []byte) (n int, err error) { err = this.client.WriteMessage(websocket.TextMessage, p) if err != nil { return 0, err } return len(p), nil } // 实现 io.Reader func (this *WsShellClient) Read(p []byte) (n int, err error) { _, b, err := this.client.ReadMessage() if err != nil { return 0, err } return copy(p, string(b)+\"\\n\"), nil } func main() { r := gin.New() r.GET(\"/\", func(c *gin.Context) { wsClient, err := ws.Upgrader.Upgrade(c.Writer, c.Request, nil) if err != nil { log.Println(err) return } shellClient := ws.NewWsShellClient(wsClient) option := \u0026coreV1.PodExecOptions{ Container: \"nginxtest\", Command: []string{\"sh\"}, Stdin: true, Stdout: true, Stderr: true, TTY: true, } req := client.CoreV1().RESTClient().Post().Resource(\"pods\"). Namespace(\"default\"). Name(\"myngx-79bdb4ccf8-nbln7\"). SubResource(\"exec\"). VersionedParams(option, scheme.ParameterCodec) exec, err := remotecommand.NewSPDYExecutor(config, \"POST\", req.URL()) if err != nil { log.Println(err) } err = exec.StreamWithContext(c, remotecommand.StreamOptions{ Stdin: shellClient, Stdout: shellClient, Stderr: shellClient, Tty: true, }) if err != nil { log.Println(err) } }) r.Run(\":8080\") } 测试html示例 \u003c!DOCTYPE html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"UTF-8\"\u003e \u003ctitle\u003eTitle\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003cdiv\u003e \u003cdiv id=\"message\" style=\"width: 500px;height:300px;border:solid 1px gray;overflow:auto\"\u003e \u003c/div\u003e \u003cdiv\u003e \u003cinput type=\"type\" id=\"txtCmd\"/\u003e \u003cinput type=\"button\" id=\"cmdBtn\" value=\"发送\"/\u003e \u003cinput type=\"button\" onclick=\"document.getElementById('message').innerHTML=''\" value=\"清空\"/\u003e \u003c/div\u003e \u003c/div\u003e \u003cscript\u003e var ws = new WebSocket(\"ws://localhost:8080/\"); ws.onopen = function(){ console.log(\"open\"); } ws.onmessage = function(e){ let html=document.getElementById(\"message\").innerHTML; html+='\u003cp\u003e服务端消息:' + e.data + '\u003c/p\u003e' document.getElementById(\"message\").innerHTML=html } ws.onclose = function(e){ console.log(\"close\"); } ws.onerror = function(e){ console.log(e); } document.getElementById(\"cmdBtn\").onclick= ()=\u003e{ console.log(document.getElementById(\"txtCmd\").value) ws.send(document.getElementById(\"txtCmd\").value) } \u003c/script\u003e \u003c/body\u003e \u003c/html\u003e ","date":"2023-01-05","objectID":"/posts/k8s-pod-shell/:2:0","tags":null,"title":"k8s 远程进入容器 terminal","uri":"/posts/k8s-pod-shell/"},{"categories":["k8s-go"],"content":"xterm.js 前端页面使用 xterm.js 进行模拟terminal展示，只要 javascript 监听 Terminal 对象的对应事件及 websocket 连接的事件，进行对应的页面展示和消息推送就可以了。 ","date":"2023-01-05","objectID":"/posts/k8s-pod-shell/:3:0","tags":null,"title":"k8s 远程进入容器 terminal","uri":"/posts/k8s-pod-shell/"},{"categories":["k8s-go"],"content":"在 Kubernetes 中，有5个主要的组件，分别是 master 节点上的 kube-api-server、kube-controller-manager 和 kube-scheduler，node 节点上的 kubelet 和kube-proxy 。这其中 kube-apiserver 是对外和对内提供资源的声明式 API 的组件，其它4个组件都需要和它交互。为了保证消息的实时性，有两种方式： 客户端组件 (kubelet, scheduler, controller-manager 等) 轮询 apiserver apiserver 通知客户端 为了降低 kube-apiserver 的压力，有一个非常关键的机制就是 list-watch。list-watch 本质上也是 client 端监听 k8s 资源变化并作出相应处理的生产者消费者框架 list-watach 机制需要满足以下需求： 实时性 (即数据变化时，相关组件越快感知越好) 保证消息的顺序性 (即消息要按发生先后顺序送达目的组件。很难想象在Pod创建消息前收到该Pod删除消息时组件应该怎么处理) 保证消息不丢失或者有可靠的重新获取机制 (比如 kubelet 和 kube-apiserver 间网络闪断，需要保证网络恢复后kubelet可以收到网络闪断期间产生的消息) ","date":"2022-12-18","objectID":"/posts/k8s-list-watch/:0:0","tags":null,"title":"K8s list-watch 机制和 Informer 模块","uri":"/posts/k8s-list-watch/"},{"categories":["k8s-go"],"content":"list-watch 机制 list-watch 由两部分组成，分别是 list 和 watch。list 非常好理解，就是调用资源的 list API 罗列资源 ，基于 HTTP 短链接实现，watch 则是调用资源的 watch API 监听资源变更事件，基于 HTTP 长链接实现 etcd 存储集群的数据信息，apiserver 作为统一入口，任何对数据的操作都必须经过 apiserver。客户端通过 list-watch 监听 apiserver 中资源的 create, update 和 delete 事件，并针对事件类型调用相应的事件处理函数 ","date":"2022-12-18","objectID":"/posts/k8s-list-watch/:1:0","tags":null,"title":"K8s list-watch 机制和 Informer 模块","uri":"/posts/k8s-list-watch/"},{"categories":["k8s-go"],"content":"informer 机制 k8s 的 informer 模块封装 list-watch API，用户只需要指定资源，编写事件处理函数，AddFunc, UpdateFunc 和 DeleteFunc 等。如下图所示，informer 首先通过 list API 罗列资源，然后调用 watch API 监听资源的变更事件，并将结果放入到一个 FIFO 队列，队列的另一头有协程从中取出事件，并调用对应的注册函数处理事件。Informer 还维护了一个只读的 Map Store 缓存，主要为了提升查询的效率，降低 apiserver 的负载 ","date":"2022-12-18","objectID":"/posts/k8s-list-watch/:2:0","tags":null,"title":"K8s list-watch 机制和 Informer 模块","uri":"/posts/k8s-list-watch/"},{"categories":["k8s-go"],"content":"在 client-go 中的应用 client-go 使用 k8s.io/client-go/tools/cache 包里的 informer 对象进行 list-watch 机制的封装 最粗暴的解释： 初始化时，调 List API 获得全量 list，缓存起来(本地缓存)，这样就不需要每次请求都去请求 ApiServer 调用 Watch API 去 watch 资源，发生变更后会通过一定机制维护缓存 type DepHandler struct{} func (this *DepHandler) OnAdd(obj interface{}) {} func (this *DepHandler) OnUpdate(oldObj, newObj interface{}) { if dep, ok := newObj.(*v1.Deployment); ok { fmt.Println(dep.Name) } } func (this *DepHandler) OnDelete(obj interface{}) {} func main() { _, c := cache.NewInformer( // 监听 default 命名空间中 deployment 的变化 cache.NewListWatchFromClient(K8sClient.AppsV1().RESTClient(), \"deployments\", \"default\", fields.Everything()), \u0026v1.Deployment{}, 0, // 重新同步时间 \u0026DepHandler{}, // 实现类 ) c.Run(wait.NeverStop) select {} } ","date":"2022-12-18","objectID":"/posts/k8s-list-watch/:3:0","tags":null,"title":"K8s list-watch 机制和 Informer 模块","uri":"/posts/k8s-list-watch/"},{"categories":["k8s-go"],"content":"SharedInformerFactory sharedInformerFactory 用来构造各种 Informer 的工厂对象，它可以共享多个 informer 资源 informerFactory := informers.NewSharedInformerFactory(K8sClient, 0) // 构建一个 deployment informer depInformer := informerFactory.Apps().V1().Deployments() depInformer.Informer().AddEventHandler(\u0026DepHandler{}) informerFactory.Start(wait.NeverStop) select {} ","date":"2022-12-18","objectID":"/posts/k8s-list-watch/:3:1","tags":null,"title":"K8s list-watch 机制和 Informer 模块","uri":"/posts/k8s-list-watch/"},{"categories":["k8s-go"],"content":"示例 ","date":"2022-12-18","objectID":"/posts/k8s-list-watch/:4:0","tags":null,"title":"K8s list-watch 机制和 Informer 模块","uri":"/posts/k8s-list-watch/"},{"categories":["k8s-go"],"content":"监听 deployment // 全局对象，存储所有deployments var DepMapImpl *DeploymentMap func init() { DepMapImpl = \u0026DeploymentMap{Data: new(sync.Map)} } type DeploymentMap struct { Data *sync.Map // key:namespace value:[]*v1.Deployments } // 添加 func (this *DeploymentMap) Add(deployment *v1.Deployment) { if depList, ok := this.Data.Load(deployment.Namespace); ok { depList = append(depList.([]*v1.Deployment), deployment) this.Data.Store(deployment.Namespace, depList) } else { this.Data.Store(deployment.Namespace, []*v1.Deployment{deployment}) } } // 获取列表 func (this *DeploymentMap) ListByNs(namespace string) ([]*v1.Deployment, error) { if depList, ok := this.Data.Load(namespace); ok { return depList.([]*v1.Deployment), nil } return nil, fmt.Errorf(\"record not found\") } // 更新 func (this *DeploymentMap) Update(deployment *v1.Deployment) error { if depList, ok := this.Data.Load(deployment.Namespace); ok { depList := depList.([]*v1.Deployment) for i, dep := range depList { if dep.Name == deployment.Name { depList[i] = deployment break } } return nil } return fmt.Errorf(\"deployment [%s] not found\", deployment.Name) } // 删除 func (this *DeploymentMap) Delete(deployment *v1.Deployment) { if depList, ok := this.Data.Load(deployment.Namespace); ok { depList := depList.([]*v1.Deployment) for i, dep := range depList { if dep.Name == deployment.Name { newDepList := append(depList[:i], depList[i+1:]...) this.Data.Store(deployment.Namespace, newDepList) break } } } } // informer实现 type DepHandler struct{} func (this *DepHandler) OnAdd(obj interface{}) { DepMapImpl.Add(obj.(*v1.Deployment)) } func (this *DepHandler) OnUpdate(oldObj, newObj interface{}) { err := DepMapImpl.Update(newObj.(*v1.Deployment)) if err != nil { log.Println(err) } } func (this *DepHandler) OnDelete(obj interface{}) { DepMapImpl.Delete(obj.(*v1.Deployment)) } // 执行监听 func InitDeployments() { informerFactory := informers.NewSharedInformerFactory(K8sClient, 0) depInformer := informerFactory.Apps().V1().Deployments() depInformer.Informer().AddEventHandler(\u0026DepHandler{}) informerFactory.Start(wait.NeverStop) } ","date":"2022-12-18","objectID":"/posts/k8s-list-watch/:4:1","tags":null,"title":"K8s list-watch 机制和 Informer 模块","uri":"/posts/k8s-list-watch/"},{"categories":["k8s-go"],"content":"获取 deployment 的关联 pod 之前做过利用 Deployment 的 MatchLabels 去匹配 pod 的 labels 的方式。这次我们利用 ReplicaSet 的标签去匹配 Pod，这种方式可以区分当多个 Deployment 的 Pod 设置为相同标签的场景 当创建完 Deployment 后，k8s 会创建对应的 ReplicaSet，它会根据 template 里的内容进行 hash，然后自动设置一个标签 pod-template-hash，且与它管理的所有 Pod 标签相对应 Labels: app=xnginx pod-template-hash=767447889d 我们只需要通过 Deployment 获取它的 ReplicaSet，再拿 labels 去匹配 Pod 第一步：监听 Deployment、ReplicaSet 和 Pod，分别实现对应的 informer 方法，将数据缓存到本地 第二步：通过 Deployment 获取对应的 ReplicaSet，拿到 labels 关键代码： // 从本地缓存中取出所有的rs rsList, err := RSMapImpl.ListByNs(namespace) // 获取 labels labels, err := GetListWatchRsLabelByDeployment(deployment, rsList) // list-watch方式 根据deployment获取当前ReplicaSet的标签 func GetListWatchRsLabelByDeployment(deployment *v1.Deployment, rsList []*v1.ReplicaSet) (map[string]string, error) { for _, rs := range rsList { if IsCurrentRsByDeployment(rs, deployment) { selector, err := metaV1.LabelSelectorAsMap(rs.Spec.Selector) if err != nil { return nil, err } return selector, nil } } return nil, nil } // 判断rs是否对应当前deployment func IsCurrentRsByDeployment(set *v1.ReplicaSet, deployment *v1.Deployment) bool { if set.ObjectMeta.Annotations[\"deployment.kubernetes.io/revision\"] != deployment.ObjectMeta.Annotations[\"deployment.kubernetes.io/revision\"] { return false } for _, rf := range set.OwnerReferences { if rf.Kind == \"Deployment\" \u0026\u0026 rf.Name == deployment.Name { return true } } return false } 第三步：通过 labels 去匹配 pods 关键代码： // 根据标签获取Pod列表 func (this *PodMap) ListByLabels(ns string, labels map[string]string) ([]*v1.Pod, error) { ret := make([]*v1.Pod, 0) if podList, ok := this.Data.Load(ns); ok { podList := podList.([]*v1.Pod) for _, p := range podList { // 判断标签完全匹配 if reflect.DeepEqual(p.Labels, labels) { ret = append(ret, p) } } return ret, nil } return nil, fmt.Errorf(\"pods not found\") } ","date":"2022-12-18","objectID":"/posts/k8s-list-watch/:4:2","tags":null,"title":"K8s list-watch 机制和 Informer 模块","uri":"/posts/k8s-list-watch/"},{"categories":["k8s-go"],"content":"获取 Pod 状态和 Event Pod 状态信息包含： 阶段：Pod 的 status 字段是一个 PodStatus 对象，其中包含一个 phase 字段 取值 描述 Pending（悬决） Pod 已被 Kubernetes 系统接受，但有一个或者多个容器尚未创建亦未运行。此阶段包括等待 Pod 被调度的时间和通过网络下载镜像的时间。 Running（运行中） Pod 已经绑定到了某个节点，Pod 中所有的容器都已被创建。至少有一个容器仍在运行，或者正处于启动或重启状态。 Succeeded（成功） Pod 中的所有容器都已成功终止，并且不会再重启。 Failed（失败） Pod 中的所有容器都已终止，并且至少有一个容器是因为失败终止。也就是说，容器以非 0 状态退出或者被系统终止。 Unknown（未知） 因为某些原因无法取得 Pod 的状态。这种情况通常是因为与 Pod 所在主机通信失败。 状况：PodStatus 对象包含一个 PodConditions 数组 字段名称 描述 type Pod 状况的名称 status 表明该状况是否适用，可能的取值有 “True\"、\"False” 或 “Unknown” lastProbeTime 上次探测 Pod 状况时的时间戳 lastTransitionTime Pod 上次从一种状态转换到另一种状态时的时间戳 reason 机器可读的、驼峰编码（UpperCamelCase）的文字，表述上次状况变化的原因 message 人类可读的消息，给出上次状态转换的详细信息 PodScheduled：Pod 已经被调度到某节点 PodHasNetwork：Pod 沙箱被成功创建并且配置了网络（Alpha 特性，必须被显式启用） ContainersReady：Pod 中所有容器都已就绪 Initialized：所有的 Init 容器都已成功完成 Ready：Pod 可以为请求提供服务，并且应该被添加到对应服务的负载均衡池中 事件对象：为用户提供了洞察集群内发生的事情的能力。为了避免主节点磁盘空间被填满，将强制执行保留策略：事件在最后一次发生的一小时后将会被删除 关键代码： // EventMapImpl 全局对象，存储所有Event var EventMapImpl *EventMap func init() { EventMapImpl = \u0026EventMap{Data: new(sync.Map)} } type EventMap struct { Data *sync.Map // key:namespace_kind_name value: *v1.Event } func (this *EventMap) GetKey(event *v1.Event) string { key := fmt.Sprintf(\"%s_%s_%s\", event.Namespace, event.InvolvedObject.Kind, event.InvolvedObject.Name) return key } // Add 添加 func (this *EventMap) Add(event *v1.Event) { EventMapImpl.Data.Store(this.GetKey(event), event) } // Delete 删除 func (this *EventMap) Delete(event *v1.Event) { EventMapImpl.Data.Delete(this.GetKey(event)) } // 获取最新一条event message func (this *EventMap) GetMessage(ns string, kind string, name string) string { key := fmt.Sprintf(\"%s_%s_%s\", ns, kind, name) if v, ok := this.Data.Load(key); ok { return v.(*v1.Event).Message } return \"\" } // EventHandler informer实现 type EventHandler struct{} func (this *EventHandler) OnAdd(obj interface{}) { EventMapImpl.Add(obj.(*v1.Event)) } func (this *EventHandler) OnUpdate(oldObj, newObj interface{}) { EventMapImpl.Add(newObj.(*v1.Event)) } func (this *EventHandler) OnDelete(obj interface{}) { EventMapImpl.Delete(obj.(*v1.Event)) } // 评估Pod是否就绪 func GetPodIsReady(pod *coreV1.Pod) bool { for _, condition := range pod.Status.Conditions { if condition.Type == \"ContainersReady\" \u0026\u0026 condition.Status != \"True\" { return false } } for _, rg := range pod.Spec.ReadinessGates { for _, condition := range pod.Status.Conditions { if condition.Type == rg.ConditionType \u0026\u0026 condition.Status != \"True\" { return false } } } return true } // 获取pods DTO 把原生的 pod 对象转换为自己的实体对象 func GetPodsByLabels(ns string, labels []map[string]string) (pods []*model.PodModel) { podList, err := PodMapImpl.ListByLabels(ns, labels) lib.CheckError(err) pods = make([]*model.PodModel, len(podList)) for i, pod := range podList { pods[i] = \u0026model.PodModel{ Name: pod.Name, NodeName: pod.Spec.NodeName, Images: GetPodImages(pod.Spec.Containers), Phase: string(pod.Status.Phase), IsReady: GetPodIsReady(pod), Message: EventMapImpl.GetMessage(pod.Namespace, \"Pod\", pod.Name), CreatedAt: pod.CreationTimestamp.Format(\"2006-01-02 15:04:05\"), } } return } ","date":"2022-12-18","objectID":"/posts/k8s-list-watch/:4:3","tags":null,"title":"K8s list-watch 机制和 Informer 模块","uri":"/posts/k8s-list-watch/"},{"categories":["k8s-go"],"content":"client-go 是负责与 Kubernetes APIServer 服务进行交互的客户端库，利用 Client-Go 与 Kubernetes APIServer 进行的交互访问，来对 Kubernetes 中的各类资源对象进行管理操作，包括内置的资源对象及 CRD ","date":"2022-12-18","objectID":"/posts/k8s-go/:0:0","tags":null,"title":"K8s client-go","uri":"/posts/k8s-go/"},{"categories":["k8s-go"],"content":"client-go 客户端 Client-Go 共提供了 4 种与 Kubernetes APIServer 交互的客户端 RESTClient：最基础的客户端，主要是对 HTTP 请求进行了封装，支持 Json 和 Protobuf 格式的数据。 DiscoveryClient：发现客户端，负责发现 APIServer 支持的资源组、资源版本和资源信息的。 ClientSet：负责操作 Kubernetes 内置的资源对象，例如：Pod、Service等。 DynamicClient：动态客户端，可以对任意的 Kubernetes 资源对象进行通用操作，包括 CRD。 ","date":"2022-12-18","objectID":"/posts/k8s-go/:1:0","tags":null,"title":"K8s client-go","uri":"/posts/k8s-go/"},{"categories":["k8s-go"],"content":"基本使用 参考 API文档 的 group 和 apiVersion 等信息 ","date":"2022-12-18","objectID":"/posts/k8s-go/:2:0","tags":null,"title":"K8s client-go","uri":"/posts/k8s-go/"},{"categories":["k8s-go"],"content":"创建 admin ServiceAccount kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: admin annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\" roleRef: kind: ClusterRole name: cluster-admin apiGroup: rbac.authorization.k8s.io subjects: - kind: ServiceAccount name: admin namespace: kube-system --- apiVersion: v1 kind: ServiceAccount metadata: name: admin namespace: kube-system labels: kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcile 查看 token $ kubectl describe sa admin -n kube-system Name: admin Namespace: kube-system Tokens: admin-token-nzxlb $ kubectl describe secret admin-token-nzxlb -n kube-system ","date":"2022-12-18","objectID":"/posts/k8s-go/:2:1","tags":null,"title":"K8s client-go","uri":"/posts/k8s-go/"},{"categories":["k8s-go"],"content":"建立连接 先使用反代的方式，在 master 节点执行 $ kubectl proxy --address='0.0.0.0' --accept-hosts='^*$' --port=8009 安装客户端库 (版本要对应 Github)，连接 API Server var K8sClient *kubernetes.Clientset func init() { config := \u0026rest.Config{ Host: \"ip:8009\", BearerToken: \"\", } client, err := kubernetes.NewForConfig(config) if err != nil { log.Fatalln(err) } K8sClient = client } ","date":"2022-12-18","objectID":"/posts/k8s-go/:2:2","tags":null,"title":"K8s client-go","uri":"/posts/k8s-go/"},{"categories":["k8s-go"],"content":"示例：获取资源列表 ctx := context.Background() // 查询 kube-system 命名空间下的 service svs, _ := K8sClient.CoreV1().Services(\"kube-system\").List(ctx, v1.ListOptions{}) // 查询 kube-system 命名空间下的 deployment deps, _ := K8sClient.AppsV1().Deployments(\"kube-system\").List(ctx, v1.ListOptions{}) ","date":"2022-12-18","objectID":"/posts/k8s-go/:2:3","tags":null,"title":"K8s client-go","uri":"/posts/k8s-go/"},{"categories":["k8s-go"],"content":"示例：获取资源详情 ctx := context.Background() // 获取名称为 ngx 的 deployment 资源 dep, _ := K8sClient.AppsV1().Deployments(namespace).Get(ctx, \"ngx\", metav1.GetOptions{}) dep.Name // 名称 dep.Namespace // 命名空间 dep.Status.Replicas // 副本数量 dep.CreationTimestamp // 创建时间 dep.Spec.Template.Spec.Containers[0].Image // 第一个镜像名称 ","date":"2022-12-18","objectID":"/posts/k8s-go/:2:4","tags":null,"title":"K8s client-go","uri":"/posts/k8s-go/"},{"categories":["k8s-go"],"content":"示例：获取 deployment 的关联 pod 最简单的方式，利用 Deployment 的 MatchLabels 去匹配 pod 的 labels type PodModel struct { Name string // pod名称 NodeName string // 节点 Images string // 镜像名称 CreatedAt string // 创建时间 } // 拼接labels字符串 func GetLabels(labels map[string]string) string { var labelStr strings.Builder for k, v := range labels { if labelStr.Len() != 0 { labelStr.WriteString(\",\") } labelStr.WriteString(fmt.Sprintf(\"%s=%s\", k, v)) } return labelStr.String() } // 根据deployment获取关联的pods集合 func GetPodsByDep(namespace string, dep *v1.Deployment) (pods []*PodModel) { ctx := context.Background() // 通过LabelSelector去匹配对应的pods listOpt := metav1.ListOptions{ LabelSelector: GetLabels(dep.Spec.Selector.MatchLabels)， } podList, _ := K8sClient.CoreV1().Pods(namespace).List(ctx, listOpt) pods = make([]*PodModel, len(podList.Items)) for i, pod := range podList.Items { pods[i] = \u0026PodModel{ Name: pod.Name, NodeName: pod.Spec.NodeName, Images: GetPodImages(pod.Spec.Containers), CreatedAt: pod.CreationTimestamp.Format(\"2006-01-02 15:04:05\"), } } return } dep, _ := K8sClient.AppsV1().Deployments(namespace).Get(context.Background(), \"ngx\", metav1.GetOptions{}) Pods = GetPodsByDep(\"default\", dep) ","date":"2022-12-18","objectID":"/posts/k8s-go/:2:5","tags":null,"title":"K8s client-go","uri":"/posts/k8s-go/"},{"categories":["k8s-go"],"content":"示例：修改 deployment 副本数量 // 获取 deployment 副本数量 scale, _ := K8sClient.AppsV1().Deployments(\"default\").GetScale(ctx, \"ngx\", v1.GetOptions{}) // 修改副本数量 scale.Spec.Replicas++ K8sClient.AppsV1().Deployments(\"default\").UpdateScale(ctx, \"ngx\", scale, v1.UpdateOptions{}) ","date":"2022-12-18","objectID":"/posts/k8s-go/:2:6","tags":null,"title":"K8s client-go","uri":"/posts/k8s-go/"},{"categories":["k8s-go"],"content":"示例：创建资源 根据 yaml 创建一个 nginx deployment apiVersion: apps/v1 kind: Deployment metadata: name: myngx namespace: default spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: nginxtest image: nginx:1.18-alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80 ngxDep := \u0026appV1.Deployment{} // 读取yaml内容 b, _ := os.ReadFile(\"nginx.yaml\") ngxJson, _ := yaml.ToJSON(b) json.Unmarshal(ngxJson, ngxDep) dep, _ := K8sClient.AppsV1().Deployments(\"default\").Create(context.Background(), ngxDep, v1.CreateOptions{}) ","date":"2022-12-18","objectID":"/posts/k8s-go/:2:7","tags":null,"title":"K8s client-go","uri":"/posts/k8s-go/"},{"categories":["kubernetes"],"content":"Kube-scheduler 是 Kubernetes 集群默认的调度器，并且是控制面中一个核心组件。scheduler 通过 kubernetes 的监测（Watch）机制来发现集群中新创建且尚未被调度到 Node 上的 Pod。 scheduler 会将发现的每一个未调度的 Pod 调度到一个合适的 Node 上来运行。 scheduler会依据下文的调度原则来做出调度选择。 对于新创建的 pod 或其他未调度的 pod来讲，kube-scheduler 选择一个最佳节点供它们运行。但是，Pod 中的每个容器对资源的要求都不同，每个 Pod 也有不同的要求。因此，需要根据具体的调度要求对现有节点进行过滤。 在Kubernetes集群中，满足 Pod 调度要求的节点称为可行节点 （feasible nodes FN） 。如果没有合适的节点，则 pod 将保持未调度状态，直到调度程序能够放置它。也就是说，当我们创建 Pod 时，如果长期处于 Pending 状态，这个时候应该看你的集群调度器是否因为某些问题没有合适的节点了 调度器为 Pod 找到 FN 后，然后运行一组函数对 FN 进行评分，并在 FN 中找到得分最高的节点来运行 Pod。 调度策略在决策时需要考虑的因素包括个人和集体资源需求、硬件/软件/策略约束 （constraints）、亲和性 (affinity) 和反亲和性（ anti-affinity ）规范、数据局部性、工作负载间干扰等。 基本调度流程： 发布 Pod ControllerManager 会把 Pod 加入待调度队列 kube-scheduler 决定调度到哪个 node，然后写入 etcd 被选中节点中的 kubelet 开始工作（pull image、启动 Pod） ","date":"2022-12-13","objectID":"/posts/kube-schedule/:0:0","tags":null,"title":"调度器 kube-schedule","uri":"/posts/kube-schedule/"},{"categories":["kubernetes"],"content":"如何为 Pod 选择节点？ kube-scheduler 给一个 Pod 做调度选择时包含两个步骤： 过滤 (Filtering) 打分 (Scoring) 过滤也被称为预选 （Predicates），该步骤会找到可调度的节点集，然后通过是否满足特定资源的请求，例如通过 PodFitsResources 过滤器检查候选节点是否有足够的资源来满足 Pod 资源的请求。这个步骤完成后会得到一个包含合适的节点的列表（通常为多个），如果列表为空，则Pod不可调度。 打分也被称为优选（Priorities），在该步骤中，会对上一个步骤的输出进行打分，Scheduer 通过打分的规则为每个通过 Filtering 步骤的节点计算出一个分数。 完成上述两个步骤之后，kube-scheduler 会将Pod分配给分数最高的 Node，如果存在多个相同分数的节点，会随机选择一个。 ","date":"2022-12-13","objectID":"/posts/kube-schedule/:1:0","tags":null,"title":"调度器 kube-schedule","uri":"/posts/kube-schedule/"},{"categories":["kubernetes"],"content":"将 Pod 指派给节点 你可以约束一个 Pod 以便限制其只能在特定的节点上运行，或优先在特定的节点上运行。有几种方法可以实现这点，推荐的方法都是用标签选择算符来进行选择。 通常这样的约束不是必须的，因为调度器将自动进行合理的放置，但在某些情况下，你可能需要进一步控制 Pod 被部署到哪个节点。 给节点 lain1 添加一个标签 disktype=ssd $ kubectl label nodes lain1 disktype=ssd # 删除标签 $ kubectl label nodes lain1 disktype- ","date":"2022-12-13","objectID":"/posts/kube-schedule/:2:0","tags":null,"title":"调度器 kube-schedule","uri":"/posts/kube-schedule/"},{"categories":["kubernetes"],"content":"nodeSelector 设置你希望目标节点所具有的节点标签。 apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent nodeSelector: disktype: ssd # 该Pod将被调度到有disktype=ssd标签的节点 ","date":"2022-12-13","objectID":"/posts/kube-schedule/:2:1","tags":null,"title":"调度器 kube-schedule","uri":"/posts/kube-schedule/"},{"categories":["kubernetes"],"content":"节点亲和性 nodeAffinity 节点亲和性有两种： requiredDuringSchedulingIgnoredDuringExecution：调度器只有在规则被满足的时候才能执行调度 preferredDuringSchedulingIgnoredDuringExecution：调度器会尝试寻找满足对应规则的节点。如果找不到匹配的节点，调度器仍然会调度该 Pod 强制的节点亲和性调度 下面的 pod 只会调度到具有 disktype=ssd 标签的节点上 apiVersion: v1 kind: Pod metadata: name: nginx spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: disktype operator: In values: - ssd containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent 首选的节点亲和性调度 apiVersion: v1 kind: Pod metadata: name: nginx spec: affinity: nodeAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 # 权重 preference: matchExpressions: - key: disktype operator: In values: - ssd containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent ","date":"2022-12-13","objectID":"/posts/kube-schedule/:2:2","tags":null,"title":"调度器 kube-schedule","uri":"/posts/kube-schedule/"},{"categories":["kubernetes"],"content":"节点污点和容忍度 节点亲和性使 Pod 被吸引到一类特定的节点，污点 (Taint) 则相反，它使节点能够排斥一类特定的 Pod。污点有三种类型： NoSchedule：不会将 Pod 调度到该节点 PreferNoSchedule：尽量避免将 Pod 调度到该节点上 NoExecute：任何不能忍受这个污点的 Pod 都会马上被驱逐 一些内置的污点： node.kubernetes.io/not-ready：节点未准备好。这相当于节点状况 Ready 的值为 False node.kubernetes.io/unreachable：节点控制器访问不到节点. 这相当于节点状况 Ready 的值为 Unknown node.kubernetes.io/memory-pressure：节点存在内存压力 node.kubernetes.io/disk-pressure：节点存在磁盘压力 node.kubernetes.io/pid-pressure: 节点的 PID 压力 node.kubernetes.io/network-unavailable：节点网络不可用 node.kubernetes.io/unschedulable: 节点不可调度 node.cloudprovider.kubernetes.io/uninitialized：如果 kubelet 启动时指定了一个“外部”云平台驱动， 它将给当前节点添加一个污点将其标志为不可用。在 cloud-controller-manager 的一个控制器初始化这个节点后，kubelet 将删除这个污点 容忍度 (Toleration) 是应用于 Pod 上的。容忍度允许调度器调度带有对应污点的 Pod。 容忍度允许调度但并不保证调度：作为其功能的一部分， 调度器也会评估其他参数。 # 查看节点的污点 $ kubectl describe node lain1 | grep Taints # 给节点打一个污点 $ kubectl taint nodes lain1 key1=value1:NoSchedule # 删除污点 $ kubectl taint node lain1 key1:NoSchedule- 使用容忍 apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent tolerations: - key: \"key1\" # 对应污点key operator: \"Equal\" value: \"value1\" effect: \"NoSchedule\" ","date":"2022-12-13","objectID":"/posts/kube-schedule/:2:3","tags":null,"title":"调度器 kube-schedule","uri":"/posts/kube-schedule/"},{"categories":["kubernetes"],"content":"Pod 亲和性 Pod 间亲和性与反亲和性使你可以基于已经在节点上运行的 Pod 的标签来约束 Pod 可以调度到的节点，而不是基于节点上的标签。 Pod 亲和性和反亲和性都需要相当的计算量，因此会在大规模集群中显著降低调度速度。 不建议在包含数百个节点的集群中使用这类设置 与节点亲和性类似，Pod 的亲和性与反亲和性也有两种类型： requiredDuringSchedulingIgnoredDuringExecution preferredDuringSchedulingIgnoredDuringExecution 实例资源清单 apiVersion: apps/v1 kind: Deployment metadata: name: ngx1 spec: selector: matchLabels: app: ngx1 replicas: 1 template: metadata: labels: app: ngx1 spec: nodeName: lain1 containers: - name: ngx1 image: nginx:1.18-alpine imagePullPolicy: IfNotPresent 下面这个 Pod 必须调度到具有 disktype 标签的节点上，并且集群中至少有一个位于该可用区的节点上运行着带有 app=ngx1 标签的 Pod apiVersion: apps/v1 kind: Deployment metadata: name: ngx2 spec: selector: matchLabels: app: ngx2 replicas: 1 template: metadata: labels: app: ngx2 spec: affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - ngx1 topologyKey: disktype containers: - name: ngx2 image: nginx:1.18-alpine imagePullPolicy: IfNotPresent ","date":"2022-12-13","objectID":"/posts/kube-schedule/:2:4","tags":null,"title":"调度器 kube-schedule","uri":"/posts/kube-schedule/"},{"categories":["kubernetes"],"content":"Horizontal Pod Autoscaling（Pod 水平自动伸缩），简称HPA。它可以基于 CPU 利用率或其他指标自动扩缩 ReplicationController、Deployment 和 ReplicaSet 中的 Pod 数量，它不适用于无法扩缩的对象，比如 DaemonSet。除了 CPU 利用率，也可以基于其他应程序提供的自定义度量指标来执行自动扩缩 文档：Pod 水平自动扩缩 | Kubernetes 我们可以简单的通过 kubectl autoscale 命令来创建一个 HPA 资源对象，HPA Controller 默认 30s 轮询一次（可通过 kube-controller-manager 的 --horizontal-pod-autoscaler-sync-period 参数进行设置)，查询指定的资源中的 Pod 资源使用率，并且与创建时设定的值和指标做对比，从而实现自动伸缩的功能。 ","date":"2022-12-12","objectID":"/posts/hpa/:0:0","tags":null,"title":"Pod 水平自动扩缩 — HPA","uri":"/posts/hpa/"},{"categories":["kubernetes"],"content":"Metrics Server Metrics Server 可以通过标准的 Kubernetes Summary API 把监控数据暴露出来，有了 Metrics Server 之后，就可以采集节点和 Pod 的内存、磁盘、CPU 和网络的使用率等。Metrics API URI 为 /apis/metrics.k8s.io/ ","date":"2022-12-12","objectID":"/posts/hpa/:1:0","tags":null,"title":"Pod 水平自动扩缩 — HPA","uri":"/posts/hpa/"},{"categories":["kubernetes"],"content":"安装 可以通过官方仓库的资源清单安装： Github 部署之前，需要修改 k8s.gcr.io/metrics-server/metrics-server 镜像的地址 # image: k8s.gcr.io/metrics-server/metrics-server:v0.4.1 image: bitnami/metrics-server:0.4.1 等待部署完成后，可以查看 pod 日志是否正常 $ kubectl get pods -n kube-system -l k8s-app=metrics-server NAME READY STATUS RESTARTS AGE metrics-server-7d8467779f-vgtzb 1/1 Running 0 18m $ kubectl logs -f metrics-server-7d8467779f-vgtzb -n kube-system ","date":"2022-12-12","objectID":"/posts/hpa/:1:1","tags":null,"title":"Pod 水平自动扩缩 — HPA","uri":"/posts/hpa/"},{"categories":["kubernetes"],"content":"查看 $ kubectl top nodes NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% lain1 149m 3% 3526Mi 45% lain2 208m 10% 2786Mi 75% $ kubectl top pod etcd-lain1 -n kube-system NAME CPU(cores) MEMORY(bytes) etcd-lain1 20m 249Mi ","date":"2022-12-12","objectID":"/posts/hpa/:1:2","tags":null,"title":"Pod 水平自动扩缩 — HPA","uri":"/posts/hpa/"},{"categories":["kubernetes"],"content":"使用 HPA 的 API 有三个版本，在当前稳定版本 autoscaling/v1 中只支持基于 CPU 指标的缩放。在 Beta 版本 autoscaling/v2beta2，引入了基于内存和自定义指标的缩放。 $ kubectl api-versions | grep autoscal autoscaling/v1 # 只支持通过cpu伸缩 autoscaling/v2beta1 # 支持通过cpu、内存和自定义数据来进行伸缩 autoscaling/v2beta2 我们部署一个测试 api，执行一些 CUP 密集型计算，然后利用 HAP 来进行自动伸缩容 test := map[string]string{ \"str\": \"requests来设置各容器需要的最小资源\", } r := gin.New() r.GET(\"/\", func(context *gin.Context) { ret := 0 for i := 0; i \u003c= 1000000; i++ { t := map[string]string{} b, _ := json.Marshal(test) _ = json.Unmarshal(b, t) ret++ } context.JSON(200, gin.H{\"message\": ret}) }) r.Run(\":8080\") 资源清单如下 apiVersion: apps/v1 kind: Deployment metadata: name: web1 spec: selector: matchLabels: app: myweb replicas: 1 template: metadata: labels: app: myweb spec: nodeName: lain1 containers: - name: web1test image: alpine:3.12 imagePullPolicy: IfNotPresent command: [\"/app/stress\"] volumeMounts: - name: app mountPath: /app resources: requests: cpu: \"200m\" memory: \"256Mi\" limits: cpu: \"400m\" # 1物理核=1000个微核(millicores) 1000m=1CPU memory: \"512Mi\" ports: - containerPort: 8080 volumes: - name: app hostPath: path: /home/txl/goapi --- apiVersion: v1 kind: Service metadata: name: web1 spec: type: ClusterIP ports: - port: 80 targetPort: 8080 selector: app: myweb requests 节点用来设置各容器需要的最小资源 limits 节点用于限制运行时容器占用的资源 ","date":"2022-12-12","objectID":"/posts/hpa/:2:0","tags":null,"title":"Pod 水平自动扩缩 — HPA","uri":"/posts/hpa/"},{"categories":["kubernetes"],"content":"创建 现在创建一个 HPA 资源对象，可以使用命令创建 $ kubectl autoscale deployment web1 --min=1 --max=5 --cpu-percent=20 $ kubectl get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE web1 Deployment/web1 0%/20% 1 5 1 12m 此命令创建了一个关联资源 web1 的 HPA，最小的 Pod 副本数为1，最大为5。HPA 会根据设定的 cpu 使用率（20%）动态的增加或者减少 Pod 数量。 也可以使用 yaml 来创建 apiVersion: autoscaling/v2beta2 kind: HorizontalPodAutoscaler metadata: name: web1hpa namespace: default spec: minReplicas: 1 maxReplicas: 5 scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: web1 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 50 # 使用率 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 50 你还可以指定资源度量指标使用绝对数值，而不是百分比，你需要将 target.type 从 Utilization 替换成 AverageValue，同时设置 target.averageValue 而非 target.averageUtilization 的值 metrics: - type: Resource resource: name: cpu target: type: AverageValue averageValue: 230m # 使用量 - type: Resource resource: name: memory target: type: AverageValue averageValue: 400m ","date":"2022-12-12","objectID":"/posts/hpa/:2:1","tags":null,"title":"Pod 水平自动扩缩 — HPA","uri":"/posts/hpa/"},{"categories":["kubernetes"],"content":"压测 $ sudo yum -y install httpd-tools $ ab -n 10000 -c 10 http://web1/ 可以看到，HPA 已经开始工作，副本数量已经从原来的1变成了4个 $ kubectl get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE web1 Deployment/web1 192%/20% 1 5 4 107s 查看 HPA 资源工作过程 $ kubectl describe hpa web1 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulRescale 20m horizontal-pod-autoscaler New size: 4; reason: cpu resource utilization (percentage of request) above target Normal SuccessfulRescale 20m horizontal-pod-autoscaler New size: 5; reason: cpu resource utilization (percentage of request) above target Normal SuccessfulRescale 14m horizontal-pod-autoscaler New size: 1; reason: All metrics below target ","date":"2022-12-12","objectID":"/posts/hpa/:2:2","tags":null,"title":"Pod 水平自动扩缩 — HPA","uri":"/posts/hpa/"},{"categories":["kubernetes"],"content":"可能出现的错误 ","date":"2022-12-12","objectID":"/posts/hpa/:3:0","tags":null,"title":"Pod 水平自动扩缩 — HPA","uri":"/posts/hpa/"},{"categories":["kubernetes"],"content":"Pod 启动异常 x509: cannot validate certificate for 192.168.0.111 because it doesn’t contain any ip sans node=“lain1” 因为 Kubelet 证书需要由群集证书颁发机构签名 ，或者给 Metrics Server 增加配置参数 –Kubelet-insecure-tls 来禁用证书验证 解决这个问题的方法是使用 APIServer 签署 Kubelet 证书。 首先编辑 kube-system namespace 中的 kubelet-config ConfigMap，在 kind: KubeletConfiguration 下方增加内容 serverTLSBootstrap: true 然后分别为每个节点上修改 kubelet-config configmap $ sudo vi /var/lib/kubelet/config.yaml # 在 kind: KubeletConfiguration 下方增加内容 serverTLSBootstrap: true 然后重启 kubelet $ systemctl restart kubelet Kubelet 都会生成一个 CSR 并将其提交给 APIServer，您需要为集群上的每个 Kubelet 批准 CSR $ kubectl get csr NAME AGE SIGNERNAME REQUESTOR CONDITION csr-clwz7 44m kubernetes.io/kubelet-serving system:node:lain1 Approved,Issued csr-w6kpf 34m kubernetes.io/kubelet-serving system:node:lain2 Approved,Issued $ kubectl certificate approve csr-clwz7 csr-w6kpf 默认情况下，这些服务证书将在一年后过期。因此，一年后，Kubelet 将生成一个新的 CSR，您需要批准它 参考：https://particule.io/en/blog/kubeadm-metrics-server ","date":"2022-12-12","objectID":"/posts/hpa/:3:1","tags":null,"title":"Pod 水平自动扩缩 — HPA","uri":"/posts/hpa/"},{"categories":["kubernetes"],"content":"为了能够屏蔽底层存储实现的细节，方便用户使用，k8s 引入 PV 和 PVC 两种资源对象。Persistent Volume 提供存储资源（并实现），Persistent Volume Claim 描述需要的存储标准，然后从现有 PV 中匹配或者动态建立新的资源，最后将两者进行绑定。 ","date":"2022-12-11","objectID":"/posts/pv_pvc/:0:0","tags":null,"title":"PV 和 PVC","uri":"/posts/pv_pvc/"},{"categories":["kubernetes"],"content":"持久卷（Persistent Volume） PV 是持久化卷的意思，是对底层的共享存储的一种抽象。一般情况下 PV 由 k8s 管理员进行创建和配置，它与底层具体的共享存储技术有关，并通过插件（如：local、NFS）等具体的底层技术来实现完成与共享存储的对接。 ","date":"2022-12-11","objectID":"/posts/pv_pvc/:1:0","tags":null,"title":"PV 和 PVC","uri":"/posts/pv_pvc/"},{"categories":["kubernetes"],"content":"持久卷的类型 cephfs - CephFS volume csi - 容器存储接口 (CSI) fc - Fibre Channel (FC) 存储 hostPath - HostPath 卷 （仅供单节点测试使用；不适用于多节点集群；请尝试使用 local 卷作为替代） iscsi - iSCSI (SCSI over IP) 存储 local - 节点上挂载的本地存储设备 nfs - 网络文件系统 (NFS) 存储 rbd - Rados 块设备 (RBD) 卷 ","date":"2022-12-11","objectID":"/posts/pv_pvc/:1:1","tags":null,"title":"PV 和 PVC","uri":"/posts/pv_pvc/"},{"categories":["kubernetes"],"content":"创建 使用 local 卷的资源清单 apiVersion: v1 kind: PersistentVolume metadata: name: local-pv spec: capacity: storage: 1Gi volumeMode: Filesystem accessModes: - ReadWriteOnce storageClassName: \"\" # 存储类别 persistentVolumeReclaimPolicy: Retain local: path: /home/txl/data nodeAffinity: required: # 指定必须满足的硬性节点约束 nodeSelectorTerms: # 节点选择器条件的列表 - matchExpressions: # 基于节点标签所设置的节点选择器要求的列表 - key: pv # 适用的标签主键 operator: In # 代表主键与值集之间的关系 values: - local ","date":"2022-12-11","objectID":"/posts/pv_pvc/:1:2","tags":null,"title":"PV 和 PVC","uri":"/posts/pv_pvc/"},{"categories":["kubernetes"],"content":"关键配置参数 储存能力 capacity：目前只支持存储空间的设置（storage=1Gi） 卷模式 volumeMode：设置为 Filesystem 的卷会被 Pod 挂载（Mount）到某个目录。 如果卷的存储来自某块设备而该设备目前为空，Kuberneretes 会在第一次挂载卷之前在设备上创建文件系统 访问模式 accessModes：用于描述用户应用对存储资源的访问权限，访问权限包括下面几种方式： ReadWriteOnce（RWO）：读写权限，但是只能被单个节点挂载 ReadOnlyMany（ROX）：只读权限，可以被多个节点挂载 ReadWriteMany（RWX）：读写权限，可以被多个节点挂载 存储类别 storageClassName：PV 可以通过 storageClassName 参数指定一个存储类别： 具有特定类别的 PV 只能与请求了该类别的 PVC 进行绑定 未设定类别的 PV 只能与不请求任何类别的 PVC 进行绑定 回收策略 persistentVolumeReclaimPolicy：当 PV 不再被使用了之后，对其的处理方式。目前支持三种策略： Retain（保留）：保留数据，需要管理员手动清理数据 Recycle（回收）：清除PV中的数据，效果相当于执行 rm -rf /thevolume/* Delete（删除）：与PV相连的后端存储完成volume的删除操作，当然这常见于云服务商的存储服务 节点亲和性 NodeAffinity：定义一些约束，进而限制从哪些节点上可以访问此卷。matchExpressions 的 operator包括： In：label 的值在某个列表中 NotIn：label 的值不在某个列表中 Exists：某个 label 存在 DoesNotExist：某个 label 不存在 Gt：label 的值大于某个值（字符串比较） Lt：label 的值小于某个值（字符串比较） 状态 status：一个 PV 的生命周期中，可能会处于4种不同的阶段 Available（可用）：表示可用状态，还未被任何PVC绑定 Bound（已绑定）：表示PV已经被PVC绑定 Released（已释放）：表示PVC被删除，但是资源还未被集群重新声明 Failed（失败）：表示该PV的自动回收失败 ","date":"2022-12-11","objectID":"/posts/pv_pvc/:1:3","tags":null,"title":"PV 和 PVC","uri":"/posts/pv_pvc/"},{"categories":["kubernetes"],"content":"标签 查看标签 $ kubectl get node --show-labels=true 给 node lain1 打一个标签 pv=local $ kubectl label nodes lain1 pv=local 删除标签 $ kubectl label nodes lain1 pv- ","date":"2022-12-11","objectID":"/posts/pv_pvc/:1:4","tags":null,"title":"PV 和 PVC","uri":"/posts/pv_pvc/"},{"categories":["kubernetes"],"content":"持久卷声明（PersistentVolumeClaim） PVC 是资源的申请，用来声明对存储空间、访问模式、存储类别需求信息 ","date":"2022-12-11","objectID":"/posts/pv_pvc/:2:0","tags":null,"title":"PV 和 PVC","uri":"/posts/pv_pvc/"},{"categories":["kubernetes"],"content":"创建 绑定：spec 关键字段要匹配，storageClassName 字段必须一致 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: ngx-pvc spec: accessModes: - ReadWriteOnce storageClassName: \"\" resources: requests: storage: 1Gi ","date":"2022-12-11","objectID":"/posts/pv_pvc/:2:1","tags":null,"title":"PV 和 PVC","uri":"/posts/pv_pvc/"},{"categories":["kubernetes"],"content":"存储类（StorageClass） Kubernetes 提供了一套可以自动创建 PV 的机制，即：Dynamic Provisioning。而这个机制的核心在于StorageClass 这个 API 对象。StorageClass 对象会定义下面两部分内容: PV 的属性，如存储类型，Volume 的大小等。 创建这种 PV 需要用到的存储插件，即存储制备器。 有了这两个信息之后，Kubernetes 就能够根据用户提交的 PVC，找到一个对应的 StorageClass，之后Kubernetes 就会调用该 StorageClass 声明的存储插件，进而创建出需要的 PV。 ","date":"2022-12-11","objectID":"/posts/pv_pvc/:3:0","tags":null,"title":"PV 和 PVC","uri":"/posts/pv_pvc/"},{"categories":["kubernetes"],"content":"为什么需要 StorageClass 在一个大规模的 Kubernetes 集群里，可能有成千上万个 PVC，这就意味着运维人员必须实现创建出这个多个 PV，此外，随着项目的需要，会有新的 PVC 不断被提交，那么运维人员就需要不断的添加新的，满足要求的 PV，否则新的 Pod 就会因为 PVC 绑定不到 PV 而导致创建失败。而且通过 PVC 请求到一定的存储空间也很有可能不足以满足应用对于存储设备的各种需求。 而且不同的应用程序对于存储性能的要求可能也不尽相同，比如读写速度、并发性能等，为了解决这一问题，Kubernetes 又为我们引入了一个新的资源对象：StorageClass，通过 StorageClass 的定义，管理员可以将存储资源定义为某种类型的资源，比如快速存储、慢速存储等，用户根据 StorageClass 的描述就可以非常直观的知道各种存储资源的具体特性了，这样就可以根据应用的特性去申请合适的存储资源了。 ","date":"2022-12-11","objectID":"/posts/pv_pvc/:3:1","tags":null,"title":"PV 和 PVC","uri":"/posts/pv_pvc/"},{"categories":["kubernetes"],"content":"创建 资源清单如下 kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: local-storage provisioner: Local # 卷插件（如：local NFS） reclaimPolicy: Retain # 回收策略 volumeBindingMode: Immediate # 绑定模式 --- apiVersion: v1 kind: PersistentVolume metadata: name: local-pv spec: capacity: storage: 1Gi volumeMode: Filesystem storageClassName: local-storage accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain local: path: /home/txl/data nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: pv operator: In values: - local --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: ngx-pvc spec: accessModes: - ReadWriteOnce storageClassName: local-storage resources: requests: storage: 1Gi --- apiVersion: apps/v1 kind: Deployment metadata: name: ngx-sc spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: ngx1 image: nginx:1.18-alpine imagePullPolicy: IfNotPresent volumeMounts: - name: mydata mountPath: /data ports: - containerPort: 80 volumes: - name: mydata persistentVolumeClaim: claimName: ngx-pvc ","date":"2022-12-11","objectID":"/posts/pv_pvc/:3:2","tags":null,"title":"PV 和 PVC","uri":"/posts/pv_pvc/"},{"categories":["kubernetes"],"content":"关键配置参数 绑定模式 WaitForFirstConsumer：控制卷绑定和动态制备应该发生在什么时候 Immediate：一旦创建 PVC 就绑定 WaitForFirstConsumer：延迟绑定，直到使用该 PVC 的 Pod 被创建 ","date":"2022-12-11","objectID":"/posts/pv_pvc/:3:3","tags":null,"title":"PV 和 PVC","uri":"/posts/pv_pvc/"},{"categories":["kubernetes"],"content":"挂载到 Pod apiVersion: apps/v1 kind: Deployment metadata: name: ngx-pv spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: ngx1 image: nginx:1.18-alpine imagePullPolicy: IfNotPresent volumeMounts: - name: mydata mountPath: /data ports: - containerPort: 80 volumes: - name: mydata persistentVolumeClaim: claimName: ngx-pvc ","date":"2022-12-11","objectID":"/posts/pv_pvc/:4:0","tags":null,"title":"PV 和 PVC","uri":"/posts/pv_pvc/"},{"categories":["kubernetes"],"content":"在 Kubernetes 中，pod 是应用程序的载体，我们可以通过 pod 的 ip 来访问应用程序，但是 pod 的 ip 地址不是固定的，这也就意味着不方便直接采用 pod 的 ip 对服务进行访问 为了解决这个问题，Kubernetes 提供了 service 资源，service 会对提供同一个服务的多个 pod 进行聚合，并且提供一个统一的入口地址，通过访问 service 的入口地址就能访问到后面的 pod 服务。 通过 service 可以提供负载均衡和服务自动发现 ","date":"2022-12-11","objectID":"/posts/service/:0:0","tags":null,"title":"Service","uri":"/posts/service/"},{"categories":["kubernetes"],"content":"服务类型 ClusterIP：k8s 默认的 ServiceType，通过集群的内部 IP 暴露服务，选择该值时服务只能够在集群内部访问 NodePort：用来对集群外暴露 Service，你可以通过访问集群内的每个 NodeIP:NodePort 的方式，访问到对应 Service 后端的 Endpoint LoadBalancer: 这也是用来对集群外暴露服务的，不同的是这需要外部负载均衡器的云提供商，比如 AWS 等 ExternalName：这个也是在集群内发布服务用的，需要借助 KubeDNS(version \u003e= 1.7) 的支持，就是用KubeDNS 将该 service 和 ExternalName 做一个 Map，KubeDNS 返回一个 CNAME 记录。 每种服务类型都是会指定一个 clusterIP 的，由 clusterIP 进入对应代理模式实现负载均衡，如果强制 spec.clusterIP: \"None\"（即 headless service），集群无法为它们实现负载均衡，直接通过 pod 域名访问pod，典型是应用是 StatefulSet。 ","date":"2022-12-11","objectID":"/posts/service/:1:0","tags":null,"title":"Service","uri":"/posts/service/"},{"categories":["kubernetes"],"content":"Service 使用 ","date":"2022-12-11","objectID":"/posts/service/:2:0","tags":null,"title":"Service","uri":"/posts/service/"},{"categories":["kubernetes"],"content":"创建 创建 deployment 信息，设置 app=nginx 的标签 apiVersion: apps/v1 kind: Deployment metadata: name: myngx spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: ngx image: \"nginx:1.18-alpine\" 创建一个名为 nginx-svc 的 service 对象，它会将请求代理到80端口且具有标签 app: nginx 的 pod 上 apiVersion: v1 kind: Service metadata: name: nginx-svc spec: type: ClusterIP selector: # 通过selector和pod建立关联 app: nginx ports: - port: 80 targetPort: 80 ","date":"2022-12-11","objectID":"/posts/service/:2:1","tags":null,"title":"Service","uri":"/posts/service/"},{"categories":["kubernetes"],"content":"EndPoint Endpoint 是 k8s 中的一个资源对象，存储在 etcd 中，用来记录一个 service 对应的所有 pod 的访问地址，它是根据 service 配置文件中的 selector 描述产生的 一个 service 由一组 pod 组成，这些 pod 通过 endpoints 暴露出来，endpoints 是实现实际服务的端点集合。换句话说，service 和 pod 之间的联系是通过 endpoints 实现的。 $ kubectl get endpoints NAME ENDPOINTS AGE kubernetes 192.168.0.111:6443 12d nginx-svc 10.244.0.181:80,10.244.3.32:80 59m ","date":"2022-12-11","objectID":"/posts/service/:3:0","tags":null,"title":"Service","uri":"/posts/service/"},{"categories":["kubernetes"],"content":"kube-proxy 主要负责 pod 网络代理，维护网络规则和四层负载均衡工作 service 在很多情况下只是一个概念，真正起作用的其实是 kube-proxy 服务进程，每个 node 节点上都运行一个kube-proxy 服务进程，当创建 service 的时候会通过 api-server 向 etcd 写入创建的 service 信息，而 kube-proxy 会基于监听的机制发现这种 service 的变动，然后它会将最新的 service 信息转换成对应的访问规则 kube-proxy 监听 10249 和 10256 端口，对外提供 /metrics 和 /healthy 的访问 # 查看配置 $ kubectl describe cm kube-proxy -n kube-system # 查看 kube-proxy pod $ kubectl get pods -n kube-system | grep kube-proxy kube-proxy-bxk96 1/1 Running 2 11d kube-proxy-xbv75 1/1 Running 4 12d ","date":"2022-12-11","objectID":"/posts/service/:4:0","tags":null,"title":"Service","uri":"/posts/service/"},{"categories":["kubernetes"],"content":"userspace 模式(废弃) ","date":"2022-12-11","objectID":"/posts/service/:4:1","tags":null,"title":"Service","uri":"/posts/service/"},{"categories":["kubernetes"],"content":"iptables 模式(默认模式) iptables 模式下，节点上 kube-proxy 持续监听 Service 以及 Endpoints 对象的变化，为 service 后端的每个 pod 创建对应的 iptables 规则，当捕获到 Service 的 clusterIP 和端口请求，利用注入的 iptables，将请求重定向到 Service 的对应的 Pod ","date":"2022-12-11","objectID":"/posts/service/:4:2","tags":null,"title":"Service","uri":"/posts/service/"},{"categories":["kubernetes"],"content":"IPVS 模式 IPVS 模式是利用 linux 的 IPVS 模块实现，同样是由 kube-proxy 实时监视集群的 service 和 endpoint。基于内核内哈希表，有更高的网络流量吞吐量（iptables 模式在大规模集群，比如10000 个服务中性能下降显著），并且具有更复杂的负载均衡算法（最小连接、局部性、 加权、持久性） 当 kube-proxy 以 IPVS 代理模式启动时，它将验证 IPVS 内核模块是否可用。 如果未检测到 IPVS 内核模块，则 kube-proxy 将退回到以 iptables 代理模式运行。 ","date":"2022-12-11","objectID":"/posts/service/:4:3","tags":null,"title":"Service","uri":"/posts/service/"},{"categories":["kubernetes"],"content":"无头(HeadLiness) 类型的 Service 在某些场景中，开发人员可能不想使用 Service 提供的负载均衡功能，而希望自己来控制负载均衡策略，针对这种情况，k8s 提供了 HeadLiness Service，这类 Service 不会分配 ClusterIP，如果想要访问 Service，只能通过service 的域名进行查询 apiVersion: v1 kind: Service metadata: name: nginx-svc spec: clusterIP: \"None\" # 将clusterIP设置为None，即可创建headliness Service type: ClusterIP selector: app: nginx ports: - port: 80 targetPort: 80 ","date":"2022-12-11","objectID":"/posts/service/:5:0","tags":null,"title":"Service","uri":"/posts/service/"},{"categories":["kubernetes"],"content":"宿主机访问 Service ","date":"2022-12-11","objectID":"/posts/service/:6:0","tags":null,"title":"Service","uri":"/posts/service/"},{"categories":["kubernetes"],"content":"安装 bind-utils $ sudo yum install bind-utils -y # 无法直接查询到service对应的ip $ nslookup nginx-svc ** server can't find nginx-svc: NXDOMAIN ","date":"2022-12-11","objectID":"/posts/service/:6:1","tags":null,"title":"Service","uri":"/posts/service/"},{"categories":["kubernetes"],"content":"设置解析 查看 kube-dns clusterIp $ kubectl get svc -n kube-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kube-dns ClusterIP 10.96.0.10 \u003cnone\u003e 53/UDP,53/TCP,9153/TCP 11d 修改 /etc/resolv.conf 文件，设置DNS服务器IP地址、DNS域名和设置主机的域名搜索顺序。加入内容 nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local 访问 $ curl nginx-svc ","date":"2022-12-11","objectID":"/posts/service/:6:2","tags":null,"title":"Service","uri":"/posts/service/"},{"categories":["kubernetes"],"content":"Secret 是一种包含少量敏感信息例如密码、OAuth 令牌和 SSH 密钥。 将这些信息放在 secret 中比放在 Pod 的定义或者 容器镜像中来说更加安全和灵活。 Kubernetes 提供若干种内置的类型，用于一些常见的使用场景。 针对这些类型，Kubernetes 所执行的合法性检查操作以及对其所实施的限制各不相同。 内置类型 用法 Opaque 用户定义的任意数据 kubernetes.io/service-account-token 服务账号令牌 kubernetes.io/dockercfg ~/.dockercfg 文件的序列化形式 kubernetes.io/dockerconfigjson ~/.docker/config.json 文件的序列化形式 kubernetes.io/basic-auth 用于基本身份认证的凭据 kubernetes.io/ssh-auth 用于 SSH 身份认证的凭据 kubernetes.io/tls 用于 TLS 客户端或者服务器端的数据 bootstrap.kubernetes.io/token 启动引导令牌数据 ","date":"2022-12-10","objectID":"/posts/secret/:0:0","tags":null,"title":"Secret","uri":"/posts/secret/"},{"categories":["kubernetes"],"content":"基本用法 ","date":"2022-12-10","objectID":"/posts/secret/:1:0","tags":null,"title":"Secret","uri":"/posts/secret/"},{"categories":["kubernetes"],"content":"创建 首先将字符串转换为 base64 $ echo -n 'admin' | base64 $ echo -n '1f2d1e2e67df' | base64 apiVersion: v1 kind: Secret metadata: name: mysecret type: Opaque data: username: YWRtaW4= password: MWYyZDFlMmU2N2Rm 如果希望使用非 base64 编码的字符串直接放入 Secret 中，应当使用 stringData 字段 stringData: config.yaml: | apiUrl: \"https://my.api.com/api/v1\" username: \"admin\" password: \"1f2d1e2e67df\" ","date":"2022-12-10","objectID":"/posts/secret/:1:1","tags":null,"title":"Secret","uri":"/posts/secret/"},{"categories":["kubernetes"],"content":"解码 输出 yaml 内容 $ kubectl get secret mysecret -o yaml $ echo -n 'YWRtaW4=' | base64 -d 使用 JSONPath 模板输出特定字段，文档：JSONPath 支持 | Kubernetes $ kubectl get secret mysecret -o jsonpath={.data.username} | base64 -d ","date":"2022-12-10","objectID":"/posts/secret/:1:2","tags":null,"title":"Secret","uri":"/posts/secret/"},{"categories":["kubernetes"],"content":"以环境变量的方式使用 apiVersion: apps/v1 kind: Deployment metadata: name: myngx spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: ngx image: nginx:1.18-alpine imagePullPolicy: IfNotPresent env: - name: SECRET_USERNAME valueFrom: secretKeyRef: name: mysecret key: username ","date":"2022-12-10","objectID":"/posts/secret/:1:3","tags":null,"title":"Secret","uri":"/posts/secret/"},{"categories":["kubernetes"],"content":"挂载到文件 apiVersion: apps/v1 kind: Deployment metadata: name: myngx spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: ngx image: nginx:1.18-alpine imagePullPolicy: IfNotPresent volumeMounts: - name: foo mountPath: /etc/foo readOnly: true volumes: - name: foo secret: secretName: mysecret items: - key: username path: my-username 如果省略 items 节点，会映射 secret 所有的 key ","date":"2022-12-10","objectID":"/posts/secret/:1:4","tags":null,"title":"Secret","uri":"/posts/secret/"},{"categories":["kubernetes"],"content":"手工配置 basic-auth 认证 ","date":"2022-12-10","objectID":"/posts/secret/:2:0","tags":null,"title":"Secret","uri":"/posts/secret/"},{"categories":["kubernetes"],"content":"生成密码文件 可以使用 htpasswd 或者 openssl passwd 命令生成 安装 $ sudo yum -y install httpd-tools 生成认证密码 创建一个用户名和密码的认证到auth文件中 $ htpasswd -c auth txl ","date":"2022-12-10","objectID":"/posts/secret/:2:1","tags":null,"title":"Secret","uri":"/posts/secret/"},{"categories":["kubernetes"],"content":"导入 Secret $ kubectl create secret generic basic-auth --from-file=auth ","date":"2022-12-10","objectID":"/posts/secret/:2:2","tags":null,"title":"Secret","uri":"/posts/secret/"},{"categories":["kubernetes"],"content":"创建 nginx Configmap apiVersion: v1 kind: ConfigMap metadata: name: ngx data: default: | server { listen 80; server_name localhost; location / { auth_basic \"test auth\"; auth_basic_user_file /etc/nginx/basicauth; # 指向生成的密码文件 root /usr/share/nginx/html; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } } ","date":"2022-12-10","objectID":"/posts/secret/:2:3","tags":null,"title":"Secret","uri":"/posts/secret/"},{"categories":["kubernetes"],"content":"创建 Deployment apiVersion: apps/v1 kind: Deployment metadata: name: ngx spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: ngx image: nginx:1.18-alpine imagePullPolicy: IfNotPresent volumeMounts: - name: nginx-default mountPath: /etc/nginx/conf.d/default.conf # 覆盖nginx默认配置 subPath: default - name: basic-auth mountPath: /etc/nginx/basicauth subPath: auth volumes: - name: nginx-default configMap: name: ngx defaultMode: 0655 - name: basic-auth secret: secretName: basic-auth defaultMode: 0655 ","date":"2022-12-10","objectID":"/posts/secret/:2:4","tags":null,"title":"Secret","uri":"/posts/secret/"},{"categories":["kubernetes"],"content":"访问 $ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES ngx-5597699bdf-ntbjx 1/1 Running 0 31m 10.244.3.29 lain2 \u003cnone\u003e \u003cnone\u003e $ curl --basic -u txl:123 http://10.244.3.29 ","date":"2022-12-10","objectID":"/posts/secret/:2:5","tags":null,"title":"Secret","uri":"/posts/secret/"},{"categories":["kubernetes"],"content":"从私有仓库拉取镜像 使用 Docker Hub 镜像仓库 ","date":"2022-12-10","objectID":"/posts/secret/:3:0","tags":null,"title":"Secret","uri":"/posts/secret/"},{"categories":["kubernetes"],"content":"登录 Docker 镜像仓库 $ docker login --username=\u003c用户名\u003e # 发布 $ docker push \u003c镜像名\u003e ","date":"2022-12-10","objectID":"/posts/secret/:3:1","tags":null,"title":"Secret","uri":"/posts/secret/"},{"categories":["kubernetes"],"content":"创建 DockerHub Secret 创建一个名为 docker-regcred 的 docker registry secret $ kubectl create secret docker-registry docker-regcred \\ --docker-server=https://index.docker.io/v1/ \\ --docker-username=\u003c用户名\u003e \\ --docker-password=\u003c密码\u003e \\ --docker-email=\u003c邮箱地址\u003e 解码 $ kubectl get secret docker-registry -o jsonpath={.data.*} | base64 -d ","date":"2022-12-10","objectID":"/posts/secret/:3:2","tags":null,"title":"Secret","uri":"/posts/secret/"},{"categories":["kubernetes"],"content":"使用 apiVersion: apps/v1 kind: Deployment metadata: name: myalpine spec: selector: matchLabels: app: myalpine replicas: 1 template: metadata: labels: app: myalpine spec: imagePullSecrets: - name: docker-regcred containers: - name: alpine image: lains3/alpine:3.12 imagePullPolicy: IfNotPresent command: [\"sh\",\"-c\",\"echo this is alpine \u0026\u0026 sleep 36000\"] ","date":"2022-12-10","objectID":"/posts/secret/:3:3","tags":null,"title":"Secret","uri":"/posts/secret/"},{"categories":["kubernetes"],"content":"ConfigMap 是一种 API 对象，用来将非机密性的数据保存到键值对中。使用时， Pod 可以将其用作环境变量、命令行参数或者存储卷中的配置文件。 ConfigMap 将你的环境配置信息和容器镜像解耦，便于应用配置的修改。 使用场景： 容器 entrypoint 的命令行参数 容器的环境变量 映射成文件 编写代码在 Pod 中运行，使用 Kubernetes API 来读取 ConfigMap ","date":"2022-12-09","objectID":"/posts/configmap/:0:0","tags":null,"title":"ConfigMap","uri":"/posts/configmap/"},{"categories":["kubernetes"],"content":"使用 apiVersion: v1 kind: ConfigMap metadata: name: mycm data: host: \"0.0.0.0\" port: \"9999\" user.properties: | user.name=txl user.age=18 查看 $ kubectl get cm -n default NAME DATA AGE mycm 3 45m ","date":"2022-12-09","objectID":"/posts/configmap/:1:0","tags":null,"title":"ConfigMap","uri":"/posts/configmap/"},{"categories":["kubernetes"],"content":"在环境变量中使用 apiVersion: apps/v1 kind: Deployment metadata: name: myngx spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: ngx image: nginx:1.18-alpine imagePullPolicy: IfNotPresent env: - name: HOST valueFrom: configMapKeyRef: name: mycm # ConfigMap 名称 key: host # 需要取值的键 ","date":"2022-12-09","objectID":"/posts/configmap/:1:1","tags":null,"title":"ConfigMap","uri":"/posts/configmap/"},{"categories":["kubernetes"],"content":"映射成文件 apiVersion: apps/v1 kind: Deployment metadata: name: myngx spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: ngx image: nginx:1.18-alpine imagePullPolicy: IfNotPresent volumeMounts: - name: cmconfig mountPath: \"/config\" readOnly: true volumes: - name: cmconfig configMap: name: mycm # ConfigMap 名称 items: # 来自 ConfigMap 的一组键，将被创建为文件 - key: \"user.properties\" path: \"userinfo.conf\" 如果省略 items 节点，会映射 ConfigMap 全部的 key $ cd /config \u0026\u0026 ls -l total 0 lrwxrwxrwx 1 root root 11 Dec 6 14:00 host -\u003e ..data/host lrwxrwxrwx 1 root root 11 Dec 6 14:00 port -\u003e ..data/port lrwxrwxrwx 1 root root 22 Dec 6 14:00 user.properties -\u003e ..data/user.properties 使用 subPath 可用于指定所引用的卷内的子路径，而不是其根路径。 apiVersion: apps/v1 kind: Deployment metadata: name: myngx spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: ngx image: nginx:1.18-alpine imagePullPolicy: IfNotPresent env: - name: HOST valueFrom: configMapKeyRef: name: mycm key: host volumeMounts: - name: cmconfig mountPath: /config/userinfo.conf subPath: user.properties volumes: - name: cmconfig configMap: defaultMode: 0655 name: mycm ","date":"2022-12-09","objectID":"/posts/configmap/:1:2","tags":null,"title":"ConfigMap","uri":"/posts/configmap/"},{"categories":["kubernetes"],"content":"使用 client-go 调用 GitHub: kubernetes/client-go: Go client for Kubernetes ","date":"2022-12-09","objectID":"/posts/configmap/:2:0","tags":null,"title":"ConfigMap","uri":"/posts/configmap/"},{"categories":["kubernetes"],"content":"集群外调用 使用 api 代理 $ kubectl proxy --address='0.0.0.0' --accept-hosts='^*$' --port=8009 获取 ConfigMap func getClient() *kubernetes.Clientset { config := \u0026rest.Config{ Host: \"http://ip:8009\", } c, err := kubernetes.NewForConfig(config) if err != nil { log.Fatalln(err) } return c } func main() { client := getClient() cm, err := client.CoreV1().ConfigMaps(\"default\").Get(context.Background(), \"mycm\", v1.GetOptions{}) if err != nil { log.Fatalln(err) } fmt.Println(cm.Data) } 执行结果 map[host:0.0.0.0 port:9999 user.properties:user.name=txl user.age=18 ] ","date":"2022-12-09","objectID":"/posts/configmap/:2:1","tags":null,"title":"ConfigMap","uri":"/posts/configmap/"},{"categories":["kubernetes"],"content":"集群内调用 创建 ServiceAccouont 拥有 default 空间内对 ConfigMap 的查看权限 apiVersion: v1 kind: ServiceAccount metadata: name: sa-cm --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: clusterrole-cm rules: - apiGroups: [\"\"] resources: [\"configmaps\"] verbs: [\"get\", \"watch\", \"list\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: clusterrolebinding-cm namespace: default roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: clusterrole-cm subjects: - kind: ServiceAccount name: sa-cm namespace: default 调用 API token 路径：/var/run/secrets/kubernetes.io/serviceaccount/token api server 地址：https://$KUBERNETES_SERVICE_HOST:$KUBERNETES_PORT_443_TCP_PORT 证书路径：/var/run/secrets/kubernetes.io/serviceaccount/ca.crt var apiServer string var token string func init() { apiServer = fmt.Sprintf(\"https://%s:%s\", os.Getenv(\"KUBERNETES_SERVICE_HOST\"), os.Getenv(\"KUBERNETES_PORT_443_TCP_PORT\")) f, err := os.Open(\"/var/run/secrets/kubernetes.io/serviceaccount/token\") if err != nil { log.Fatal(err) } b, _ := io.ReadAll(f) token = string(b) } func getClient() *kubernetes.Clientset { config := \u0026rest.Config{ Host: apiServer, BearerToken: token, TLSClientConfig: rest.TLSClientConfig{CAFile: \"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\"}, } c, err := kubernetes.NewForConfig(config) if err != nil { log.Fatalln(err) } return c } func main() { client := getClient() cm, err := client.CoreV1().ConfigMaps(\"default\").Get(context.Background(), \"mycm\", v1.GetOptions{}) if err != nil { log.Fatalln(err) } fmt.Println(cm.Data) select {} } 交叉编译 set GOOS=linux set GOARCH=amd64 go build -o cmtest main.go 创建 Deployment apiVersion: apps/v1 kind: Deployment metadata: name: cmtest spec: selector: matchLabels: app: cmtest replicas: 1 template: metadata: labels: app: cmtest spec: serviceAccount: sa-cm # 指定 ServiceAccount nodeName: lain1 # 指定 node containers: - name: cmtest image: alpine:3.12 imagePullPolicy: IfNotPresent command: [\"/app/cmtest\"] volumeMounts: - name: app mountPath: /app volumes: - name: app hostPath: path: /home/txl/goapi type: Directory 查看 $ kubectl get pods NAME READY STATUS RESTARTS AGE cmtest-96b96c458-szxhk 1/1 Running 0 2m46s $ kubectl logs cmtest-96b96c458-szxhk map[host:0.0.0.0 port:9999 user.properties:user.name=txl user.age=18 ] ","date":"2022-12-09","objectID":"/posts/configmap/:2:2","tags":null,"title":"ConfigMap","uri":"/posts/configmap/"},{"categories":["kubernetes"],"content":"监控 ConfigMap 变化 type CmHandler struct{} func(this *CmHandler) OnAdd(obj interface{}){} func(this *CmHandler) OnUpdate(oldObj, newObj interface{}){ if newObj.(*v1.ConfigMap).Name==\"mycm\"{ log.Println(\"mycm发生了变化\") } } func(this *CmHandler) OnDelete(obj interface{}){} func main() { fact:=informers.NewSharedInformerFactory(getClient(), 0) cmInformer:=fact.Core().V1().ConfigMaps() cmInformer.Informer().AddEventHandler(\u0026CmHandler{}) fact.Start(wait.NeverStop) select {} } ","date":"2022-12-09","objectID":"/posts/configmap/:2:3","tags":null,"title":"ConfigMap","uri":"/posts/configmap/"},{"categories":["kubernetes"],"content":"文档：Pod | Kubernetes Pod 是可以在 Kubernetes 中创建和管理的、最小的可部署的计算单元。它是一个或多个容器的组合。这些容器共享存储、网络和命名空间，以及如何运行的规范。其它的资源对象都是用来支撑或者扩展 Pod 对象功能的，比如控制器对象是用来管控 Pod 对象的，Service 或者 Ingress 资源对象是用来暴露 Pod 引用对象的，PersistentVolume 资源对象是用来为 Pod 提供存储等等，K8S 不会直接处理容器，而是 Pod，Pod 是由一个或多个 container 组成。基本的好处有： 方便部署、扩展和收缩、方便调度等 Pod中的容器共享数据和网络空间，统一的资源管理与分配 在Pod中，所有容器都被同一安排和调度，并运行在共享的上下文中。对于具体应用而言，Pod是它们的逻辑主机，Pod包含业务相关的多个应用容器。 每一个 Pod 都有一个特殊的被称为 “根容器” 的 Pause 容器。Pause 容器对应的镜像属于 Kubernetes 平台的一部分，除了 Pause 容器，每个 Pod 还包含一个或多个紧密相关的用户业务容器。Pause 容器的作用： 扮演 Pid=1 的，回收僵尸进程 基于 Linux 的 namespace 的共享 ","date":"2022-12-04","objectID":"/posts/pod/:0:0","tags":null,"title":"Pod、ReplicaSet 和 Deployment","uri":"/posts/pod/"},{"categories":["kubernetes"],"content":"Pod 基本使用 Pod 通常不是直接创建的，而是使用工作负载资源创建的 ","date":"2022-12-04","objectID":"/posts/pod/:1:0","tags":null,"title":"Pod、ReplicaSet 和 Deployment","uri":"/posts/pod/"},{"categories":["kubernetes"],"content":"创建 apiVersion: v1 kind: Pod metadata: name: myngx spec: containers: - name: ngx image: \"nginx:1.18-alpine\" 展示详细信息 $ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES myngx 1/1 Running 0 13m 10.244.3.7 lain2 \u003cnone\u003e \u003cnone\u003e ","date":"2022-12-04","objectID":"/posts/pod/:1:1","tags":null,"title":"Pod、ReplicaSet 和 Deployment","uri":"/posts/pod/"},{"categories":["kubernetes"],"content":"ReplicaSet ReplicaSet 是为了保持维护的期待 Pod 副本数量与现时 Pod 副本数量一致。如在由于 Pod 异常退出导致期待的副本数量不足时，会自动创建新的 Pod 保证到与期望的 Pod 副本数量一致 ","date":"2022-12-04","objectID":"/posts/pod/:2:0","tags":null,"title":"Pod、ReplicaSet 和 Deployment","uri":"/posts/pod/"},{"categories":["kubernetes"],"content":"使用 Deployment Deployment 运行一组相同的 Pod（副本水平扩展）、滚动更新。通过副本集管理和创建POD。我们往往不会直接在集群中使用 ReplicaSet 部署一个新的微服务，一方面是因为 ReplicaSet 的功能其实不够强大，一些常见的更新、扩容和缩容运维操作都不支持，Deployment 的引入就是为了就是为了支持这些复杂的操作 ","date":"2022-12-04","objectID":"/posts/pod/:3:0","tags":null,"title":"Pod、ReplicaSet 和 Deployment","uri":"/posts/pod/"},{"categories":["kubernetes"],"content":"挂载 挂载 hostPath 主机目录卷实例 apiVersion: apps/v1 kind: Deployment metadata: name: myngx spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: ngx image: \"nginx:1.18-alpine\" volumeMounts: # 声明容器中的挂载位置 - name: mydata mountPath: /data - name: alpine # 测试多容器 command: [\"sh\",\"-c\",\"echo this is alpine \u0026\u0026 sleep 3600\"] image: \"alpine:3.12\" volumes: - name: mydata hostPath: path: /home/txl/yaml/data # 声明主机节点目录 type: Directory # 指定type hostPath 支持的 type 值如下： ","date":"2022-12-04","objectID":"/posts/pod/:3:1","tags":null,"title":"Pod、ReplicaSet 和 Deployment","uri":"/posts/pod/"},{"categories":["kubernetes"],"content":"共享文件夹 同一个 pod 内的容器都能读写 EmptyDir 中的文件。常用于临时空间、多容器共享，如日志或者tmp文件需要的临时目录 apiVersion: apps/v1 kind: Deployment metadata: name: myngx spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: ngx image: nginx:1.18-alpine imagePullPolicy: IfNotPresent volumeMounts: - name: sharedata mountPath: /data - name: alpine image: alpine:3.12 imagePullPolicy: IfNotPresent command: [\"sh\",\"-c\",\"echo this is alpine \u0026\u0026 sleep 36000\"] volumeMounts: - name: sharedata mountPath: /data volumes: - name: sharedata emptyDir: {} ","date":"2022-12-04","objectID":"/posts/pod/:3:2","tags":null,"title":"Pod、ReplicaSet 和 Deployment","uri":"/posts/pod/"},{"categories":["kubernetes"],"content":"Init 容器 文档：Init 容器 | Kubernetes Init 容器是一种特殊容器，在 Pod 内的应用容器启动之前运行。Init 容器可以包括一些应用镜像中不存在的实用工具和安装脚本。 Init 容器与普通的容器非常像，除了如下两点： 它们总是运行到完成。 每个都必须在下一个启动之前成功完成。 如果 Pod 的 Init 容器失败，kubelet 会不断地重启该 Init 容器直到该容器成功为止。 然而，如果 Pod 对应的 restartPolicy 值为 “Never”，Kubernetes 不会重新启动 Pod。 原理 在 Pod 启动过程中，每个 Init 容器会在网络和数据卷初始化之后按顺序启动。 依据 Init 容器在 Pod spec 配置中的出现顺序依次运行。由于 Pod 可能各种原因多次重启，所以 Init 容器中的操作，须具备幂等性。 应用场景 环境检查：例如确保应用容器依赖的服务启动后再启动应用容器 初始化配置：例如给应用容器准备配置文件 基本配置 apiVersion: apps/v1 kind: Deployment metadata: name: myngx spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: initContainers: - name: init-mydb image: alpine:3.12 command: ['sh', '-c', 'echo wait for db \u0026\u0026 sleep 35 \u0026\u0026 echo done'] # 模拟等待35s containers: - name: ngx image: nginx:1.18-alpine imagePullPolicy: IfNotPresent volumeMounts: - name: sharedata mountPath: /data - name: alpine image: alpine:3.12 imagePullPolicy: IfNotPresent command: [\"sh\",\"-c\",\"echo this is alpine \u0026\u0026 sleep 36000\"] volumeMounts: - name: sharedata mountPath: /data volumes: - name: sharedata emptyDir: {} 查看 init 容器状态 $ kubectl get pod NAME READY STATUS RESTARTS AGE myngx-89dd8586b-k59pl 0/2 Init:0/1 0 8s 状态 含义 Init:N/M Pod 包含 M 个 Init 容器，其中 N 个已经运行完成。 Init:Error Init 容器已执行失败。 Init:CrashLoopBackOff Init 容器执行总是失败。 Pending Pod 还没有开始执行 Init 容器。 PodInitializing or Running Pod 已经完成执行 Init 容器。 ","date":"2022-12-04","objectID":"/posts/pod/:3:3","tags":null,"title":"Pod、ReplicaSet 和 Deployment","uri":"/posts/pod/"},{"categories":["kubernetes"],"content":"文档：使用 RBAC 鉴权 | Kubernetes ","date":"2022-12-03","objectID":"/posts/rbac/:0:0","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"用户 UserAccount（普通用户）：集群外部访问时使用的用户账号，最常见的就是 kubectl 命令就是作为 kubernetes-admin 用户来执行，k8s本身不记录这些账号 ServiceAccount（服务账户）：它们被绑定到特定的名字空间，服务账号与一组以 Secret 保存的凭据相关，这些凭据会被挂载到 Pod 中，从而允许集群内的进程访问 Kubernetes API ","date":"2022-12-03","objectID":"/posts/rbac/:1:0","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"用户认证 ","date":"2022-12-03","objectID":"/posts/rbac/:2:0","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"A. 使用 X509 客户证书 生成证书 安装 OpenSSL $ sudo yum install openssl openssl-devel 生成一个名称为txl的普通用户的客户端证书 $ mkdir ua/txl $ cd ua/txl # 生成客户端私钥 $ openssl genrsa -out client.key 2048 # 根据私钥生成csr, 指定用户名txl $ openssl req -new -key client.key -out client.csr -subj \"/CN=txl\" # 根据k8s的CA证书生成客户端证书 $ sudo openssl x509 -req -in client.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out client.crt -days 365 证书反解 获取证书设置的CN(Common name) $ openssl x509 -noout -subject -in client.crt 使用证书初步请求API $ kubectl get endpoints NAME ENDPOINTS AGE kubernetes 192.168.0.111:6443 4d5h $ curl --cert ./client.crt --key ./client.key --cacert /etc/kubernetes/pki/ca.crt -s https://192.168.0.111:6443/api { \"kind\": \"APIVersions\", \"versions\": [ \"v1\" ], \"serverAddressByClientCIDRs\": [ { \"clientCIDR\": \"0.0.0.0/0\", \"serverAddress\": \"192.168.0.111:6443\" } ] } 可以使用 --insecure 代替 --cacert /etc/kubernetes/pki/ca.crt 忽略服务端证书验证 证书加入 kube config 把 client.crt 加入到 ~/.kube/config $ kubectl config --kubeconfig=/home/txl/.kube/config set-credentials txl --client-certificate=/home/txl/ua/txl/client.crt --client-key=/home/txl/ua/txl/client.key 创建一个名为 user_context 的 context $ kubectl config --kubeconfig=/home/txl/.kube/config set-context user_context --cluster=kubernetes --user=txl 切换当前上下文为 user_context $ kubectl config use-context user_context # 查看 $ kubectl config current-context # 重新切回默认管理员 $ kubectl config use-context kubernetes-admin@kubernetes ","date":"2022-12-03","objectID":"/posts/rbac/:2:1","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"B. 使用静态令牌文件(Token) token 和证书只能配一个 生成 Token $ head -c 16 /dev/urandom | od -An -t x | tr -d ' ' 加入 kube config $ kubectl config set-credentials txl --token=fdb72d94a1c2c2cfbf82341d1f98c68c 修改 api-server 启动参数 $ sudo vi /etc/kubernetes/pki/token_auth # 加入 fdb72d94a1c2c2cfbf82341d1f98c68c,txl,1001 $ sudo vi /etc/kubernetes/manifests/kube-apiserver.yaml # 加入 --token-auth-file=/etc/kubernetes/pki/token_auth 查看 $ curl -H \"Authorization: Bearer fdb72d94a1c2c2cfbf82341d1f98c68c\" https://192.168.0.111:6443/api/v1/namespaces/default/pods --insecure { \"kind\": \"PodList\", \"apiVersion\": \"v1\", \"metadata\": { \"selfLink\": \"/api/v1/namespaces/default/pods\", \"resourceVersion\": \"1586027\" }, \"items\": [] } ","date":"2022-12-03","objectID":"/posts/rbac/:2:2","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"资源 查看所有资源 $ kubectl api-resources -o wide 其中 VERBS 列展示了该资源对应的操作，比如 role create 创建 delete 删除 deletecollection 批量删除 get 获取 list 列表 patch 合并变更 update 更新 watch 监听 ","date":"2022-12-03","objectID":"/posts/rbac/:3:0","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"Role 和 RoleBinding Role（角色）：包含一组代表相关权限的规则，用于授予对单个命名空间的资源访问 RoleBinding（角色绑定）：将角色中定义的权限赋予一个或者一组用户 ","date":"2022-12-03","objectID":"/posts/rbac/:4:0","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"创建 Role 下面是一个位于default的role，拥有对pod的读访问权限 $ vi role_mypod.yaml kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: namespace: default name: mypod rules: - apiGroups: [\"*\"] resources: [\"pods\"] verbs: [\"get\", \"watch\", \"list\"] 执行创建 $ kubectl apply -f role_mypod.yaml # 查看default空间所有role $ kubectl get role -n default 删除 $ kubectl delete role mypod -n default ","date":"2022-12-03","objectID":"/posts/rbac/:4:1","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"绑定 RoleBinding 创建一个名为mypodbinding的 rolebinding，关联创建的用户txl和创建的角色mypod 1. 使用命令 $ kubectl create rolebinding mypodbinding -n default --role mypod --user txl 2. 使用 yaml apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: creationTimestamp: null name: mypodrolebinding roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: mypod subjects: - apiGroup: rbac.authorization.k8s.io kind: User name: txl - kind: ServiceAccount name: txl 查看 $ kubectl get rolebinding -n default NAME ROLE mypodrolebinding Role/mypod 删除 $ kubectl delete rolebinding mypodbinding -n default ","date":"2022-12-03","objectID":"/posts/rbac/:4:2","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"ClusterRole 和 ClusterRoleBinding ClusterRole 同样可以用于授予 Role 能够授予的权限。 因为 ClusterRole 属于集群范围，所以它也可以为以下资源授予访问权限： 集群范围资源（如节点 Node） 非资源端点（如 /healthz） 跨名字空间访问的名字空间作用域的资源（如 Pod） clusterRole不限定命名空间，绑定既可以使用 RoleBinding，也可以使用 ClusterRoleBinding ","date":"2022-12-03","objectID":"/posts/rbac/:5:0","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"创建 ClusterRole kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: mypod-cluster rules: - apiGroups: [\"*\"] resources: [\"pods\"] verbs: [\"get\", \"watch\", \"list\"] 查看 $ kubectl get clusterrole ","date":"2022-12-03","objectID":"/posts/rbac/:5:1","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"绑定 RoleBinding 需要指定命名空间 apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: mypodrolebinding-cluster namespace: kube-system roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: mypod-cluster subjects: - apiGroup: rbac.authorization.k8s.io kind: User name: txl 查看 $ kubectl get rolebinding -n kube-system NAME ROLE mypodrolebinding-cluster ClusterRole/mypod-cluster ","date":"2022-12-03","objectID":"/posts/rbac/:5:2","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"绑定 ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: mypod-clusterrolebinding roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: mypod-cluster subjects: - apiGroup: rbac.authorization.k8s.io kind: User name: txl - kind: ServiceAccount name: txl namespace: default ","date":"2022-12-03","objectID":"/posts/rbac/:5:3","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"ServiceAccount UserAccount 是可以跨 namespace 的，而 ServiceAccount 只能局限在自己所属的 namespace 中，每个 namespace 都会有一个默认的 default 账号 。 ","date":"2022-12-03","objectID":"/posts/rbac/:6:0","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"创建 SA $ kubectl create sa mysa 也可以导出到 yaml kubectl create sa mysa -o yaml --dry-run=client \u003e mysa.yaml 查看 $ kubectl get sa -n default NAME SECRETS AGE default 1 4d7h mysa 1 5m21s # 查看令牌 $ kubectl describe sa mysa Name: mysa Namespace: default Labels: \u003cnone\u003e Annotations: \u003cnone\u003e Image pull secrets: \u003cnone\u003e Mountable secrets: mysa-token-5bnbm Tokens: mysa-token-5bnbm Events: \u003cnone\u003e $ kubectl describe secret mysa-token-5bnbm ","date":"2022-12-03","objectID":"/posts/rbac/:6:1","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"绑定 ClusterRoleBinding 使用命令的方式 $ kubectl create clusterrolebinding mysa-clusterrolebinding --clusterrole=mypod-cluster --serviceaccount=default:mysa 查看 $ kubectl get clusterrolebinding NAME ROLE mypod-clusterrolebinding ClusterRole/mypod-cluster mysa-clusterrolebinding ClusterRole/mypod-cluster ","date":"2022-12-03","objectID":"/posts/rbac/:6:2","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"外部访问 API 安装 jq 轻量级的 json 处理命令。可以对 json 数据进行分片、过滤、映射、转换和格式化输出 $ sudo yum install jq -y 获取 SA Token 保存到临时变量 mysatoken 中 $ mysatoken=$(kubectl get secret $(kubectl get sa mysa -o json | jq -Mr '.secrets[0].name') -o json | jq -Mr '.data.token' | base64 -d) # 查看token $ echo $mysatoken 请求 $ curl -H \"Authorization: Bearer $mysatoken\" --insecure https://192.168.0.111:6443/api/v1/namespaces/default/pods ","date":"2022-12-03","objectID":"/posts/rbac/:6:3","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"在 Pod 里访问 API 创建一个测试 pod apiVersion: apps/v1 kind: Deployment metadata: name: myngx spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: serviceAccountName: mysa # 指定SA，否则使用的default SA containers: - name: nginxtest image: nginx:1.18-alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80 查看 pod $ kubectl get pod NAME READY STATUS RESTARTS AGE myngx-74748c5956-5rfrs 1/1 Running 0 4m21s 进入容器 $ kubectl exec -it myngx-74748c5956-5rfrs -- sh 设置临时变量 # SA token $ TOKEN=`cat /var/run/secrets/kubernetes.io/serviceaccount/token` # api server地址 $ APISERVER=\"https://$KUBERNETES_SERVICE_HOST:$KUBERNETES_PORT_443_TCP_PORT\" 请求（跳过服务器证书检查） $ curl --header \"Authorization: Bearer $TOKEN\" --insecure -s $APISERVER/api/v1/namespaces/default/pods 使用证书请求 $ curl --header \"Authorization: Bearer $TOKEN\" --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt $APISERVER/api/v1/namespaces/default/pods ","date":"2022-12-03","objectID":"/posts/rbac/:6:4","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"kubeadm是官方社区推出的一个用于快速部署kubernetes集群的工具 ","date":"2022-12-03","objectID":"/posts/kubeadm/:0:0","tags":null,"title":"使用 kubeadm 部署 k8s","uri":"/posts/kubeadm/"},{"categories":["kubernetes"],"content":"部署 文档地址：安装 kubeadm | Kubernetes ","date":"2022-12-03","objectID":"/posts/kubeadm/:1:0","tags":null,"title":"使用 kubeadm 部署 k8s","uri":"/posts/kubeadm/"},{"categories":["kubernetes"],"content":"添加 kubenetes 的 yum 源 在每个节点上分别执行 $ su - $ cat \u003c\u003cEOF \u003e /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF $ yum makecache ","date":"2022-12-03","objectID":"/posts/kubeadm/:1:1","tags":null,"title":"使用 kubeadm 部署 k8s","uri":"/posts/kubeadm/"},{"categories":["kubernetes"],"content":"安装 kubeadm、kubelet 和 kubectl 在每个节点上分别执行 $ sudo yum install -y kubelet-1.21.0 kubeadm-1.21.0 kubectl-1.21.0 安装后查看列表 $ rpm -aq kubelet kubectl kubeadm 把kubelet设置为开机启动 $ sudo systemctl enable kubelet kubeadm init 集群的快速初始化，部署Master节点的各个组件 kubeadm join 节点加入到指定集群中 kubeadm token 管理用于加入集群时使用的认证令牌 (如list，create) kubeadm reset 重置集群，如删除构建文件以回到初始状态 ","date":"2022-12-03","objectID":"/posts/kubeadm/:1:2","tags":null,"title":"使用 kubeadm 部署 k8s","uri":"/posts/kubeadm/"},{"categories":["kubernetes"],"content":"使用 systemd 作为 docker 的 cgroup driver 在每个节点上执行 $ sudo vi /etc/docker/daemon.json 加入内容 { \"exec-opts\": [\"native.cgroupdriver=systemd\"] } 重启docker $ systemctl daemon-reload \u0026\u0026 systemctl restart docker 验证结果 $ docker info |grep Cgroup Cgroup Driver: systemd Cgroup Version: 1 ","date":"2022-12-03","objectID":"/posts/kubeadm/:1:3","tags":null,"title":"使用 kubeadm 部署 k8s","uri":"/posts/kubeadm/"},{"categories":["kubernetes"],"content":"关闭 swap 临时关闭 $ swapoff -a 永久关闭 $ sudo vi /etc/fstab # 注释掉SWAP分区项 # swap was on /dev/sda11 during installation # UUID=0xxxxxxxxxxxxxx4f69 none swap sw 0 0 ","date":"2022-12-03","objectID":"/posts/kubeadm/:1:4","tags":null,"title":"使用 kubeadm 部署 k8s","uri":"/posts/kubeadm/"},{"categories":["kubernetes"],"content":"初始化集群 $ sudo kubeadm init --kubernetes-version=v1.21.0 --image-repository registry.aliyuncs.com/google_containers --pod-network-cidr=10.244.0.0/16 根据输出提示操作 $ mkdir -p $HOME/.kube $ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config $ sudo chown $(id -u):$(id -g) $HOME/.kube/config 默认token的有效期为24小时，当过期之后，该token就不可用了。 查看token列表： $ sudo kubeadm token list 重新生成token： $ sudo kubeadm token create --print-join-command ","date":"2022-12-03","objectID":"/posts/kubeadm/:1:5","tags":null,"title":"使用 kubeadm 部署 k8s","uri":"/posts/kubeadm/"},{"categories":["kubernetes"],"content":"安装网络组件 CNI (Container Network Interface) 容器网络接口，为了让用户在容器创建或销毁时都能够更容易地配置容器网络。常见的组件有： Flannel: 最基本的网络组件 Calico: 支持网络策略 Canal: 前两者的合体 Weave: 同样支持策略机制，还支持加密 使用 flannel Github地址：https://github.com/coreos/flannel 在每个节点执行 $ sudo sysctl net.bridge.bridge-nf-call-iptables=1 在master节点执行 $ kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml # 去污点 $ kubectl taint nodes --all node-role.kubernetes.io/master- 查看 kubectl get pods --all-namespaces 可能出现的错误 基本排查命令 $ kubectl describe $ journalctl -f -u kubelet $ for p in $(kubectl get pods --namespace=kube-system -l k8s-app=kube-dns -o name); do kubectl logs --namespace=kube-system $p; done coredns $ kubectl describe pod -n kube-system coredns-xxx network: open /run/flannel/subnet.env: no such file or directory 手动在每个节点上创建 $ sudo vi /run/flannel/subnet.env # 加入内容 FLANNEL_NETWORK=10.244.0.0/16 FLANNEL_SUBNET=10.244.0.1/24 FLANNEL_MTU=1450 FLANNEL_IPMASQ=true “cni0” already has an IP address different from 10.244.1.1/24 $ sudo ip link delete cni0 # 重启pod $ kubectl delete pod xxx Readiness probe failed: HTTP probe failed with statuscode: 503 $ systemctl stop kubelet $ systemctl stop docker $ iptables --flush $ iptables -tnat --flush $ systemctl start kubelet $ systemctl start docker 节点 not ready $ kubectl describe node lain2 failed to find plugin “xxx” in path [/opt/cni/bin] 把master节点的 /opt/cni/bin 拷贝过来，在master节点执行 $ cd /opt/cni $ sudo scp -r bin root@192.168.0.105:/opt/cni/bin 原因可能是重装k8s的时候没有删除 /etc/cni/net.d 目录 ","date":"2022-12-03","objectID":"/posts/kubeadm/:1:6","tags":null,"title":"使用 kubeadm 部署 k8s","uri":"/posts/kubeadm/"},{"categories":["kubernetes"],"content":"加入子节点 在每个node节点执行刚刚初始化集群时生成的token $ sudo kubeadm join 192.168.0.111:6443 --token fnq8dx.doxwr7sctdm57p0t \\ --discovery-token-ca-cert-hash sha256:114acfe6e30bc0181a93b9135296af62c5f946fa590691577071a4ebf21fc3ee ","date":"2022-12-03","objectID":"/posts/kubeadm/:1:7","tags":null,"title":"使用 kubeadm 部署 k8s","uri":"/posts/kubeadm/"},{"categories":["kubernetes"],"content":"查看集群健康状况 $ kubectl get cs controller-manager \u0026\u0026 scheduler Unhealthy: dial tcp 127.0.0.1:10252 connection refuse 在master节点执行 $ sudo vi /etc/kubernetes/manifests/kube-scheduler.yaml $ sudo vi /etc/kubernetes/manifests/kube-controller-manager.yaml # 注释 # - --port=0 # 重启kublet $ systemctl restart kubelet ","date":"2022-12-03","objectID":"/posts/kubeadm/:1:8","tags":null,"title":"使用 kubeadm 部署 k8s","uri":"/posts/kubeadm/"},{"categories":["kubernetes"],"content":"将集群导入 Rancher 找一个节点执行下载v2.6 $ sudo docker run --privileged -d --restart=unless-stopped -p 80:80 -p 443:443 rancher/rancher:v2.6-head 根据指引执行命令 ","date":"2022-12-03","objectID":"/posts/kubeadm/:2:0","tags":null,"title":"使用 kubeadm 部署 k8s","uri":"/posts/kubeadm/"}]