[{"categories":["Istio"],"content":"EnvoyFilter 资源允许你定制由 Istio Pilot 生成的 Envoy 配置。使用该资源，你可以更新数值，添加特定的过滤器，甚至添加新的监听器、集群等等。小心使用这个功能，因为不正确的定制可能会破坏整个网格的稳定性。 这些 EnvoyFilter 被应用的顺序是：首先是配置在根命名空间中的所有 EnvoyFilter，其次是配置在工作负载命名空间中的所有匹配的 EnvoyFilter。当多个 EnvoyFilter 被绑定到给定命名空间中的相同工作负载时，将按照创建时间的顺序依次应用。如果有多个 EnvoyFilter 配置相互冲突，那么将无法确定哪个配置被应用。 配置参考文档：Istio EnvoyFilter、Envoy ","date":"2023-02-05","objectID":"/posts/istio-envoyfilter/:0:0","tags":null,"title":"Istio Envoy Filter","uri":"/posts/istio-envoyfilter/"},{"categories":["Istio"],"content":"基本示例 过滤器： Network Filters：网络过滤器。处理连接的核心 HTTP Filters：HTTP 过滤器。由特殊的网络过滤器 HttpConnectionManager 管理，处理 HTTP1/HTTP2/gRPC 请求 可以参考 envoy 文档：全部 http_filter、全部 filter ","date":"2023-02-05","objectID":"/posts/istio-envoyfilter/:1:0","tags":null,"title":"Istio Envoy Filter","uri":"/posts/istio-envoyfilter/"},{"categories":["Istio"],"content":"增加响应头 下面的示例中在响应中添加了一个名为 api-version 的头 apiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata: name: myfilter namespace: istio-system spec: workloadSelector: labels: istio: ingressgateway configPatches: - applyTo: HTTP_FILTER match: context: GATEWAY # 网关侦听器 proxy: proxyVersion: ^1\\.11.* listener: filterChain: # 匹配侦听器中的特定筛选器链 filter: name: \"envoy.filters.network.http_connection_manager\" subFilter: # 此筛选器中要匹配的下一级筛选器 name: \"envoy.filters.http.router\" patch: operation: INSERT_BEFORE value: name: my.lua typed_config: \"@type\": \"type.googleapis.com/envoy.extensions.filters.http.lua.v3.Lua\" inlineCode: | function envoy_on_response(response_handle) response_handle:headers():add(\"api-version\", \"1.0\") end 再创建一个 filter，响应时给 api-version 头加上前缀 apiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata: name: myfilter-prefix namespace: istio-system spec: workloadSelector: labels: istio: ingressgateway configPatches: - applyTo: HTTP_FILTER match: context: GATEWAY proxy: proxyVersion: ^1\\.11.* listener: filterChain: filter: name: \"envoy.filters.network.http_connection_manager\" subFilter: name: \"my.lua\" patch: operation: INSERT_BEFORE # 在 my.lua 之前插入 value: name: myprefix.lua typed_config: \"@type\": \"type.googleapis.com/envoy.extensions.filters.http.lua.v3.Lua\" inlineCode: | function envoy_on_response(response_handle) local ver = response_handle:headers():get(\"Api-version\") response_handle:headers():replace(\"Api-version\", \"version_\"..ver) end filter 在请求时会按照从前向后的顺序执行，响应时则相反 ","date":"2023-02-05","objectID":"/posts/istio-envoyfilter/:1:1","tags":null,"title":"Istio Envoy Filter","uri":"/posts/istio-envoyfilter/"},{"categories":["Istio"],"content":"增加请求头 apiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata: name: myfilter-adduserid namespace: istio-system spec: workloadSelector: labels: istio: ingressgateway configPatches: - applyTo: HTTP_FILTER match: context: GATEWAY proxy: proxyVersion: ^1\\.11.* listener: filterChain: filter: name: \"envoy.filters.network.http_connection_manager\" subFilter: name: \"my.lua\" patch: operation: INSERT_BEFORE value: name: adduserid.lua typed_config: \"@type\": \"type.googleapis.com/envoy.extensions.filters.http.lua.v3.Lua\" inlineCode: | function envoy_on_request(request_handle) request_handle:headers():add(\"userid\", \"101\") end ","date":"2023-02-05","objectID":"/posts/istio-envoyfilter/:1:2","tags":null,"title":"Istio Envoy Filter","uri":"/posts/istio-envoyfilter/"},{"categories":["Istio"],"content":"查看动态配置 在 istio-ingressgateway 服务的 pod 中执行 curl http://localhost:15000/config_dump?resource=dynamic_listeners ","date":"2023-02-05","objectID":"/posts/istio-envoyfilter/:1:3","tags":null,"title":"Istio Envoy Filter","uri":"/posts/istio-envoyfilter/"},{"categories":["Istio"],"content":"打印 Lua 日志 默认情况只会打印 err 以上级别的日志，可以进入 pod 临时开启 curl -X POST http://localhost:15000/logging?level=info 输出 info 日志 function envoy_on_request(request_handle) local userid = request_handle:headers():get(\"userid\") request_handle:logInfo(\"userId=\"..userid) end ","date":"2023-02-05","objectID":"/posts/istio-envoyfilter/:1:4","tags":null,"title":"Istio Envoy Filter","uri":"/posts/istio-envoyfilter/"},{"categories":["Istio"],"content":"结束响应 如请求时没有携带 appid 头，则直接结束响应 apiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata: name: myfilter-checkappid namespace: istio-system spec: workloadSelector: labels: istio: ingressgateway configPatches: - applyTo: HTTP_FILTER match: context: GATEWAY proxy: proxyVersion: ^1\\.11.* listener: filterChain: filter: name: \"envoy.filters.network.http_connection_manager\" subFilter: name: \"envoy.filters.http.cors\" patch: operation: INSERT_AFTER # 在cors后插入，确保响应时携带跨域头 value: name: checkappid.lua typed_config: \"@type\": \"type.googleapis.com/envoy.extensions.filters.http.lua.v3.Lua\" inlineCode: | function envoy_on_request(request_handle) local appid = request_handle:headers():get(\"appid\") if(appid == nil) then request_handle:respond( {[\":status\"] = \"400\"}, \"error appid\" ) end end ","date":"2023-02-05","objectID":"/posts/istio-envoyfilter/:1:5","tags":null,"title":"Istio Envoy Filter","uri":"/posts/istio-envoyfilter/"},{"categories":["Istio"],"content":"Envoy 将 gRPC 转码为 HTTP/JSON 一旦有了一个可用的 gRPC 服务，可以通过向服务添加一些额外的注解（annotation）将其作为 HTTP/JSON API 发布。你需要一个代理来转换 HTTP/JSON 调用并将其传递给 gRPC 服务。我们称这个过程为转码。然后你的服务就可以通过 gRPC 和 HTTP/JSON 访问。 ","date":"2023-02-05","objectID":"/posts/istio-envoyfilter/:2:0","tags":null,"title":"Istio Envoy Filter","uri":"/posts/istio-envoyfilter/"},{"categories":["Istio"],"content":"步骤1：使用HTTP选项标注服务进行转码 在每个 rpc 操作的花括号中可以添加选项，允许你指定如何将操作转换到 HTTP 请求（endpoint）。在 proto 中引入 ‘ google/api/annotations.proto’ 即可使用该选项 import \"google/api/annotations.proto\"; 转码为 GET 方法 service ProdService { rpc GetProd(ProdRequest) returns (ProdResponse) { option (google.api.http) = { get: \"/detail/{prod_id}\" }; } } 在 URL 中有一个名为 prod_id 的路径变量，这个变量会自动映射到输入操作中同名的字段 转码为 POST 方法 service ProdService { rpc GetProd(ProdRequest) returns (ProdResponse) { option (google.api.http) = { post: \"/detail\" body: \"*\" }; } } ","date":"2023-02-05","objectID":"/posts/istio-envoyfilter/:2:1","tags":null,"title":"Istio Envoy Filter","uri":"/posts/istio-envoyfilter/"},{"categories":["Istio"],"content":"步骤2：生成 descriptor descriptor 文件是 ProtoBuf 提供的动态解析机制，通过提供对应类（对象）的 Descriptor 对象，在解析时就可以动态获取类成员 protoc --proto_path=gsrc/protos --include_imports --include_source_info --descriptor_set_out=prod.descriptor prod_service.proto ","date":"2023-02-05","objectID":"/posts/istio-envoyfilter/:2:2","tags":null,"title":"Istio Envoy Filter","uri":"/posts/istio-envoyfilter/"},{"categories":["Istio"],"content":"步骤3：转码 可以使用 grpc-transcoder 库 go get github.com/AliyunContainerService/grpc-transcoder 执行 grpc-transcoder --version 1.11 --service_port 80 --service_name gprodsvc.myistio --proto_svc ProdService --descriptor prod.descriptor service_port：service 端口 service_name：service 全路径名称 proto_svc：proto service 名称 descriptor：生成的 descriptor 文件 执行成功会在当前目录下生成一个 grpc-transcoder-envoyfilter.yaml 和 header2metadata-envoyfilter.yaml 文件 ","date":"2023-02-05","objectID":"/posts/istio-envoyfilter/:2:3","tags":null,"title":"Istio Envoy Filter","uri":"/posts/istio-envoyfilter/"},{"categories":["Istio"],"content":"步骤4：创建过滤器 apiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata: name: grpcfilter namespace: istio-system spec: workloadSelector: labels: istio: grpc-ingressgateway configPatches: - applyTo: HTTP_FILTER match: context: GATEWAY proxy: proxyVersion: ^1\\.11.* listener: filterChain: filter: name: \"envoy.filters.network.http_connection_manager\" subFilter: name: \"envoy.filters.http.router\" patch: operation: INSERT_BEFORE value: name: envoy.grpc_json_transcoder typed_config: '@type': \"type.googleapis.com/envoy.extensions.filters.http.grpc_json_transcoder.v3.GrpcJsonTranscoder\" proto_descriptor_bin: ... services: - ProdService print_options: add_whitespace: true always_print_primitive_fields: true always_print_enums_as_ints: false preserve_proto_field_names: false ","date":"2023-02-05","objectID":"/posts/istio-envoyfilter/:2:4","tags":null,"title":"Istio Envoy Filter","uri":"/posts/istio-envoyfilter/"},{"categories":["Istio"],"content":"通过 HTTP 访问服务 // 根据 ca 和证书获取 tlsConfig 配置对象 func getTLSConfig() *tls.Config { cert, err := tls.LoadX509KeyPair(\"tools/out/clientgrpc.crt\", \"tools/out/clientgrpc.key\") if err != nil { log.Fatal(err) } certPool := x509.NewCertPool() ca, err := os.ReadFile(\"tools/out/virtuallainCA.crt\") if err != nil { log.Fatal(err) } certPool.AppendCertsFromPEM(ca) return \u0026tls.Config{ Certificates: []tls.Certificate{cert}, ServerName: \"grpc.virtuallain.com\", RootCAs: certPool, } } func main() { req, _ := http.NewRequest(\"POST\", \"https://grpc.virtuallain.com:30090/detail\", strings.NewReader(`{\"prod_id\":101}`)) tr := \u0026http.Transport{ TLSClientConfig: getTLSConfig(), } client := http.Client{ Transport: tr, } rsp, _ := client.Do(req) fmt.Println(rsp.Header) defer rsp.Body.Close() b, _ := io.ReadAll(rsp.Body) fmt.Println(string(b)) } ","date":"2023-02-05","objectID":"/posts/istio-envoyfilter/:2:5","tags":null,"title":"Istio Envoy Filter","uri":"/posts/istio-envoyfilter/"},{"categories":["Istio"],"content":"Envoy 限流过滤器 Envoy 支持两种速率限制：全局和本地。本地限流是在envoy内部提供一种令牌桶限速的功能，全局限流需要访问外部限速服务。下面是一个使用全局限流的示例 ","date":"2023-02-05","objectID":"/posts/istio-envoyfilter/:3:0","tags":null,"title":"Istio Envoy Filter","uri":"/posts/istio-envoyfilter/"},{"categories":["Istio"],"content":"基本配置 1. 限流配置 这个 ConfigMap 是限速服务用到的配置文件，在 EnvoyFilter 中被引用。这里配置了 /prods 每分钟限流3个请求，其他 url 限流每分钟100个请求 apiVersion: v1 kind: ConfigMap metadata: name: ratelimit-config data: config.yaml: | domain: prod-ratelimit descriptors: - key: PATH value: \"/prods\" rate_limit: unit: minute requests_per_unit: 3 - key: PATH rate_limit: unit: minute requests_per_unit: 100 2. 独立限流服务 参考 官方 rate-limit-service.yaml apiVersion: v1 kind: Service metadata: name: redis labels: app: redis spec: ports: - name: redis port: 6379 selector: app: redis --- apiVersion: apps/v1 kind: Deployment metadata: name: redis spec: replicas: 1 selector: matchLabels: app: redis template: metadata: labels: app: redis spec: containers: - image: redis:alpine imagePullPolicy: Always name: redis ports: - name: redis containerPort: 6379 restartPolicy: Always serviceAccountName: \"\" --- apiVersion: v1 kind: Service metadata: name: ratelimit labels: app: ratelimit spec: ports: - name: http-port port: 8080 targetPort: 8080 protocol: TCP - name: grpc-port port: 8081 targetPort: 8081 protocol: TCP - name: http-debug port: 6070 targetPort: 6070 protocol: TCP selector: app: ratelimit --- apiVersion: apps/v1 kind: Deployment metadata: name: ratelimit spec: replicas: 1 selector: matchLabels: app: ratelimit strategy: type: Recreate template: metadata: labels: app: ratelimit spec: containers: - image: envoyproxy/ratelimit:6f5de117 # 2021/01/08 imagePullPolicy: Always name: ratelimit command: [\"/bin/ratelimit\"] env: - name: LOG_LEVEL value: debug - name: REDIS_SOCKET_TYPE value: tcp - name: REDIS_URL value: redis:6379 - name: USE_STATSD value: \"false\" - name: RUNTIME_ROOT value: /data - name: RUNTIME_SUBDIRECTORY value: ratelimit ports: - containerPort: 8080 - containerPort: 8081 - containerPort: 6070 volumeMounts: - name: config-volume mountPath: /data/ratelimit/config/config.yaml subPath: config.yaml volumes: - name: config-volume configMap: name: ratelimit-config 3. 创建 EnvoyFilter 这个 EnvoyFilter 作用在网关上，配置了 http 过滤器 envoy.filters.http.ratelimit，和一个 cluster。http 过滤器的 cluster 地址指向 cluster 配置的地址，就是 ratelimit service 所在的地址。domain 和步骤1中 configmap 的值一致，failure_mode_deny 表示超过请求限值就拒绝，rate_limit_service 配置 ratelimit 服务的地址（cluster），可以配置 grpc 类型或 http 类型 apiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata: name: filter-ratelimit namespace: istio-system spec: workloadSelector: # select by label in the same namespace labels: istio: ingressgateway configPatches: # The Envoy config you want to modify - applyTo: HTTP_FILTER match: context: GATEWAY listener: filterChain: filter: name: \"envoy.filters.network.http_connection_manager\" subFilter: name: \"envoy.filters.http.router\" patch: operation: INSERT_BEFORE # Adds the Envoy Rate Limit Filter in HTTP filter chain. value: name: envoy.filters.http.ratelimit typed_config: \"@type\": type.googleapis.com/envoy.extensions.filters.http.ratelimit.v3.RateLimit # domain can be anything! Match it to the ratelimter service config domain: prod-ratelimit failure_mode_deny: true rate_limit_service: grpc_service: envoy_grpc: cluster_name: rate_limit_cluster timeout: 10s transport_api_version: V3 - applyTo: CLUSTER match: cluster: service: ratelimit.default.svc.cluster.local patch: operation: ADD # Adds the rate limit service cluster for rate limit service defined in step 1. value: name: rate_limit_cluster type: STRICT_DNS connect_timeout: 10s lb_policy: ROUND_ROBIN http2_protocol_options: {} load_assignment: cluster_name: rate_limit_cluster endpoints: - lb_endpoints: - endpoint: address: socket_address: address: ratelimit.default.svc.cluster.local port_value: 8081 4. 创建 Action EnvoyFilter 这个 EnvoyFilter 作用在入口网关处，给80端口的虚拟主机配置了一个 rate_limits 动作，descriptor_key 用于选择在 configmap 里配置的 key apiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata: name: filter-ratelimit-svc namespace: istio-system spec: workloadSelector: labels: istio: ingressgateway configPatches: - applyTo: VIRTUAL_HOST match: context: GATEWAY routeConfiguration: vhost: name: \"p.virtuallai","date":"2023-02-05","objectID":"/posts/istio-envoyfilter/:3:1","tags":null,"title":"Istio Envoy Filter","uri":"/posts/istio-envoyfilter/"},{"categories":["Istio"],"content":"使用 header_value_match 参考 文档 修改 Action EnvoyFilter 下面第一个 action 配置了 /prods/\\d+ 路由规则的匹配。第二个 action 配置了存在头 version=v2 的匹配 apiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata: name: filter-ratelimit-svc namespace: istio-system spec: workloadSelector: labels: istio: ingressgateway configPatches: - applyTo: VIRTUAL_HOST match: context: GATEWAY routeConfiguration: vhost: name: \"p.virtuallain.com:80\" route: action: ANY patch: operation: MERGE # Applies the rate limit rules. value: rate_limits: - actions: - header_value_match: descriptor_value: path headers: - name: :path # exact_match: /prods safe_regex_match: # 正则匹配 google_re2: {} regex: /prods/\\d+ - actions: - header_value_match: descriptor_value: version-v2 headers: - name: version exact_match: v2 基本的匹配方式有： exact_match：精确匹配 safe_regex_match：正则匹配 range_match：范围匹配（数字范围，如[-10,0)） prefix_match：前缀匹配 suffix_match：后缀匹配 contains_match：包含匹配 invert_match：反向匹配 修改 ConfigMap 配置 下面配置了 /prods/\\d+ 的路由每分钟限流5次，当存在 header 头 version=v2 时每分钟限流2次。同时匹配到多个规则时优先生效次数少的规则。value 关联 header_value_match 里的 descriptor_value apiVersion: v1 kind: ConfigMap metadata: name: ratelimit-config data: config.yaml: | domain: prod-ratelimit descriptors: - key: header_match value: path rate_limit: requests_per_unit: 5 unit: minute - key: header_match value: version-v2 rate_limit: requests_per_unit: 2 unit: minute ","date":"2023-02-05","objectID":"/posts/istio-envoyfilter/:3:2","tags":null,"title":"Istio Envoy Filter","uri":"/posts/istio-envoyfilter/"},{"categories":["Istio"],"content":"IP 限流 修改 Action EnvoyFilter rate_limits: - actions: - remote_address: {} 修改 ConfigMap 配置 data: config.yaml: | domain: prod-ratelimit descriptors: - key: remote_address rate_limit: requests_per_unit: 10 unit: minute X-Forwarded-For 配置 当存在多个受信任代理的环境中，需要配置生效的 XFF 是第几个，参考 文档。实际运行可以用 nginx-ingress 来反代 istio 的 gateway 从而自动传递这个头 apiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata: name: xff-trust-hops namespace: istio-system spec: workloadSelector: labels: istio: ingressgateway configPatches: - applyTo: NETWORK_FILTER match: context: ANY listener: filterChain: filter: name: \"envoy.filters.network.http_connection_manager\" patch: operation: MERGE value: typed_config: \"@type\": \"type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\" use_remote_address: true xff_num_trusted_hops: 1 # Change as needed ","date":"2023-02-05","objectID":"/posts/istio-envoyfilter/:3:3","tags":null,"title":"Istio Envoy Filter","uri":"/posts/istio-envoyfilter/"},{"categories":["Istio"],"content":"IP 组合条件限流 修改 Action EnvoyFilter rate_limits: - actions: - header_value_match: descriptor_value: path headers: - name: :path safe_regex_match: google_re2: {} regex: /prods/\\d+ - remote_address: {} 修改 ConfigMap 配置 下面配置了每个 ip 在 /prods/\\d+ 的路由每分钟限流5次 data: config.yaml: | domain: prod-ratelimit descriptors: - key: header_match value: path descriptors: - key: remote_address rate_limit: requests_per_unit: 5 unit: minute ","date":"2023-02-05","objectID":"/posts/istio-envoyfilter/:3:4","tags":null,"title":"Istio Envoy Filter","uri":"/posts/istio-envoyfilter/"},{"categories":["Istio"],"content":"自定义限流服务 上面使用的是官方 envoyproxy/ratelimit 限流服务，也可以自己实现一个。示例： package main import ( \"context\" pb \"github.com/envoyproxy/go-control-plane/envoy/service/ratelimit/v3\" \"google.golang.org/grpc\" \"log\" \"net\" \"time\" ) type MyServer struct{} func NewMyServer() *MyServer { return \u0026MyServer{} } // 实现限流方法 func (s *MyServer) ShouldRateLimit(ctx context.Context, request *pb.RateLimitRequest) (*pb.RateLimitResponse, error) { var overallCode pb.RateLimitResponse_Code if time.Now().Unix()%2 == 0 { log.Println(\"限流了\") overallCode = pb.RateLimitResponse_OVER_LIMIT } else { log.Println(\"通过了\") overallCode = pb.RateLimitResponse_OK } response := \u0026pb.RateLimitResponse{OverallCode: overallCode} return response, nil } func main() { lis, err := net.Listen(\"tcp\", \":8080\") if err != nil { log.Fatal(err) } s := grpc.NewServer() pb.RegisterRateLimitServiceServer(s, NewMyServer()) if err := s.Serve(lis); err != nil { log.Fatal(err) } } ","date":"2023-02-05","objectID":"/posts/istio-envoyfilter/:3:5","tags":null,"title":"Istio Envoy Filter","uri":"/posts/istio-envoyfilter/"},{"categories":["Istio"],"content":"gRPC 环境 gRPC 是 Google公司基于 Protobuf 开发的跨语言的开源 RPC 框架。gRPC 基于 HTTP/2 协议设计，可以基于一个HTTP/2链接提供多个服务 ","date":"2023-02-04","objectID":"/posts/istio-grpc/:1:0","tags":null,"title":"Istio 部署 gRPC 服务并配置网关证书","uri":"/posts/istio-grpc/"},{"categories":["Istio"],"content":"安装 1. protobuf 从 protobuf 这里下载，把 bin 目录加入环境变量 2. go protobuf 库 go get -u github.com/golang/protobuf@v1.5.2 3. go 插件 go install google.golang.org/protobuf/cmd/protoc-gen-go@v1.28 该插件会根据 .proto 文件生成一个后缀为 .pb.go 的文件，包含所有 .proto 文件中定义的类型及其序列化方法 go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@v1.2 该插件会生成一个后缀为 _grpc.pb.go 的文件，其中包含： 一种接口类型(或存根) ，供客户端调用的服务方法。 服务器要实现的接口类型。 上述命令会默认将插件安装到 $GOPATH/bin，为了 protoc 编译器能找到这些插件，请确保你的$GOPATH/bin在环境变量中 4. grpc 库 go get -u google.golang.org/grpc@v1.46.2 ","date":"2023-02-04","objectID":"/posts/istio-grpc/:1:1","tags":null,"title":"Istio 部署 gRPC 服务并配置网关证书","uri":"/posts/istio-grpc/"},{"categories":["Istio"],"content":"编写 .proto 定义服务 src/prod_model.proto: syntax = \"proto3\"; option go_package = \"src/pbfiles\"; message ProdRequest { int32 prod_id =1; } message ProdModel { int32 id=1; string name=2; } message ProdResponse { ProdModel result=1; } src/prod_service.proto: syntax = \"proto3\"; import \"prod_model.proto\"; option go_package = \"src/pbfiles\"; service ProdService { rpc GetProd(ProdRequest) returns (ProdResponse); } 在项目更目录执行以下命令，根据 proto 生成 go 源码文件 protoc320 --proto_path=src --go_out=./ prod_model.proto protoc320 --proto_path=src --go-grpc_out=./ prod_service.proto 编写 server 端： type ProdService struct { pbfiles.UnimplementedProdServiceServer } func NewProdService() *ProdService { return \u0026ProdService{} } func (this *ProdService) GetProd(ctx context.Context, req *pbfiles.ProdRequest) (*pbfiles.ProdResponse, error) { model := \u0026pbfiles.ProdModel{ Id: req.ProdId, Name: fmt.Sprintf(\"%s%d\", \"测试商品\", req.ProdId), } rsp := \u0026pbfiles.ProdResponse{Result: model} return rsp, nil } func main() { myserver := grpc.NewServer() // 创建服务 pbfiles.RegisterProdServiceServer(myserver, services.NewProdService()) // 监听8080 lis, _ := net.Listen(\"tcp\", \":8080\") if err := myserver.Serve(lis); err != nil { log.Fatal(err) } } ","date":"2023-02-04","objectID":"/posts/istio-grpc/:1:2","tags":null,"title":"Istio 部署 gRPC 服务并配置网关证书","uri":"/posts/istio-grpc/"},{"categories":["Istio"],"content":"测试客户端 func main() { client, err := grpc.DialContext(context.Background(), \":8080\", grpc.WithInsecure()) rsp := \u0026pbfiles.ProdResponse{} err = client.Invoke(context.Background(), \"/ProdService/GetProd\", \u0026pbfiles.ProdRequest{ProdId: 123}, rsp) if err != nil { log.Fatal(err) } fmt.Println(rsp.Result) } ","date":"2023-02-04","objectID":"/posts/istio-grpc/:1:3","tags":null,"title":"Istio 部署 gRPC 服务并配置网关证书","uri":"/posts/istio-grpc/"},{"categories":["Istio"],"content":"部署网格内 gRPC 服务 示例如下 ","date":"2023-02-04","objectID":"/posts/istio-grpc/:2:0","tags":null,"title":"Istio 部署 gRPC 服务并配置网关证书","uri":"/posts/istio-grpc/"},{"categories":["Istio"],"content":"创建 gRPC Gate 网关 demo.yaml apiVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: components: egressGateways: - enabled: true name: istio-egressgateway ingressGateways: - enabled: true name: istio-ingressgateway - enabled: true label: app: grpc-ingressgateway # 自定义标签 istio: grpc-ingressgateway k8s: resources: requests: cpu: 10m memory: 40Mi service: ports: - name: status-port port: 15021 targetPort: 15021 - name: http2 port: 80 targetPort: 8080 - name: https port: 443 targetPort: 8443 - name: tcp port: 31400 targetPort: 31400 - name: tls port: 15443 targetPort: 15443 name: grpc-ingressgateway 执行部署： istioctl install -f demo.yaml ","date":"2023-02-04","objectID":"/posts/istio-grpc/:2:1","tags":null,"title":"Istio 部署 gRPC 服务并配置网关证书","uri":"/posts/istio-grpc/"},{"categories":["Istio"],"content":"创建网关规则 apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: grpc-gateway namespace: myistio spec: selector: istio: grpc-ingressgateway servers: - port: number: 80 name: grpc protocol: HTTPS hosts: - \"*\" ","date":"2023-02-04","objectID":"/posts/istio-grpc/:2:2","tags":null,"title":"Istio 部署 gRPC 服务并配置网关证书","uri":"/posts/istio-grpc/"},{"categories":["Istio"],"content":"创建虚拟服务 apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: grpcvs namespace: myistio spec: hosts: - \"*\" gateways: - grpc-gateway http: - route: - destination: host: gprodsvc port: number: 80 ","date":"2023-02-04","objectID":"/posts/istio-grpc/:2:3","tags":null,"title":"Istio 部署 gRPC 服务并配置网关证书","uri":"/posts/istio-grpc/"},{"categories":["Istio"],"content":"配置网关证书 ","date":"2023-02-04","objectID":"/posts/istio-grpc/:3:0","tags":null,"title":"Istio 部署 gRPC 服务并配置网关证书","uri":"/posts/istio-grpc/"},{"categories":["Istio"],"content":"配置方式 1. 文件挂载的方式 Istio 网关将会自动加载 istio-system 命名空间下名称为 stio-ingressgateway-certs 的 secret，并分别挂载到 /etc/istio/ingressgateway-certs/tls.crt 和 /etc/istio/ingressgateway-certs/tls.key。指定 serverCertificate 和 privateKey 字段 2. 指定密钥的方式 指定 credentialName 字段 ","date":"2023-02-04","objectID":"/posts/istio-grpc/:3:1","tags":null,"title":"Istio 部署 gRPC 服务并配置网关证书","uri":"/posts/istio-grpc/"},{"categories":["Istio"],"content":"配置 HTTP TLS 证书 kubectl create -n istio-system secret tls istio-ingressgateway-certs --key api.virtuallain.com.key --cert api.virtuallain.com.crt Gateway 加入 tls 节点： apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: p-gateway namespace: myistio spec: selector: istio: ingressgateway # use Istio default gateway implementation servers: - port: number: 80 name: http protocol: HTTP hosts: - api.virtuallain.com - port: number: 443 name: https protocol: HTTPS tls: mode: SIMPLE serverCertificate: /etc/istio/ingressgateway-certs/tls.crt # 使用挂载证书文件 privateKey: /etc/istio/ingressgateway-certs/tls.key # credentialName: ssl-ingressgateway-certs hosts: - api.virtuallain.com ","date":"2023-02-04","objectID":"/posts/istio-grpc/:3:2","tags":null,"title":"Istio 部署 gRPC 服务并配置网关证书","uri":"/posts/istio-grpc/"},{"categories":["Istio"],"content":"配置 gRPC 证书（单向认证） 使用 certstrap 开源库生成证书 1. 自签 CA 证书 ./cert init --common-name \"virtuallainCA\" --expires \"20 years\" 2. 服务端证书 # 得到证书请求文件 ./cert request-cert -cn grpc.virtuallain.com -domain \"*.virtuallain.com\" # CA去签名请求文件 ./cert sign grpc.virtuallain.com --CA virtuallainCA 3. 导入 k8s kubectl create -n istio-system secret tls grpc-ingressgateway-certs --key=grpc.virtuallain.com.key --cert=grpc.virtuallain.com.crt # 效果同上 kubectl create -n istio-system secret generic grpc-ingressgateway-certs --from-file=key=grpc.virtuallain.com.key --from-file=cert=grpc.virtuallain.com.crt 4. 配置 Gateway apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: grpc-gateway namespace: myistio spec: selector: istio: grpc-ingressgateway servers: - port: number: 80 name: grpc protocol: HTTPS tls: mode: SIMPLE credentialName: grpc-ingressgateway-certs hosts: - \"*\" 5. 客户端测试 func main() { creds, err := credentials.NewClientTLSFromFile(\"tools/out/grpc.virtuallain.com.crt\", \"grpc.virtuallain.com\") if err != nil { log.Fatal(err) } client, err := grpc.DialContext(context.Background(), \"grpc.virtuallain.com:30090\", grpc.WithTransportCredentials(creds)) if err != nil { log.Fatal(err) } rsp := \u0026pbfiles.ProdResponse{} err = client.Invoke(context.Background(), \"/ProdService/GetProd\", \u0026pbfiles.ProdRequest{ProdId: 123}, rsp) if err != nil { log.Fatal(err) } fmt.Println(rsp.Result) } ","date":"2023-02-04","objectID":"/posts/istio-grpc/:3:3","tags":null,"title":"Istio 部署 gRPC 服务并配置网关证书","uri":"/posts/istio-grpc/"},{"categories":["Istio"],"content":"配置 gRPC 证书（双向认证） 1. 同步生成客户端证书 ./cert request-cert -cn clientgrpc ./cert sign clientgrpc --CA virtuallainCA 2. 重新导入证书 kubectl create -n istio-system secret generic grpc-ingressgateway-certs --from-file=key=grpc.virtuallain.com.key --from-file=cert=grpc.virtuallain.com.crt --from-file=cacert=virtuallainCA.crt 3. 配置 Gateway 修改 mode 为 MUTUAL tls: mode: MUTUAL credentialName: grpc-ingressgateway-certs 4. 客户端测试 func main() { cert, err := tls.LoadX509KeyPair(\"tools/out/clientgrpc.crt\", \"tools/out/clientgrpc.key\") if err != nil { log.Fatal(err) } certPool := x509.NewCertPool() ca, err := os.ReadFile(\"tools/out/virtuallainCA.crt\") if err != nil { log.Fatal(err) } certPool.AppendCertsFromPEM(ca) creds := credentials.NewTLS(\u0026tls.Config{ Certificates: []tls.Certificate{cert}, ServerName: \"grpc.virtuallain.com\", RootCAs: certPool, }) client, err := grpc.DialContext(context.Background(), \"grpc.virtuallain.com:30090\", grpc.WithTransportCredentials(creds)) rsp := \u0026pbfiles.ProdResponse{} err = client.Invoke(context.Background(), \"/ProdService/GetProd\", \u0026pbfiles.ProdRequest{ProdId: 123}, rsp) if err != nil { log.Fatal(err) } fmt.Println(rsp.Result) } ","date":"2023-02-04","objectID":"/posts/istio-grpc/:3:4","tags":null,"title":"Istio 部署 gRPC 服务并配置网关证书","uri":"/posts/istio-grpc/"},{"categories":["Istio"],"content":"Istio 是一个开源的微服务管理、保护和监控框架，它有如下特性： 流量管理：利用配置，我们可以控制服务间的流量。设置断路器、超时或重试都可以通过简单的配置改变来完成。 可观察性：Istio 通过跟踪、监控和记录让我们更好地了解服务，让我们能够快速发现和修复问题。 安全性：Istio 可以在代理层面上管理认证、授权和通信的加密。我们可以通过快速的配置变更在各个服务中执行策略。 ","date":"2023-02-02","objectID":"/posts/istio/:0:0","tags":null,"title":"Istio 快速入门","uri":"/posts/istio/"},{"categories":["Istio"],"content":"Istio 组件 Istio 服务网格有两个部分：数据平面和控制平面 数据平面由一组智能代理 (Envoy) 组成，被部署为 sidecar，控制服务之间的通信 控制平面管理并配置代理来进行流量路由 下图展示了组成每个平面的不同组件： ","date":"2023-02-02","objectID":"/posts/istio/:1:0","tags":null,"title":"Istio 快速入门","uri":"/posts/istio/"},{"categories":["Istio"],"content":"安装 从 Github 上下载 Istio， (测试环境) 执行 istioctl manifest apply --set profile=demo 可以查看其中的配置 istioctl profile dump demo ","date":"2023-02-02","objectID":"/posts/istio/:2:0","tags":null,"title":"Istio 快速入门","uri":"/posts/istio/"},{"categories":["Istio"],"content":"Sidecar 注入 网格中的 Pod 必须运行一个 Istio Sidecar 代理，两种方法：使用 istioctl 手动注入或启用 Pod 所属命名空间的 Istio sidecar 注入器自动注入，启用自动注入后，自动注入器会使用准入控制器在创建 Pod 时自动注入代理配置 ","date":"2023-02-02","objectID":"/posts/istio/:3:0","tags":null,"title":"Istio 快速入门","uri":"/posts/istio/"},{"categories":["Istio"],"content":"手动注入 istioctl kube-inject -f api.yaml | kubectl apply -f – ","date":"2023-02-02","objectID":"/posts/istio/:3:1","tags":null,"title":"Istio 快速入门","uri":"/posts/istio/"},{"categories":["Istio"],"content":"自动注入 将 myistio 命名空间标记为 istio-injection=enabled kubectl label namespace myistio istio-injection=enabled ","date":"2023-02-02","objectID":"/posts/istio/:3:2","tags":null,"title":"Istio 快速入门","uri":"/posts/istio/"},{"categories":["Istio"],"content":"示例配置 apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: p-gateway namespace: myistio spec: selector: istio: ingressgateway # use Istio default gateway implementation servers: - port: number: 80 name: http protocol: HTTP hosts: - p.virtuallain.com --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: prodvs namespace: myistio spec: hosts: - prodsvc - reviewsvc - p.virtuallain.com gateways: - p-gateway - mesh # 用于内部服务互访 http: - match: - uri: prefix: \"/p\" rewrite: uri: \"/prods\" # 路径重写 route: - destination: host: prodsvc subset: v1svc # 服务端点 port: number: 80 fault: # 延迟故障注入 delay: fixedDelay: 1ms percentage: value: 15 timeout: 1s - match: - uri: prefix: / route: - destination: host: reviewsvc port: number: 80 # fault: # abort: # httpStatus: 500 # percentage: # value: 100 --- apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: prod-rule namespace: myistio spec: host: prodsvc trafficPolicy: loadBalancer: # 负载均衡 # simple: ROUND_ROBIN consistentHash: httpHeaderName: myname connectionPool: # 限流 tcp: maxConnections: 1 http: http1MaxPendingRequests: 1 maxRequestsPerConnection: 1 outlierDetection: # 熔断 consecutive5xxErrors: 2 interval: 10s maxEjectionPercent: 50 baseEjectionTime: 10s subsets: # 定义服务端点集合 - name: v1svc labels: # version: v2 app: prod ","date":"2023-02-02","objectID":"/posts/istio/:4:0","tags":null,"title":"Istio 快速入门","uri":"/posts/istio/"},{"categories":["Istio"],"content":"Gateway 网格服务的负载均衡器，负责网格边缘的服务暴露。可以使用 Gateway 资源来配置网关，网关资源描述了负责均衡器的暴露端口、协议、SNI (服务器名称指示) 配置等。网关资源在背后控制着 Envoy 代理在网络接口上的监听方式以及它出示的证书，其中 Ingress-gateway: 用于管理网格边缘入站的流量，通过入口网关将网格内部的服务暴露到外部提供访问，配合 VirtualService Egress gateway: 控制网格内服务访问外部服务，配合 DestinationRule ","date":"2023-02-02","objectID":"/posts/istio/:5:0","tags":null,"title":"Istio 快速入门","uri":"/posts/istio/"},{"categories":["Istio"],"content":"Virtual Service 虚拟服务可以理解为 k8s service 的一种增强，我们可以使用 VirtualService 资源在 Istio 服务网格中定义流量路由规则，并在客户端试图连接到服务时应用这些规则 配置如何在服务网格内将请求路由到服务 和网关整合并配置流量规则来控制出入流量 ","date":"2023-02-02","objectID":"/posts/istio/:6:0","tags":null,"title":"Istio 快速入门","uri":"/posts/istio/"},{"categories":["Istio"],"content":"Destination Rule 目标规则用于配置将流量转发到实际工作负载时应用的策略，如流量拆分、灰度发布、负载均衡等 subset (子集) 是服务端点的集合，可以用于 A/B 测试或者分版本路由等场景 ","date":"2023-02-02","objectID":"/posts/istio/:7:0","tags":null,"title":"Istio 快速入门","uri":"/posts/istio/"},{"categories":["Istio"],"content":"负载均衡器设置 基本配置参数： 简单类型 (simple)： ROUND_ROBIN: 轮询 (默认) LEAST_CONN: 最少连接，选择请求较少的主机 RANDOM: 随机选择 一致性哈希 (consistentHash): 目的是让同一用户的请求一直转发到后端同一实例 httpHeaderName: 基于 header 头 httpCookie: 基于 cookie useSourceIp: 基于 ip minimumRingSize: 环形哈希算法，适用于较多节点、频繁新增和删除节点的场景，填数字虚拟节点数量 ","date":"2023-02-02","objectID":"/posts/istio/:7:1","tags":null,"title":"Istio 快速入门","uri":"/posts/istio/"},{"categories":["Istio"],"content":"connectionPool 限流设置 ConnectionPool 可以对上游服务的并发连接数和请求数进行限制。适用于 HTTP 和 TCP 服务 基本配置参数： TCP maxConnections: 最大 HTTP1 / TCP 连接数 (默认值2 ^ 32-1) connectTimeout: 连接超时。格式为 1h / 1m / 1s / 1ms (默认值为10秒) tcpKeepalive: keepalive 设置 probes: 确认连接dead之前继续发送探测数据，发送几次 (默认9) time: 发送probe之前连接的空闲时间 (默认2小时) interval: 两次probe发送的间隔 (默认75秒) HTTP http1MaxPendingRequests: 等待时将排队的最大请求数 就绪的连接池连接 (默认为1024) http2MaxRequests: 对目标的活动请求的最大数量 (默认为1024) maxRequestsPerConnection: 每个连接的最大请求数量。如果将这一参数设置为 1 则会禁止 keepalive 特性 idleTimeout: 上游连接池连接的空闲超时，当达到空闲超时时，连接将被关闭 maxRetries: 最大重试次数 (默认为3) ","date":"2023-02-02","objectID":"/posts/istio/:7:2","tags":null,"title":"Istio 快速入门","uri":"/posts/istio/"},{"categories":["Istio"],"content":"outlierDetection 熔断设置 跟踪每个状态的断路器实现上游服务中的单个主机。适用于 HTTP 和 TCP 服务 基本配置参数： consecutive5xxErrors: 连续 5xx 错误异常数 interval: 错误异常的扫描间隔 (默认10秒) ，期间连续发生指定数量个异常则熔断 baseEjectionTime: 驱逐时间 (默认30秒) maxEjectionPercent: 最大驱逐百分比 (默认10%) minHealthPercent: 至少最低健康百分比的主机，就将启用异常检测。当负载平衡池中的正常主机百分比降至此阈值以下时，异常检测将被禁用默认值为0％，因为它通常不适用于每项服务只有少量 pod 的 k8s 环境 ","date":"2023-02-02","objectID":"/posts/istio/:7:3","tags":null,"title":"Istio 快速入门","uri":"/posts/istio/"},{"categories":["Istio"],"content":"故障注入 为了帮助我们提高服务的弹性，我们可以使用故障注入功能。我们可以在 HTTP 流量上应用故障注入策略，在转发目的地的请求时指定一个或多个故障注入 有两种类型的故障注入。我们可以在转发前延迟（delay）请求，模拟缓慢的网络或过载的服务，我们可以中止（abort） HTTP 请求，并返回一个特定的 HTTP 错误代码给调用者。通过中止，我们可以模拟一个有故障的上游服务 ","date":"2023-02-02","objectID":"/posts/istio/:8:0","tags":null,"title":"Istio 快速入门","uri":"/posts/istio/"},{"categories":["Istio"],"content":"配置网关 JWT 验证 Istio 支持使用 JWT 对终端用户进行身份验证，支持多种 JWT 签名算法，常见的 JWT 签名算法： HS256: 对称加密，加密和解密用的是同一个秘钥 RS256: RSA 私钥签名，公钥进行验证 ES256: 类似 RS256 Istio 要求提供 JWKS 格式的信息，用于 JWT 签名验证，是一个 JSON 格式的文件，如： { \"keys\":[ { \"e\":\"AQAB\", \"kid\":\"DHFbpoIUqrY8t2zpA2qXfCmr5VO5ZEr4RzHU_-envvQ\", \"kty\":\"RSA\", \"n\":\"xAE7eB6qugXyCAG3yhh7pkD...\" } ] } JWKS 描述一组 JWK 密钥。它能同时描述多个可用的公钥，应用场景之一是密钥的 Rotate。而 JWK，全称是 Json Web Key，它描述了一个加密密钥（公钥或私钥）的各项属性，包括密钥的值。Istio 使用 JWK 描述验证 JWT 签名所需要的信息。在使用 RSA 签名算法时，JWK 描述的应该是用于验证的 RSA 公钥。一个 RSA 公钥的 JWK 描述如下： { \"alg\": \"RS256\", # 算法「可选参数」 \"kty\": \"RSA\", # 密钥类型 \"use\": \"sig\", # 被用于签名「可选参数」 \"kid\": \"NjVBRjY5MDlCMUIwNzU4RTA2QzZFMDQ4QzQ2MDAyQjVDNjk1RTM2Qg\", # key 的唯一 id \"n\": \"yeNlzlub94YgerT030codqEztjfU_S6X4DbDA_iVKkjAWtYfPHDzz_sPCT1Axz6isZdf3lHpq_gYX4Sz-cbe4rjmigxUxr-FgKHQy3HeCdK6hNq9ASQvMK9LBOpXDNn7mei6RZWom4wo3CMvvsY1w8tjtfLb-yQwJPltHxShZq5-ihC9irpLI9xEBTgG12q5lGIFPhTl_7inA1PFK97LuSLnTJzW0bj096v_TMDg7pOWm_zHtF53qbVsI0e3v5nmdKXdFf9BjIARRfVrbxVxiZHjU6zL6jY5QJdh1QCmENoejj_ytspMmGW7yMRxzUqgxcAqOBpVm0b-_mW3HoBdjQ\", \"e\": \"AQAB\" } RSA 是基于大数分解的加密/签名算法，上述参数中，e 是公钥的模数(modulus)，n 是公钥的指数(exponent)，两个参数都是 base64 字符串。 JWK 中 RSA 公钥的具体定义参见 RSA Keys - JSON Web Algorithms (JWA) ","date":"2023-02-02","objectID":"/posts/istio/:9:0","tags":null,"title":"Istio 快速入门","uri":"/posts/istio/"},{"categories":["Istio"],"content":"JWK 生成 RS256 使用 RSA 算法进行签名，可通过如下命令生成 RSA 密钥： # 1. 生成 2048 位（不是 256 位）的 RSA 密钥 openssl genrsa -out rsa-private-key.pem 2048 # 2. 通过密钥生成公钥 openssl rsa -in rsa-private-key.pem -pubout -out rsa-public-key.pem 接下来使用 jwx 库生成 JWK func pubKey() []byte { f, _ := os.Open(\"./rsa-public-key.pem\") b, _ := io.ReadAll(f) return b } func main() { key, err := jwk.ParseKey(pubKey(), jwk.WithPEM(true)) if err != nil { log.Fatalln(err) } if rsaKey, ok := key.(jwk.RSAPublicKey); ok { b, err := json.Marshal(rsaKey) if err != nil { log.Fatalln(err) } fmt.Println(string(b)) } } 可以在 jwt.io 中测试密钥的可用性和生成 Token ","date":"2023-02-02","objectID":"/posts/istio/:9:1","tags":null,"title":"Istio 快速入门","uri":"/posts/istio/"},{"categories":["Istio"],"content":"定义 RequestAuthentication 启用 Istio 的身份验证 apiVersion: security.istio.io/v1beta1 kind: RequestAuthentication metadata: name: jwt-test namespace: istio-system spec: selector: matchLabels: istio: ingressgateway # 在带有这些 labels 的 ingressgateway/sidecar 上生效 jwtRules: # issuer 即签发者，需要和 JWT payload 中的 iss 属性完全一致 - issuer: \"user@virtuallain.com\" jwks: | { \"keys\": [ { \"e\":\"AQAB\", \"kty\":\"RSA\", \"n\":\"tFLKvS2EMOu3vgPnUPkdn5xVau9-dWf0z30_EdbpadQLiVsHH0FqWl-8CgtNtxnUjrI6WN__BMX8jLzvEqKrdZnbTMS0EaTh8lfGbFxNd0qziVHlYZTH-gtPNI4r815y9OuY7DEuR8fG-B_iHuCslN3BcJ4TDF_tzKCF0USGzzEiiRPR4SBtZgz0tmteQgRTv1NfciOwCtedEtXRKnGI5W1GV5u2dmF6UCiWJdgsqHMsVzTXJz_wliVvKhczwhrFZfqvdBoOe_aays89AjcO4x7eUntZVvOlkowaD-UeUeT6ZL8q4oTWGpswA4YNJ_daZmtAU5ho11EW6F3q1YHjVQ\" } ] } # jwks 或 jwksUri 二选其一 (测试中发现 jwksUri 会出现各种问题) # jwksUri: \"http://nginx.test.local/istio/jwks.json\" forwardOriginalToken: true # 转发 Authorization 请求头 outputPayloadToHeader: \"Userinfo\" # 转发 jwt payload 数据 (base64编码) 可以看到 jwtRules 是一个列表，因此可以为每个 issuers 配置不同的 jwtRule. 对同一个 issuers（jwt 签发者），可以通过 jwks 设置多个公钥，以实现JWT签名密钥的轮转。 JWT 的验证规则是： JWT 的 payload 中有 issuer 属性，首先通过 issuer 匹配到对应的 istio 中配置的 jwks。 JWT 的 header 中有 kid 属性，第二步在 jwks 的公钥列表中，中找到 kid 相同的公钥。 使用找到的公钥进行 JWT 签名验证。 配置中的 spec.selector 可以省略，这样会直接在整个 namespace 中生效，而如果是在 istio-system 名字空间，该配置将在全集群的所有 sidecar/ingressgateway 上生效 加了转发后，流程图如下： ","date":"2023-02-02","objectID":"/posts/istio/:9:2","tags":null,"title":"Istio 快速入门","uri":"/posts/istio/"},{"categories":["Istio"],"content":"跨域配置 给 vs 增加 corsPolicy 节点 apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: prodvs namespace: myistio spec: hosts: - prodsvc - p.virtuallain.com gateways: - pp-gateway - mesh http: - match: - uri: prefix: / route: - destination: host: prodsvc subset: v1svc port: number: 80 corsPolicy: allowOrigins: - exact: \"*\" allowMethods: - GET - POST - PATCH - PUT - DELETE - OPTIONS allowCredentials: true allowHeaders: - authorization maxAge: \"24h\" ","date":"2023-02-02","objectID":"/posts/istio/:9:3","tags":null,"title":"Istio 快速入门","uri":"/posts/istio/"},{"categories":["Istio"],"content":"定义 Envoy Filter RequestsAuthentication 不支持自定义响应头信息，这导致对于前后端分离的 Web API 而言， 一旦 JWT 失效，Istio 会直接将 401 返回给前端 Web 页面。 因为响应头中不包含 Access-Crontrol-Allow-Origin，响应将被浏览器拦截，需要通过 EnvoyFilter 自定义响应头，添加跨域信息 apiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata: name: reorder-cors-before-jwt namespace: istio-system spec: workloadSelector: labels: istio: ingressgateway configPatches: - applyTo: HTTP_FILTER match: context: GATEWAY listener: filterChain: filter: name: \"envoy.filters.network.http_connection_manager\" subFilter: name: \"envoy.filters.http.cors\" patch: operation: REMOVE - applyTo: HTTP_FILTER match: context: GATEWAY listener: filterChain: filter: name: \"envoy.filters.network.http_connection_manager\" subFilter: name: \"envoy.filters.http.jwt_authn\" patch: operation: INSERT_BEFORE value: name: \"envoy.filters.http.cors\" typed_config: \"@type\": \"type.googleapis.com/envoy.extensions.filters.http.cors.v3.Cors\" ","date":"2023-02-02","objectID":"/posts/istio/:9:4","tags":null,"title":"Istio 快速入门","uri":"/posts/istio/"},{"categories":["Istio"],"content":"Authorization Policy Istio 允许我们使用 AuthorizationPolicy (授权策略) 资源在网格、命名空间和工作负载层面定义访问控制。 支持 DENY、ALLOW、AUDIT 和 CUSTOM 操作。 rule 规则包括 from (来源)、to (操作) 和 when (条件) 来源 principals (如 my-service-account): 具有 my-service-account 的工作负载，需要开启 mTLS (双向认证) requestPrincipals (如 my-issuer/hello): 具有有效 JWT 和请求主体 my-issuer/hello 的工作负载 namespaces (如 default): 任何来自 default 命名空间的工作负载 ipBlocks (如 [“1.2.3.4”, “9.8.7.6/15”]): 任何 IP 或者 CIDR 块的 IP 的工作负载 remoteIpBlocks: 针对 remote.ip (如 X-Forwarded-For) 每个选项都有一个 notXxx 作为反义词 操作 hosts 和 notHosts ports 和 notPorts methods 和 notMethods path 和 notPath 所有这些操作都适用于请求属性。例如，要在一个特定的请求路径上进行匹配，我们可以使用路径 (如 [\"/api/*\", “/admin”]) 或特定的端口 ([“8080”]) 条件 为了指定条件，我们必须提供一个 key 字段，key 字段是一个 Istio 属性的名称。例如 request.headers、source.ip、request.auth.claims[xx] 等。条件的第二部分是 values 或 notValues 的字符串列表 apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: prod-authpolicy namespace: istio-system spec: action: ALLOW selector: matchLabels: istio: ingressgateway rules: - from: - source: requestPrincipals: [\"*\"] to: - operation: methods: [\"GET\"] paths: [\"/prods/*\"] - to: - operation: methods: [\"GET\",\"POST\"] paths: [\"/admin\"] when: # payload 中必须包含值为 admin 或 superadmin 的 role 字段 - key: request.auth.claims[role] values: [\"admin\",\"superadmin\"] ","date":"2023-02-02","objectID":"/posts/istio/:10:0","tags":null,"title":"Istio 快速入门","uri":"/posts/istio/"},{"categories":["Istio"],"content":"开启自动 mTLS 服务中的工作负载之间的通信是通过 Envoy 代理进行的。当一个工作负载使用 mTLS 向另一个工作负载发送请求时，Istio 会将流量重新路由到 sidecar 代理（Envoy） 然后，sidecar Envoy 开始与服务器端的 Envoy 进行 mTLS 握手。在握手过程中，调用者会进行安全命名检查，以验证服务器证书中的服务账户是否被授权运行目标服务。一旦 mTLS 连接建立，Istio 就会将请求从客户端的 Envoy 代理转发到服务器端的 Envoy 代理。在服务器端的授权后，sidecar 将流量转发到工作负载 可以创建 PeerAuthentication 资源，首先在每个命名空间中分别执行严格模式。然后，我们可以在根命名空间创建一个策略，在整个服务网格中执行该策略。或者指定 selector，仅应用于网格中的特定工作负载。它有三大模式： PERMISSIVE：同时接受未加密连接和双向加密连接 STRICT：只接受加密连接 DISABLE：关闭双向加密连接 apiVersion: security.istio.io/v1beta1 kind: PeerAuthentication metadata: name: testmtls namespace: myistio spec: selector: matchLabels: app: reviews mtls: mode: STRICT ","date":"2023-02-02","objectID":"/posts/istio/:11:0","tags":null,"title":"Istio 快速入门","uri":"/posts/istio/"},{"categories":["go"],"content":"数字证书是一个经证书授权中心数字签名的包含公开密钥拥有者信息以及公开密钥的文件 ","date":"2023-01-24","objectID":"/posts/go-ca-cert/:0:0","tags":null,"title":"使用 Go 生成自签 CA 证书","uri":"/posts/go-ca-cert/"},{"categories":["go"],"content":"使用 Go 自签发证书 go 的 x509 标准库下有个 Certificate 结构，这个结构就是证书解析后对应的实体，新证书需要先生成秘钥对，然后使用根证书的私钥进行签名，证书和私钥以及公钥这里使用的是pem编码方式 package main import ( \"crypto/rand\" \"crypto/rsa\" \"crypto/x509\" \"crypto/x509/pkix\" \"encoding/pem\" \"log\" \"math/big\" mathRand \"math/rand\" \"os\" \"time\" ) const ( CAFile = \"./test/certs/ca.crt\" // CA证书 CAKey = \"./test/certs/ca.key\" // CA私钥 ClientFile = \"./test/certs/lisi.pem\" // 客户端证书 ClientKey = \"./test/certs/lisi_key.pem\" // 客户端私钥 ) func main() { // 解析根证书 caFile, err := os.ReadFile(CAFile) if err != nil { log.Fatal(err) } caBlock, _ := pem.Decode(caFile) caCert, err := x509.ParseCertificate(caBlock.Bytes) // CA证书对象 if err != nil { log.Fatal(err) } // 解析私钥 keyFile, err := os.ReadFile(CAKey) if err != nil { log.Fatal(err) } keyBlock, _ := pem.Decode(keyFile) caPriKey, err := x509.ParsePKCS1PrivateKey(keyBlock.Bytes) // 私钥对象 if err != nil { log.Fatal(err) } // Go 提供了标准库 crypto/x509 给我们提供了 x509 签证的能力，我们可以先通过 x509.Certificate 构建证书签名请求 CSR 然后再进行签证 // 构建新的证书模板，里面的字段可以根据自己需求填写 certTemplate := \u0026x509.Certificate{ SerialNumber: big.NewInt(mathRand.Int63()), // 证书序列号 Subject: pkix.Name{ Country: []string{\"CN\"}, //Organization: []string{\"填的话这里可以用作用户组\"}, //OrganizationalUnit: []string{\"可填课不填\"}, Province: []string{\"beijing\"}, CommonName: \"lisi\", // CN Locality: []string{\"beijing\"}, }, NotBefore: time.Now(), // 证书有效期开始时间 NotAfter: time.Now().AddDate(1, 0, 0), // 证书有效期 BasicConstraintsValid: true, // 基本的有效性约束 IsCA: false, // 是否是根证书 ExtKeyUsage: []x509.ExtKeyUsage{x509.ExtKeyUsageClientAuth, x509.ExtKeyUsageServerAuth}, // 证书用途(客户端认证，数据加密) KeyUsage: x509.KeyUsageDigitalSignature | x509.KeyUsageDataEncipherment, EmailAddresses: []string{\"UserAccount@virtuallain.com\"}, } // 生成公私钥秘钥对 priKey, err := rsa.GenerateKey(rand.Reader, 2048) if err != nil { log.Fatal(err) } // 创建证书对象 clientCert, err := x509.CreateCertificate(rand.Reader, certTemplate, caCert, \u0026priKey.PublicKey, caPriKey) if err != nil { log.Fatal(err) } // 编码证书文件和私钥文件 clientCertPem := \u0026pem.Block{ Type: \"CERTIFICATE\", Bytes: clientCert, } clientCertFile, err := os.OpenFile(ClientFile, os.O_WRONLY|os.O_CREATE|os.O_TRUNC, 0600) if err != nil { log.Fatal(err) } err = pem.Encode(clientCertFile, clientCertPem) if err != nil { log.Fatal(err) } buf := x509.MarshalPKCS1PrivateKey(priKey) keyPem := \u0026pem.Block{ Type: \"PRIVATE KEY\", Bytes: buf, } clientKeyFile, _ := os.OpenFile(ClientKey, os.O_WRONLY|os.O_CREATE|os.O_TRUNC, 0600) err = pem.Encode(clientKeyFile, keyPem) if err != nil { log.Fatal(err) } } ","date":"2023-01-24","objectID":"/posts/go-ca-cert/:1:0","tags":null,"title":"使用 Go 生成自签 CA 证书","uri":"/posts/go-ca-cert/"},{"categories":["go"],"content":"使用证书请求k8s api 关联 roleBinding 后测试一下 curl --cert ./lisi.pem --key ./lisi_key.pem --cacert /etc/kubernetes/pki/ca.crt -s https://192.168.0.111:6443/api/v1/namespaces/default/pods ","date":"2023-01-24","objectID":"/posts/go-ca-cert/:2:0","tags":null,"title":"使用 Go 生成自签 CA 证书","uri":"/posts/go-ca-cert/"},{"categories":["k8s-go"],"content":"k8s 实现的“进入某个容器”的功能，底层本质是 Docker 容器通过 exec 进入容器的扩展。本质是新建了一个“与目标容器，共享 namespace 的”新的 shell 进程。所以该 shell 进程，看到的世界，就是容器内的世界了。 通过 client-go 提供的方法，实现通过网页进入 kubernetes 任意容器的终端操作 ","date":"2023-01-05","objectID":"/posts/k8s-pod-shell/:0:0","tags":null,"title":"k8s 远程进入容器 terminal","uri":"/posts/k8s-pod-shell/"},{"categories":["k8s-go"],"content":"remotecommand http://k8s.io/client-go/tools/remotecommand 是 kubernetes client-go 提供的 remotecommand 包，提供了方法与集群中的容器建立长连接，并设置容器的 stdin，stdout 等。 remotecommand 包提供基于 SPDY 协议的 Executor interface，进行和 pod 终端的流的传输。初始化一个 Executor 很简单，只需要调用 remotecommand 的 NewSPDYExecutor 并传入对应参数。 func main() { config, err := clientcmd.BuildConfigFromFlags(\"\", \"kubeconfig\") if err != nil { log.Fatal(err) } client, err := kubernetes.NewForConfig(config) if err != nil { log.Fatal(err) } option := \u0026coreV1.PodExecOptions{ Container: \"nginxtest\", // 容器名称 Command: []string{\"sh\", \"-c\", \"ls\"}, // 命令 Stdin: true, Stdout: true, Stderr: true, } req := client.CoreV1().RESTClient().Post().Resource(\"pods\"). Namespace(\"default\"). Name(\"myngx-79bdb4ccf8-nbln7\"). // pod名称 SubResource(\"exec\"). VersionedParams(option, scheme.ParameterCodec) // 这里初始化了一个 remote-cmd 的对象 exec, err := remotecommand.NewSPDYExecutor(config, \"POST\", req.URL()) if err != nil { log.Fatal(err) } // 这里开始，将输入输出，进行实时传递（Stream） err = exec.StreamWithContext(context.Background(), remotecommand.StreamOptions{ Stdin: os.Stdin, Stdout: os.Stdout, Stderr: os.Stderr, Tty: true, }) if err != nil { log.Fatal(err) } } 将 TTY 设置为 true，命令设置为 sh 进入容器交互式执行 option := \u0026coreV1.PodExecOptions{ Container: \"nginxtest\", Command: []string{\"sh\"}, Stdin: true, Stdout: true, Stderr: true, TTY: true, } ","date":"2023-01-05","objectID":"/posts/k8s-pod-shell/:1:0","tags":null,"title":"k8s 远程进入容器 terminal","uri":"/posts/k8s-pod-shell/"},{"categories":["k8s-go"],"content":"websocket Executor 的 StreamWithContext 方法，会建立一个流传输的连接，直到服务端和调用端一端关闭连接，才会停止传输。常用的做法是定义一个你想用的客户端，实现 Read(p []byte) (int, error) 和 Write(p []byte) (int, error) 方法即可，调用 Stream 方法时，只要将 StreamOptions 的 Stdin Stdout 都设置为该客户端，Executor 就会通过你定义的 write 和 read 方法来传输数据。 var Upgrader websocket.Upgrader func init() { Upgrader = websocket.Upgrader{ CheckOrigin: func(r *http.Request) bool { return true }, } } type WsShellClient struct { client *websocket.Conn } func NewWsShellClient(client *websocket.Conn) *WsShellClient { return \u0026WsShellClient{client: client} } // 实现 io.Writer func (this *WsShellClient) Write(p []byte) (n int, err error) { err = this.client.WriteMessage(websocket.TextMessage, p) if err != nil { return 0, err } return len(p), nil } // 实现 io.Reader func (this *WsShellClient) Read(p []byte) (n int, err error) { _, b, err := this.client.ReadMessage() if err != nil { return 0, err } return copy(p, string(b)+\"\\n\"), nil } func main() { r := gin.New() r.GET(\"/\", func(c *gin.Context) { wsClient, err := ws.Upgrader.Upgrade(c.Writer, c.Request, nil) if err != nil { log.Println(err) return } shellClient := ws.NewWsShellClient(wsClient) option := \u0026coreV1.PodExecOptions{ Container: \"nginxtest\", Command: []string{\"sh\"}, Stdin: true, Stdout: true, Stderr: true, TTY: true, } req := client.CoreV1().RESTClient().Post().Resource(\"pods\"). Namespace(\"default\"). Name(\"myngx-79bdb4ccf8-nbln7\"). SubResource(\"exec\"). VersionedParams(option, scheme.ParameterCodec) exec, err := remotecommand.NewSPDYExecutor(config, \"POST\", req.URL()) if err != nil { log.Println(err) } err = exec.StreamWithContext(c, remotecommand.StreamOptions{ Stdin: shellClient, Stdout: shellClient, Stderr: shellClient, Tty: true, }) if err != nil { log.Println(err) } }) r.Run(\":8080\") } 测试html示例 \u003c!DOCTYPE html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"UTF-8\"\u003e \u003ctitle\u003eTitle\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003cdiv\u003e \u003cdiv id=\"message\" style=\"width: 500px;height:300px;border:solid 1px gray;overflow:auto\"\u003e \u003c/div\u003e \u003cdiv\u003e \u003cinput type=\"type\" id=\"txtCmd\"/\u003e \u003cinput type=\"button\" id=\"cmdBtn\" value=\"发送\"/\u003e \u003cinput type=\"button\" onclick=\"document.getElementById('message').innerHTML=''\" value=\"清空\"/\u003e \u003c/div\u003e \u003c/div\u003e \u003cscript\u003e var ws = new WebSocket(\"ws://localhost:8080/\"); ws.onopen = function(){ console.log(\"open\"); } ws.onmessage = function(e){ let html=document.getElementById(\"message\").innerHTML; html+='\u003cp\u003e服务端消息:' + e.data + '\u003c/p\u003e' document.getElementById(\"message\").innerHTML=html } ws.onclose = function(e){ console.log(\"close\"); } ws.onerror = function(e){ console.log(e); } document.getElementById(\"cmdBtn\").onclick= ()=\u003e{ console.log(document.getElementById(\"txtCmd\").value) ws.send(document.getElementById(\"txtCmd\").value) } \u003c/script\u003e \u003c/body\u003e \u003c/html\u003e ","date":"2023-01-05","objectID":"/posts/k8s-pod-shell/:2:0","tags":null,"title":"k8s 远程进入容器 terminal","uri":"/posts/k8s-pod-shell/"},{"categories":["k8s-go"],"content":"xterm.js 前端页面使用 xterm.js 进行模拟terminal展示，只要 javascript 监听 Terminal 对象的对应事件及 websocket 连接的事件，进行对应的页面展示和消息推送就可以了。 ","date":"2023-01-05","objectID":"/posts/k8s-pod-shell/:3:0","tags":null,"title":"k8s 远程进入容器 terminal","uri":"/posts/k8s-pod-shell/"},{"categories":["k8s-go"],"content":"在 Kubernetes 中，有5个主要的组件，分别是 master 节点上的 kube-api-server、kube-controller-manager 和 kube-scheduler，node 节点上的 kubelet 和kube-proxy 。这其中 kube-apiserver 是对外和对内提供资源的声明式 API 的组件，其它4个组件都需要和它交互。为了保证消息的实时性，有两种方式： 客户端组件 (kubelet, scheduler, controller-manager 等) 轮询 apiserver apiserver 通知客户端 为了降低 kube-apiserver 的压力，有一个非常关键的机制就是 list-watch。list-watch 本质上也是 client 端监听 k8s 资源变化并作出相应处理的生产者消费者框架 list-watach 机制需要满足以下需求： 实时性 (即数据变化时，相关组件越快感知越好) 保证消息的顺序性 (即消息要按发生先后顺序送达目的组件。很难想象在Pod创建消息前收到该Pod删除消息时组件应该怎么处理) 保证消息不丢失或者有可靠的重新获取机制 (比如 kubelet 和 kube-apiserver 间网络闪断，需要保证网络恢复后kubelet可以收到网络闪断期间产生的消息) ","date":"2022-12-18","objectID":"/posts/k8s-list-watch/:0:0","tags":null,"title":"K8s list-watch 机制和 Informer 模块","uri":"/posts/k8s-list-watch/"},{"categories":["k8s-go"],"content":"list-watch 机制 list-watch 由两部分组成，分别是 list 和 watch。list 非常好理解，就是调用资源的 list API 罗列资源 ，基于 HTTP 短链接实现，watch 则是调用资源的 watch API 监听资源变更事件，基于 HTTP 长链接实现 etcd 存储集群的数据信息，apiserver 作为统一入口，任何对数据的操作都必须经过 apiserver。客户端通过 list-watch 监听 apiserver 中资源的 create, update 和 delete 事件，并针对事件类型调用相应的事件处理函数 ","date":"2022-12-18","objectID":"/posts/k8s-list-watch/:1:0","tags":null,"title":"K8s list-watch 机制和 Informer 模块","uri":"/posts/k8s-list-watch/"},{"categories":["k8s-go"],"content":"informer 机制 k8s 的 informer 模块封装 list-watch API，用户只需要指定资源，编写事件处理函数，AddFunc, UpdateFunc 和 DeleteFunc 等。如下图所示，informer 首先通过 list API 罗列资源，然后调用 watch API 监听资源的变更事件，并将结果放入到一个 FIFO 队列，队列的另一头有协程从中取出事件，并调用对应的注册函数处理事件。Informer 还维护了一个只读的 Map Store 缓存，主要为了提升查询的效率，降低 apiserver 的负载 ","date":"2022-12-18","objectID":"/posts/k8s-list-watch/:2:0","tags":null,"title":"K8s list-watch 机制和 Informer 模块","uri":"/posts/k8s-list-watch/"},{"categories":["k8s-go"],"content":"在 client-go 中的应用 client-go 使用 k8s.io/client-go/tools/cache 包里的 informer 对象进行 list-watch 机制的封装 最粗暴的解释： 初始化时，调 List API 获得全量 list，缓存起来(本地缓存)，这样就不需要每次请求都去请求 ApiServer 调用 Watch API 去 watch 资源，发生变更后会通过一定机制维护缓存 type DepHandler struct{} func (this *DepHandler) OnAdd(obj interface{}) {} func (this *DepHandler) OnUpdate(oldObj, newObj interface{}) { if dep, ok := newObj.(*v1.Deployment); ok { fmt.Println(dep.Name) } } func (this *DepHandler) OnDelete(obj interface{}) {} func main() { _, c := cache.NewInformer( // 监听 default 命名空间中 deployment 的变化 cache.NewListWatchFromClient(K8sClient.AppsV1().RESTClient(), \"deployments\", \"default\", fields.Everything()), \u0026v1.Deployment{}, 0, // 重新同步时间 \u0026DepHandler{}, // 实现类 ) c.Run(wait.NeverStop) select {} } ","date":"2022-12-18","objectID":"/posts/k8s-list-watch/:3:0","tags":null,"title":"K8s list-watch 机制和 Informer 模块","uri":"/posts/k8s-list-watch/"},{"categories":["k8s-go"],"content":"SharedInformerFactory sharedInformerFactory 用来构造各种 Informer 的工厂对象，它可以共享多个 informer 资源 informerFactory := informers.NewSharedInformerFactory(K8sClient, 0) // 构建一个 deployment informer depInformer := informerFactory.Apps().V1().Deployments() depInformer.Informer().AddEventHandler(\u0026DepHandler{}) informerFactory.Start(wait.NeverStop) select {} ","date":"2022-12-18","objectID":"/posts/k8s-list-watch/:3:1","tags":null,"title":"K8s list-watch 机制和 Informer 模块","uri":"/posts/k8s-list-watch/"},{"categories":["k8s-go"],"content":"示例 ","date":"2022-12-18","objectID":"/posts/k8s-list-watch/:4:0","tags":null,"title":"K8s list-watch 机制和 Informer 模块","uri":"/posts/k8s-list-watch/"},{"categories":["k8s-go"],"content":"监听 deployment // 全局对象，存储所有deployments var DepMapImpl *DeploymentMap func init() { DepMapImpl = \u0026DeploymentMap{Data: new(sync.Map)} } type DeploymentMap struct { Data *sync.Map // key:namespace value:[]*v1.Deployments } // 添加 func (this *DeploymentMap) Add(deployment *v1.Deployment) { if depList, ok := this.Data.Load(deployment.Namespace); ok { depList = append(depList.([]*v1.Deployment), deployment) this.Data.Store(deployment.Namespace, depList) } else { this.Data.Store(deployment.Namespace, []*v1.Deployment{deployment}) } } // 获取列表 func (this *DeploymentMap) ListByNs(namespace string) ([]*v1.Deployment, error) { if depList, ok := this.Data.Load(namespace); ok { return depList.([]*v1.Deployment), nil } return nil, fmt.Errorf(\"record not found\") } // 更新 func (this *DeploymentMap) Update(deployment *v1.Deployment) error { if depList, ok := this.Data.Load(deployment.Namespace); ok { depList := depList.([]*v1.Deployment) for i, dep := range depList { if dep.Name == deployment.Name { depList[i] = deployment break } } return nil } return fmt.Errorf(\"deployment [%s] not found\", deployment.Name) } // 删除 func (this *DeploymentMap) Delete(deployment *v1.Deployment) { if depList, ok := this.Data.Load(deployment.Namespace); ok { depList := depList.([]*v1.Deployment) for i, dep := range depList { if dep.Name == deployment.Name { newDepList := append(depList[:i], depList[i+1:]...) this.Data.Store(deployment.Namespace, newDepList) break } } } } // informer实现 type DepHandler struct{} func (this *DepHandler) OnAdd(obj interface{}) { DepMapImpl.Add(obj.(*v1.Deployment)) } func (this *DepHandler) OnUpdate(oldObj, newObj interface{}) { err := DepMapImpl.Update(newObj.(*v1.Deployment)) if err != nil { log.Println(err) } } func (this *DepHandler) OnDelete(obj interface{}) { DepMapImpl.Delete(obj.(*v1.Deployment)) } // 执行监听 func InitDeployments() { informerFactory := informers.NewSharedInformerFactory(K8sClient, 0) depInformer := informerFactory.Apps().V1().Deployments() depInformer.Informer().AddEventHandler(\u0026DepHandler{}) informerFactory.Start(wait.NeverStop) } ","date":"2022-12-18","objectID":"/posts/k8s-list-watch/:4:1","tags":null,"title":"K8s list-watch 机制和 Informer 模块","uri":"/posts/k8s-list-watch/"},{"categories":["k8s-go"],"content":"获取 deployment 的关联 pod 之前做过利用 Deployment 的 MatchLabels 去匹配 pod 的 labels 的方式。这次我们利用 ReplicaSet 的标签去匹配 Pod，这种方式可以区分当多个 Deployment 的 Pod 设置为相同标签的场景 当创建完 Deployment 后，k8s 会创建对应的 ReplicaSet，它会根据 template 里的内容进行 hash，然后自动设置一个标签 pod-template-hash，且与它管理的所有 Pod 标签相对应 Labels: app=xnginx pod-template-hash=767447889d 我们只需要通过 Deployment 获取它的 ReplicaSet，再拿 labels 去匹配 Pod 第一步：监听 Deployment、ReplicaSet 和 Pod，分别实现对应的 informer 方法，将数据缓存到本地 第二步：通过 Deployment 获取对应的 ReplicaSet，拿到 labels 关键代码： // 从本地缓存中取出所有的rs rsList, err := RSMapImpl.ListByNs(namespace) // 获取 labels labels, err := GetListWatchRsLabelByDeployment(deployment, rsList) // list-watch方式 根据deployment获取当前ReplicaSet的标签 func GetListWatchRsLabelByDeployment(deployment *v1.Deployment, rsList []*v1.ReplicaSet) (map[string]string, error) { for _, rs := range rsList { if IsCurrentRsByDeployment(rs, deployment) { selector, err := metaV1.LabelSelectorAsMap(rs.Spec.Selector) if err != nil { return nil, err } return selector, nil } } return nil, nil } // 判断rs是否对应当前deployment func IsCurrentRsByDeployment(set *v1.ReplicaSet, deployment *v1.Deployment) bool { if set.ObjectMeta.Annotations[\"deployment.kubernetes.io/revision\"] != deployment.ObjectMeta.Annotations[\"deployment.kubernetes.io/revision\"] { return false } for _, rf := range set.OwnerReferences { if rf.Kind == \"Deployment\" \u0026\u0026 rf.Name == deployment.Name { return true } } return false } 第三步：通过 labels 去匹配 pods 关键代码： // 根据标签获取Pod列表 func (this *PodMap) ListByLabels(ns string, labels map[string]string) ([]*v1.Pod, error) { ret := make([]*v1.Pod, 0) if podList, ok := this.Data.Load(ns); ok { podList := podList.([]*v1.Pod) for _, p := range podList { // 判断标签完全匹配 if reflect.DeepEqual(p.Labels, labels) { ret = append(ret, p) } } return ret, nil } return nil, fmt.Errorf(\"pods not found\") } ","date":"2022-12-18","objectID":"/posts/k8s-list-watch/:4:2","tags":null,"title":"K8s list-watch 机制和 Informer 模块","uri":"/posts/k8s-list-watch/"},{"categories":["k8s-go"],"content":"获取 Pod 状态和 Event Pod 状态信息包含： 阶段：Pod 的 status 字段是一个 PodStatus 对象，其中包含一个 phase 字段 取值 描述 Pending（悬决） Pod 已被 Kubernetes 系统接受，但有一个或者多个容器尚未创建亦未运行。此阶段包括等待 Pod 被调度的时间和通过网络下载镜像的时间。 Running（运行中） Pod 已经绑定到了某个节点，Pod 中所有的容器都已被创建。至少有一个容器仍在运行，或者正处于启动或重启状态。 Succeeded（成功） Pod 中的所有容器都已成功终止，并且不会再重启。 Failed（失败） Pod 中的所有容器都已终止，并且至少有一个容器是因为失败终止。也就是说，容器以非 0 状态退出或者被系统终止。 Unknown（未知） 因为某些原因无法取得 Pod 的状态。这种情况通常是因为与 Pod 所在主机通信失败。 状况：PodStatus 对象包含一个 PodConditions 数组 字段名称 描述 type Pod 状况的名称 status 表明该状况是否适用，可能的取值有 “True\"、\"False” 或 “Unknown” lastProbeTime 上次探测 Pod 状况时的时间戳 lastTransitionTime Pod 上次从一种状态转换到另一种状态时的时间戳 reason 机器可读的、驼峰编码（UpperCamelCase）的文字，表述上次状况变化的原因 message 人类可读的消息，给出上次状态转换的详细信息 PodScheduled：Pod 已经被调度到某节点 PodHasNetwork：Pod 沙箱被成功创建并且配置了网络（Alpha 特性，必须被显式启用） ContainersReady：Pod 中所有容器都已就绪 Initialized：所有的 Init 容器都已成功完成 Ready：Pod 可以为请求提供服务，并且应该被添加到对应服务的负载均衡池中 事件对象：为用户提供了洞察集群内发生的事情的能力。为了避免主节点磁盘空间被填满，将强制执行保留策略：事件在最后一次发生的一小时后将会被删除 关键代码： // EventMapImpl 全局对象，存储所有Event var EventMapImpl *EventMap func init() { EventMapImpl = \u0026EventMap{Data: new(sync.Map)} } type EventMap struct { Data *sync.Map // key:namespace_kind_name value: *v1.Event } func (this *EventMap) GetKey(event *v1.Event) string { key := fmt.Sprintf(\"%s_%s_%s\", event.Namespace, event.InvolvedObject.Kind, event.InvolvedObject.Name) return key } // Add 添加 func (this *EventMap) Add(event *v1.Event) { EventMapImpl.Data.Store(this.GetKey(event), event) } // Delete 删除 func (this *EventMap) Delete(event *v1.Event) { EventMapImpl.Data.Delete(this.GetKey(event)) } // 获取最新一条event message func (this *EventMap) GetMessage(ns string, kind string, name string) string { key := fmt.Sprintf(\"%s_%s_%s\", ns, kind, name) if v, ok := this.Data.Load(key); ok { return v.(*v1.Event).Message } return \"\" } // EventHandler informer实现 type EventHandler struct{} func (this *EventHandler) OnAdd(obj interface{}) { EventMapImpl.Add(obj.(*v1.Event)) } func (this *EventHandler) OnUpdate(oldObj, newObj interface{}) { EventMapImpl.Add(newObj.(*v1.Event)) } func (this *EventHandler) OnDelete(obj interface{}) { EventMapImpl.Delete(obj.(*v1.Event)) } // 评估Pod是否就绪 func GetPodIsReady(pod *coreV1.Pod) bool { for _, condition := range pod.Status.Conditions { if condition.Type == \"ContainersReady\" \u0026\u0026 condition.Status != \"True\" { return false } } for _, rg := range pod.Spec.ReadinessGates { for _, condition := range pod.Status.Conditions { if condition.Type == rg.ConditionType \u0026\u0026 condition.Status != \"True\" { return false } } } return true } // 获取pods DTO 把原生的 pod 对象转换为自己的实体对象 func GetPodsByLabels(ns string, labels []map[string]string) (pods []*model.PodModel) { podList, err := PodMapImpl.ListByLabels(ns, labels) lib.CheckError(err) pods = make([]*model.PodModel, len(podList)) for i, pod := range podList { pods[i] = \u0026model.PodModel{ Name: pod.Name, NodeName: pod.Spec.NodeName, Images: GetPodImages(pod.Spec.Containers), Phase: string(pod.Status.Phase), IsReady: GetPodIsReady(pod), Message: EventMapImpl.GetMessage(pod.Namespace, \"Pod\", pod.Name), CreatedAt: pod.CreationTimestamp.Format(\"2006-01-02 15:04:05\"), } } return } ","date":"2022-12-18","objectID":"/posts/k8s-list-watch/:4:3","tags":null,"title":"K8s list-watch 机制和 Informer 模块","uri":"/posts/k8s-list-watch/"},{"categories":["k8s-go"],"content":"client-go 是负责与 Kubernetes APIServer 服务进行交互的客户端库，利用 Client-Go 与 Kubernetes APIServer 进行的交互访问，来对 Kubernetes 中的各类资源对象进行管理操作，包括内置的资源对象及 CRD ","date":"2022-12-18","objectID":"/posts/k8s-go/:0:0","tags":null,"title":"K8s client-go","uri":"/posts/k8s-go/"},{"categories":["k8s-go"],"content":"client-go 客户端 Client-Go 共提供了 4 种与 Kubernetes APIServer 交互的客户端 RESTClient：最基础的客户端，主要是对 HTTP 请求进行了封装，支持 Json 和 Protobuf 格式的数据。 DiscoveryClient：发现客户端，负责发现 APIServer 支持的资源组、资源版本和资源信息的。 ClientSet：负责操作 Kubernetes 内置的资源对象，例如：Pod、Service等。 DynamicClient：动态客户端，可以对任意的 Kubernetes 资源对象进行通用操作，包括 CRD。 ","date":"2022-12-18","objectID":"/posts/k8s-go/:1:0","tags":null,"title":"K8s client-go","uri":"/posts/k8s-go/"},{"categories":["k8s-go"],"content":"基本使用 参考 API文档 的 group 和 apiVersion 等信息 ","date":"2022-12-18","objectID":"/posts/k8s-go/:2:0","tags":null,"title":"K8s client-go","uri":"/posts/k8s-go/"},{"categories":["k8s-go"],"content":"创建 admin ServiceAccount kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: admin annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\" roleRef: kind: ClusterRole name: cluster-admin apiGroup: rbac.authorization.k8s.io subjects: - kind: ServiceAccount name: admin namespace: kube-system --- apiVersion: v1 kind: ServiceAccount metadata: name: admin namespace: kube-system labels: kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcile 查看 token $ kubectl describe sa admin -n kube-system Name: admin Namespace: kube-system Tokens: admin-token-nzxlb $ kubectl describe secret admin-token-nzxlb -n kube-system ","date":"2022-12-18","objectID":"/posts/k8s-go/:2:1","tags":null,"title":"K8s client-go","uri":"/posts/k8s-go/"},{"categories":["k8s-go"],"content":"建立连接 先使用反代的方式，在 master 节点执行 $ kubectl proxy --address='0.0.0.0' --accept-hosts='^*$' --port=8009 安装客户端库 (版本要对应 Github)，连接 API Server var K8sClient *kubernetes.Clientset func init() { config := \u0026rest.Config{ Host: \"ip:8009\", BearerToken: \"\", } client, err := kubernetes.NewForConfig(config) if err != nil { log.Fatalln(err) } K8sClient = client } ","date":"2022-12-18","objectID":"/posts/k8s-go/:2:2","tags":null,"title":"K8s client-go","uri":"/posts/k8s-go/"},{"categories":["k8s-go"],"content":"示例：获取资源列表 ctx := context.Background() // 查询 kube-system 命名空间下的 service svs, _ := K8sClient.CoreV1().Services(\"kube-system\").List(ctx, v1.ListOptions{}) // 查询 kube-system 命名空间下的 deployment deps, _ := K8sClient.AppsV1().Deployments(\"kube-system\").List(ctx, v1.ListOptions{}) ","date":"2022-12-18","objectID":"/posts/k8s-go/:2:3","tags":null,"title":"K8s client-go","uri":"/posts/k8s-go/"},{"categories":["k8s-go"],"content":"示例：获取资源详情 ctx := context.Background() // 获取名称为 ngx 的 deployment 资源 dep, _ := K8sClient.AppsV1().Deployments(namespace).Get(ctx, \"ngx\", metav1.GetOptions{}) dep.Name // 名称 dep.Namespace // 命名空间 dep.Status.Replicas // 副本数量 dep.CreationTimestamp // 创建时间 dep.Spec.Template.Spec.Containers[0].Image // 第一个镜像名称 ","date":"2022-12-18","objectID":"/posts/k8s-go/:2:4","tags":null,"title":"K8s client-go","uri":"/posts/k8s-go/"},{"categories":["k8s-go"],"content":"示例：获取 deployment 的关联 pod 最简单的方式，利用 Deployment 的 MatchLabels 去匹配 pod 的 labels type PodModel struct { Name string // pod名称 NodeName string // 节点 Images string // 镜像名称 CreatedAt string // 创建时间 } // 拼接labels字符串 func GetLabels(labels map[string]string) string { var labelStr strings.Builder for k, v := range labels { if labelStr.Len() != 0 { labelStr.WriteString(\",\") } labelStr.WriteString(fmt.Sprintf(\"%s=%s\", k, v)) } return labelStr.String() } // 根据deployment获取关联的pods集合 func GetPodsByDep(namespace string, dep *v1.Deployment) (pods []*PodModel) { ctx := context.Background() // 通过LabelSelector去匹配对应的pods listOpt := metav1.ListOptions{ LabelSelector: GetLabels(dep.Spec.Selector.MatchLabels)， } podList, _ := K8sClient.CoreV1().Pods(namespace).List(ctx, listOpt) pods = make([]*PodModel, len(podList.Items)) for i, pod := range podList.Items { pods[i] = \u0026PodModel{ Name: pod.Name, NodeName: pod.Spec.NodeName, Images: GetPodImages(pod.Spec.Containers), CreatedAt: pod.CreationTimestamp.Format(\"2006-01-02 15:04:05\"), } } return } dep, _ := K8sClient.AppsV1().Deployments(namespace).Get(context.Background(), \"ngx\", metav1.GetOptions{}) Pods = GetPodsByDep(\"default\", dep) ","date":"2022-12-18","objectID":"/posts/k8s-go/:2:5","tags":null,"title":"K8s client-go","uri":"/posts/k8s-go/"},{"categories":["k8s-go"],"content":"示例：修改 deployment 副本数量 // 获取 deployment 副本数量 scale, _ := K8sClient.AppsV1().Deployments(\"default\").GetScale(ctx, \"ngx\", v1.GetOptions{}) // 修改副本数量 scale.Spec.Replicas++ K8sClient.AppsV1().Deployments(\"default\").UpdateScale(ctx, \"ngx\", scale, v1.UpdateOptions{}) ","date":"2022-12-18","objectID":"/posts/k8s-go/:2:6","tags":null,"title":"K8s client-go","uri":"/posts/k8s-go/"},{"categories":["k8s-go"],"content":"示例：创建资源 根据 yaml 创建一个 nginx deployment apiVersion: apps/v1 kind: Deployment metadata: name: myngx namespace: default spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: nginxtest image: nginx:1.18-alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80 ngxDep := \u0026appV1.Deployment{} // 读取yaml内容 b, _ := os.ReadFile(\"nginx.yaml\") ngxJson, _ := yaml.ToJSON(b) json.Unmarshal(ngxJson, ngxDep) dep, _ := K8sClient.AppsV1().Deployments(\"default\").Create(context.Background(), ngxDep, v1.CreateOptions{}) ","date":"2022-12-18","objectID":"/posts/k8s-go/:2:7","tags":null,"title":"K8s client-go","uri":"/posts/k8s-go/"},{"categories":["kubernetes"],"content":"Kube-scheduler 是 Kubernetes 集群默认的调度器，并且是控制面中一个核心组件。scheduler 通过 kubernetes 的监测（Watch）机制来发现集群中新创建且尚未被调度到 Node 上的 Pod。 scheduler 会将发现的每一个未调度的 Pod 调度到一个合适的 Node 上来运行。 scheduler会依据下文的调度原则来做出调度选择。 对于新创建的 pod 或其他未调度的 pod来讲，kube-scheduler 选择一个最佳节点供它们运行。但是，Pod 中的每个容器对资源的要求都不同，每个 Pod 也有不同的要求。因此，需要根据具体的调度要求对现有节点进行过滤。 在Kubernetes集群中，满足 Pod 调度要求的节点称为可行节点 （feasible nodes FN） 。如果没有合适的节点，则 pod 将保持未调度状态，直到调度程序能够放置它。也就是说，当我们创建 Pod 时，如果长期处于 Pending 状态，这个时候应该看你的集群调度器是否因为某些问题没有合适的节点了 调度器为 Pod 找到 FN 后，然后运行一组函数对 FN 进行评分，并在 FN 中找到得分最高的节点来运行 Pod。 调度策略在决策时需要考虑的因素包括个人和集体资源需求、硬件/软件/策略约束 （constraints）、亲和性 (affinity) 和反亲和性（ anti-affinity ）规范、数据局部性、工作负载间干扰等。 基本调度流程： 发布 Pod ControllerManager 会把 Pod 加入待调度队列 kube-scheduler 决定调度到哪个 node，然后写入 etcd 被选中节点中的 kubelet 开始工作（pull image、启动 Pod） ","date":"2022-12-13","objectID":"/posts/kube-schedule/:0:0","tags":null,"title":"调度器 kube-schedule","uri":"/posts/kube-schedule/"},{"categories":["kubernetes"],"content":"如何为 Pod 选择节点？ kube-scheduler 给一个 Pod 做调度选择时包含两个步骤： 过滤 (Filtering) 打分 (Scoring) 过滤也被称为预选 （Predicates），该步骤会找到可调度的节点集，然后通过是否满足特定资源的请求，例如通过 PodFitsResources 过滤器检查候选节点是否有足够的资源来满足 Pod 资源的请求。这个步骤完成后会得到一个包含合适的节点的列表（通常为多个），如果列表为空，则Pod不可调度。 打分也被称为优选（Priorities），在该步骤中，会对上一个步骤的输出进行打分，Scheduer 通过打分的规则为每个通过 Filtering 步骤的节点计算出一个分数。 完成上述两个步骤之后，kube-scheduler 会将Pod分配给分数最高的 Node，如果存在多个相同分数的节点，会随机选择一个。 ","date":"2022-12-13","objectID":"/posts/kube-schedule/:1:0","tags":null,"title":"调度器 kube-schedule","uri":"/posts/kube-schedule/"},{"categories":["kubernetes"],"content":"将 Pod 指派给节点 你可以约束一个 Pod 以便限制其只能在特定的节点上运行，或优先在特定的节点上运行。有几种方法可以实现这点，推荐的方法都是用标签选择算符来进行选择。 通常这样的约束不是必须的，因为调度器将自动进行合理的放置，但在某些情况下，你可能需要进一步控制 Pod 被部署到哪个节点。 给节点 lain1 添加一个标签 disktype=ssd $ kubectl label nodes lain1 disktype=ssd # 删除标签 $ kubectl label nodes lain1 disktype- ","date":"2022-12-13","objectID":"/posts/kube-schedule/:2:0","tags":null,"title":"调度器 kube-schedule","uri":"/posts/kube-schedule/"},{"categories":["kubernetes"],"content":"nodeSelector 设置你希望目标节点所具有的节点标签。 apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent nodeSelector: disktype: ssd # 该Pod将被调度到有disktype=ssd标签的节点 ","date":"2022-12-13","objectID":"/posts/kube-schedule/:2:1","tags":null,"title":"调度器 kube-schedule","uri":"/posts/kube-schedule/"},{"categories":["kubernetes"],"content":"节点亲和性 nodeAffinity 节点亲和性有两种： requiredDuringSchedulingIgnoredDuringExecution：调度器只有在规则被满足的时候才能执行调度 preferredDuringSchedulingIgnoredDuringExecution：调度器会尝试寻找满足对应规则的节点。如果找不到匹配的节点，调度器仍然会调度该 Pod 强制的节点亲和性调度 下面的 pod 只会调度到具有 disktype=ssd 标签的节点上 apiVersion: v1 kind: Pod metadata: name: nginx spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: disktype operator: In values: - ssd containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent 首选的节点亲和性调度 apiVersion: v1 kind: Pod metadata: name: nginx spec: affinity: nodeAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 # 权重 preference: matchExpressions: - key: disktype operator: In values: - ssd containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent ","date":"2022-12-13","objectID":"/posts/kube-schedule/:2:2","tags":null,"title":"调度器 kube-schedule","uri":"/posts/kube-schedule/"},{"categories":["kubernetes"],"content":"节点污点和容忍度 节点亲和性使 Pod 被吸引到一类特定的节点，污点 (Taint) 则相反，它使节点能够排斥一类特定的 Pod。污点有三种类型： NoSchedule：不会将 Pod 调度到该节点 PreferNoSchedule：尽量避免将 Pod 调度到该节点上 NoExecute：任何不能忍受这个污点的 Pod 都会马上被驱逐 一些内置的污点： node.kubernetes.io/not-ready：节点未准备好。这相当于节点状况 Ready 的值为 False node.kubernetes.io/unreachable：节点控制器访问不到节点. 这相当于节点状况 Ready 的值为 Unknown node.kubernetes.io/memory-pressure：节点存在内存压力 node.kubernetes.io/disk-pressure：节点存在磁盘压力 node.kubernetes.io/pid-pressure: 节点的 PID 压力 node.kubernetes.io/network-unavailable：节点网络不可用 node.kubernetes.io/unschedulable: 节点不可调度 node.cloudprovider.kubernetes.io/uninitialized：如果 kubelet 启动时指定了一个“外部”云平台驱动， 它将给当前节点添加一个污点将其标志为不可用。在 cloud-controller-manager 的一个控制器初始化这个节点后，kubelet 将删除这个污点 容忍度 (Toleration) 是应用于 Pod 上的。容忍度允许调度器调度带有对应污点的 Pod。 容忍度允许调度但并不保证调度：作为其功能的一部分， 调度器也会评估其他参数。 # 查看节点的污点 $ kubectl describe node lain1 | grep Taints # 给节点打一个污点 $ kubectl taint nodes lain1 key1=value1:NoSchedule # 删除污点 $ kubectl taint node lain1 key1:NoSchedule- 使用容忍 apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent tolerations: - key: \"key1\" # 对应污点key operator: \"Equal\" value: \"value1\" effect: \"NoSchedule\" ","date":"2022-12-13","objectID":"/posts/kube-schedule/:2:3","tags":null,"title":"调度器 kube-schedule","uri":"/posts/kube-schedule/"},{"categories":["kubernetes"],"content":"Pod 亲和性 Pod 间亲和性与反亲和性使你可以基于已经在节点上运行的 Pod 的标签来约束 Pod 可以调度到的节点，而不是基于节点上的标签。 Pod 亲和性和反亲和性都需要相当的计算量，因此会在大规模集群中显著降低调度速度。 不建议在包含数百个节点的集群中使用这类设置 与节点亲和性类似，Pod 的亲和性与反亲和性也有两种类型： requiredDuringSchedulingIgnoredDuringExecution preferredDuringSchedulingIgnoredDuringExecution 实例资源清单 apiVersion: apps/v1 kind: Deployment metadata: name: ngx1 spec: selector: matchLabels: app: ngx1 replicas: 1 template: metadata: labels: app: ngx1 spec: nodeName: lain1 containers: - name: ngx1 image: nginx:1.18-alpine imagePullPolicy: IfNotPresent 下面这个 Pod 必须调度到具有 disktype 标签的节点上，并且集群中至少有一个位于该可用区的节点上运行着带有 app=ngx1 标签的 Pod apiVersion: apps/v1 kind: Deployment metadata: name: ngx2 spec: selector: matchLabels: app: ngx2 replicas: 1 template: metadata: labels: app: ngx2 spec: affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - ngx1 topologyKey: disktype containers: - name: ngx2 image: nginx:1.18-alpine imagePullPolicy: IfNotPresent ","date":"2022-12-13","objectID":"/posts/kube-schedule/:2:4","tags":null,"title":"调度器 kube-schedule","uri":"/posts/kube-schedule/"},{"categories":["kubernetes"],"content":"Horizontal Pod Autoscaling（Pod 水平自动伸缩），简称HPA。它可以基于 CPU 利用率或其他指标自动扩缩 ReplicationController、Deployment 和 ReplicaSet 中的 Pod 数量，它不适用于无法扩缩的对象，比如 DaemonSet。除了 CPU 利用率，也可以基于其他应程序提供的自定义度量指标来执行自动扩缩 文档：Pod 水平自动扩缩 | Kubernetes 我们可以简单的通过 kubectl autoscale 命令来创建一个 HPA 资源对象，HPA Controller 默认 30s 轮询一次（可通过 kube-controller-manager 的 --horizontal-pod-autoscaler-sync-period 参数进行设置)，查询指定的资源中的 Pod 资源使用率，并且与创建时设定的值和指标做对比，从而实现自动伸缩的功能。 ","date":"2022-12-12","objectID":"/posts/hpa/:0:0","tags":null,"title":"Pod 水平自动扩缩 — HPA","uri":"/posts/hpa/"},{"categories":["kubernetes"],"content":"Metrics Server Metrics Server 可以通过标准的 Kubernetes Summary API 把监控数据暴露出来，有了 Metrics Server 之后，就可以采集节点和 Pod 的内存、磁盘、CPU 和网络的使用率等。Metrics API URI 为 /apis/metrics.k8s.io/ ","date":"2022-12-12","objectID":"/posts/hpa/:1:0","tags":null,"title":"Pod 水平自动扩缩 — HPA","uri":"/posts/hpa/"},{"categories":["kubernetes"],"content":"安装 可以通过官方仓库的资源清单安装： Github 部署之前，需要修改 k8s.gcr.io/metrics-server/metrics-server 镜像的地址 # image: k8s.gcr.io/metrics-server/metrics-server:v0.4.1 image: bitnami/metrics-server:0.4.1 等待部署完成后，可以查看 pod 日志是否正常 $ kubectl get pods -n kube-system -l k8s-app=metrics-server NAME READY STATUS RESTARTS AGE metrics-server-7d8467779f-vgtzb 1/1 Running 0 18m $ kubectl logs -f metrics-server-7d8467779f-vgtzb -n kube-system ","date":"2022-12-12","objectID":"/posts/hpa/:1:1","tags":null,"title":"Pod 水平自动扩缩 — HPA","uri":"/posts/hpa/"},{"categories":["kubernetes"],"content":"查看 $ kubectl top nodes NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% lain1 149m 3% 3526Mi 45% lain2 208m 10% 2786Mi 75% $ kubectl top pod etcd-lain1 -n kube-system NAME CPU(cores) MEMORY(bytes) etcd-lain1 20m 249Mi ","date":"2022-12-12","objectID":"/posts/hpa/:1:2","tags":null,"title":"Pod 水平自动扩缩 — HPA","uri":"/posts/hpa/"},{"categories":["kubernetes"],"content":"使用 HPA 的 API 有三个版本，在当前稳定版本 autoscaling/v1 中只支持基于 CPU 指标的缩放。在 Beta 版本 autoscaling/v2beta2，引入了基于内存和自定义指标的缩放。 $ kubectl api-versions | grep autoscal autoscaling/v1 # 只支持通过cpu伸缩 autoscaling/v2beta1 # 支持通过cpu、内存和自定义数据来进行伸缩 autoscaling/v2beta2 我们部署一个测试 api，执行一些 CUP 密集型计算，然后利用 HAP 来进行自动伸缩容 test := map[string]string{ \"str\": \"requests来设置各容器需要的最小资源\", } r := gin.New() r.GET(\"/\", func(context *gin.Context) { ret := 0 for i := 0; i \u003c= 1000000; i++ { t := map[string]string{} b, _ := json.Marshal(test) _ = json.Unmarshal(b, t) ret++ } context.JSON(200, gin.H{\"message\": ret}) }) r.Run(\":8080\") 资源清单如下 apiVersion: apps/v1 kind: Deployment metadata: name: web1 spec: selector: matchLabels: app: myweb replicas: 1 template: metadata: labels: app: myweb spec: nodeName: lain1 containers: - name: web1test image: alpine:3.12 imagePullPolicy: IfNotPresent command: [\"/app/stress\"] volumeMounts: - name: app mountPath: /app resources: requests: cpu: \"200m\" memory: \"256Mi\" limits: cpu: \"400m\" # 1物理核=1000个微核(millicores) 1000m=1CPU memory: \"512Mi\" ports: - containerPort: 8080 volumes: - name: app hostPath: path: /home/txl/goapi --- apiVersion: v1 kind: Service metadata: name: web1 spec: type: ClusterIP ports: - port: 80 targetPort: 8080 selector: app: myweb requests 节点用来设置各容器需要的最小资源 limits 节点用于限制运行时容器占用的资源 ","date":"2022-12-12","objectID":"/posts/hpa/:2:0","tags":null,"title":"Pod 水平自动扩缩 — HPA","uri":"/posts/hpa/"},{"categories":["kubernetes"],"content":"创建 现在创建一个 HPA 资源对象，可以使用命令创建 $ kubectl autoscale deployment web1 --min=1 --max=5 --cpu-percent=20 $ kubectl get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE web1 Deployment/web1 0%/20% 1 5 1 12m 此命令创建了一个关联资源 web1 的 HPA，最小的 Pod 副本数为1，最大为5。HPA 会根据设定的 cpu 使用率（20%）动态的增加或者减少 Pod 数量。 也可以使用 yaml 来创建 apiVersion: autoscaling/v2beta2 kind: HorizontalPodAutoscaler metadata: name: web1hpa namespace: default spec: minReplicas: 1 maxReplicas: 5 scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: web1 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 50 # 使用率 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 50 你还可以指定资源度量指标使用绝对数值，而不是百分比，你需要将 target.type 从 Utilization 替换成 AverageValue，同时设置 target.averageValue 而非 target.averageUtilization 的值 metrics: - type: Resource resource: name: cpu target: type: AverageValue averageValue: 230m # 使用量 - type: Resource resource: name: memory target: type: AverageValue averageValue: 400m ","date":"2022-12-12","objectID":"/posts/hpa/:2:1","tags":null,"title":"Pod 水平自动扩缩 — HPA","uri":"/posts/hpa/"},{"categories":["kubernetes"],"content":"压测 $ sudo yum -y install httpd-tools $ ab -n 10000 -c 10 http://web1/ 可以看到，HPA 已经开始工作，副本数量已经从原来的1变成了4个 $ kubectl get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE web1 Deployment/web1 192%/20% 1 5 4 107s 查看 HPA 资源工作过程 $ kubectl describe hpa web1 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulRescale 20m horizontal-pod-autoscaler New size: 4; reason: cpu resource utilization (percentage of request) above target Normal SuccessfulRescale 20m horizontal-pod-autoscaler New size: 5; reason: cpu resource utilization (percentage of request) above target Normal SuccessfulRescale 14m horizontal-pod-autoscaler New size: 1; reason: All metrics below target ","date":"2022-12-12","objectID":"/posts/hpa/:2:2","tags":null,"title":"Pod 水平自动扩缩 — HPA","uri":"/posts/hpa/"},{"categories":["kubernetes"],"content":"可能出现的错误 ","date":"2022-12-12","objectID":"/posts/hpa/:3:0","tags":null,"title":"Pod 水平自动扩缩 — HPA","uri":"/posts/hpa/"},{"categories":["kubernetes"],"content":"Pod 启动异常 x509: cannot validate certificate for 192.168.0.111 because it doesn’t contain any ip sans node=“lain1” 因为 Kubelet 证书需要由群集证书颁发机构签名 ，或者给 Metrics Server 增加配置参数 –Kubelet-insecure-tls 来禁用证书验证 解决这个问题的方法是使用 APIServer 签署 Kubelet 证书。 首先编辑 kube-system namespace 中的 kubelet-config ConfigMap，在 kind: KubeletConfiguration 下方增加内容 serverTLSBootstrap: true 然后分别为每个节点上修改 kubelet-config configmap $ sudo vi /var/lib/kubelet/config.yaml # 在 kind: KubeletConfiguration 下方增加内容 serverTLSBootstrap: true 然后重启 kubelet $ systemctl restart kubelet Kubelet 都会生成一个 CSR 并将其提交给 APIServer，您需要为集群上的每个 Kubelet 批准 CSR $ kubectl get csr NAME AGE SIGNERNAME REQUESTOR CONDITION csr-clwz7 44m kubernetes.io/kubelet-serving system:node:lain1 Approved,Issued csr-w6kpf 34m kubernetes.io/kubelet-serving system:node:lain2 Approved,Issued $ kubectl certificate approve csr-clwz7 csr-w6kpf 默认情况下，这些服务证书将在一年后过期。因此，一年后，Kubelet 将生成一个新的 CSR，您需要批准它 参考：https://particule.io/en/blog/kubeadm-metrics-server ","date":"2022-12-12","objectID":"/posts/hpa/:3:1","tags":null,"title":"Pod 水平自动扩缩 — HPA","uri":"/posts/hpa/"},{"categories":["kubernetes"],"content":"为了能够屏蔽底层存储实现的细节，方便用户使用，k8s 引入 PV 和 PVC 两种资源对象。Persistent Volume 提供存储资源（并实现），Persistent Volume Claim 描述需要的存储标准，然后从现有 PV 中匹配或者动态建立新的资源，最后将两者进行绑定。 ","date":"2022-12-11","objectID":"/posts/pv_pvc/:0:0","tags":null,"title":"PV 和 PVC","uri":"/posts/pv_pvc/"},{"categories":["kubernetes"],"content":"持久卷（Persistent Volume） PV 是持久化卷的意思，是对底层的共享存储的一种抽象。一般情况下 PV 由 k8s 管理员进行创建和配置，它与底层具体的共享存储技术有关，并通过插件（如：local、NFS）等具体的底层技术来实现完成与共享存储的对接。 ","date":"2022-12-11","objectID":"/posts/pv_pvc/:1:0","tags":null,"title":"PV 和 PVC","uri":"/posts/pv_pvc/"},{"categories":["kubernetes"],"content":"持久卷的类型 cephfs - CephFS volume csi - 容器存储接口 (CSI) fc - Fibre Channel (FC) 存储 hostPath - HostPath 卷 （仅供单节点测试使用；不适用于多节点集群；请尝试使用 local 卷作为替代） iscsi - iSCSI (SCSI over IP) 存储 local - 节点上挂载的本地存储设备 nfs - 网络文件系统 (NFS) 存储 rbd - Rados 块设备 (RBD) 卷 ","date":"2022-12-11","objectID":"/posts/pv_pvc/:1:1","tags":null,"title":"PV 和 PVC","uri":"/posts/pv_pvc/"},{"categories":["kubernetes"],"content":"创建 使用 local 卷的资源清单 apiVersion: v1 kind: PersistentVolume metadata: name: local-pv spec: capacity: storage: 1Gi volumeMode: Filesystem accessModes: - ReadWriteOnce storageClassName: \"\" # 存储类别 persistentVolumeReclaimPolicy: Retain local: path: /home/txl/data nodeAffinity: required: # 指定必须满足的硬性节点约束 nodeSelectorTerms: # 节点选择器条件的列表 - matchExpressions: # 基于节点标签所设置的节点选择器要求的列表 - key: pv # 适用的标签主键 operator: In # 代表主键与值集之间的关系 values: - local ","date":"2022-12-11","objectID":"/posts/pv_pvc/:1:2","tags":null,"title":"PV 和 PVC","uri":"/posts/pv_pvc/"},{"categories":["kubernetes"],"content":"关键配置参数 储存能力 capacity：目前只支持存储空间的设置（storage=1Gi） 卷模式 volumeMode：设置为 Filesystem 的卷会被 Pod 挂载（Mount）到某个目录。 如果卷的存储来自某块设备而该设备目前为空，Kuberneretes 会在第一次挂载卷之前在设备上创建文件系统 访问模式 accessModes：用于描述用户应用对存储资源的访问权限，访问权限包括下面几种方式： ReadWriteOnce（RWO）：读写权限，但是只能被单个节点挂载 ReadOnlyMany（ROX）：只读权限，可以被多个节点挂载 ReadWriteMany（RWX）：读写权限，可以被多个节点挂载 存储类别 storageClassName：PV 可以通过 storageClassName 参数指定一个存储类别： 具有特定类别的 PV 只能与请求了该类别的 PVC 进行绑定 未设定类别的 PV 只能与不请求任何类别的 PVC 进行绑定 回收策略 persistentVolumeReclaimPolicy：当 PV 不再被使用了之后，对其的处理方式。目前支持三种策略： Retain（保留）：保留数据，需要管理员手动清理数据 Recycle（回收）：清除PV中的数据，效果相当于执行 rm -rf /thevolume/* Delete（删除）：与PV相连的后端存储完成volume的删除操作，当然这常见于云服务商的存储服务 节点亲和性 NodeAffinity：定义一些约束，进而限制从哪些节点上可以访问此卷。matchExpressions 的 operator包括： In：label 的值在某个列表中 NotIn：label 的值不在某个列表中 Exists：某个 label 存在 DoesNotExist：某个 label 不存在 Gt：label 的值大于某个值（字符串比较） Lt：label 的值小于某个值（字符串比较） 状态 status：一个 PV 的生命周期中，可能会处于4种不同的阶段 Available（可用）：表示可用状态，还未被任何PVC绑定 Bound（已绑定）：表示PV已经被PVC绑定 Released（已释放）：表示PVC被删除，但是资源还未被集群重新声明 Failed（失败）：表示该PV的自动回收失败 ","date":"2022-12-11","objectID":"/posts/pv_pvc/:1:3","tags":null,"title":"PV 和 PVC","uri":"/posts/pv_pvc/"},{"categories":["kubernetes"],"content":"标签 查看标签 $ kubectl get node --show-labels=true 给 node lain1 打一个标签 pv=local $ kubectl label nodes lain1 pv=local 删除标签 $ kubectl label nodes lain1 pv- ","date":"2022-12-11","objectID":"/posts/pv_pvc/:1:4","tags":null,"title":"PV 和 PVC","uri":"/posts/pv_pvc/"},{"categories":["kubernetes"],"content":"持久卷声明（PersistentVolumeClaim） PVC 是资源的申请，用来声明对存储空间、访问模式、存储类别需求信息 ","date":"2022-12-11","objectID":"/posts/pv_pvc/:2:0","tags":null,"title":"PV 和 PVC","uri":"/posts/pv_pvc/"},{"categories":["kubernetes"],"content":"创建 绑定：spec 关键字段要匹配，storageClassName 字段必须一致 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: ngx-pvc spec: accessModes: - ReadWriteOnce storageClassName: \"\" resources: requests: storage: 1Gi ","date":"2022-12-11","objectID":"/posts/pv_pvc/:2:1","tags":null,"title":"PV 和 PVC","uri":"/posts/pv_pvc/"},{"categories":["kubernetes"],"content":"存储类（StorageClass） Kubernetes 提供了一套可以自动创建 PV 的机制，即：Dynamic Provisioning。而这个机制的核心在于StorageClass 这个 API 对象。StorageClass 对象会定义下面两部分内容: PV 的属性，如存储类型，Volume 的大小等。 创建这种 PV 需要用到的存储插件，即存储制备器。 有了这两个信息之后，Kubernetes 就能够根据用户提交的 PVC，找到一个对应的 StorageClass，之后Kubernetes 就会调用该 StorageClass 声明的存储插件，进而创建出需要的 PV。 ","date":"2022-12-11","objectID":"/posts/pv_pvc/:3:0","tags":null,"title":"PV 和 PVC","uri":"/posts/pv_pvc/"},{"categories":["kubernetes"],"content":"为什么需要 StorageClass 在一个大规模的 Kubernetes 集群里，可能有成千上万个 PVC，这就意味着运维人员必须实现创建出这个多个 PV，此外，随着项目的需要，会有新的 PVC 不断被提交，那么运维人员就需要不断的添加新的，满足要求的 PV，否则新的 Pod 就会因为 PVC 绑定不到 PV 而导致创建失败。而且通过 PVC 请求到一定的存储空间也很有可能不足以满足应用对于存储设备的各种需求。 而且不同的应用程序对于存储性能的要求可能也不尽相同，比如读写速度、并发性能等，为了解决这一问题，Kubernetes 又为我们引入了一个新的资源对象：StorageClass，通过 StorageClass 的定义，管理员可以将存储资源定义为某种类型的资源，比如快速存储、慢速存储等，用户根据 StorageClass 的描述就可以非常直观的知道各种存储资源的具体特性了，这样就可以根据应用的特性去申请合适的存储资源了。 ","date":"2022-12-11","objectID":"/posts/pv_pvc/:3:1","tags":null,"title":"PV 和 PVC","uri":"/posts/pv_pvc/"},{"categories":["kubernetes"],"content":"创建 资源清单如下 kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: local-storage provisioner: Local # 卷插件（如：local NFS） reclaimPolicy: Retain # 回收策略 volumeBindingMode: Immediate # 绑定模式 --- apiVersion: v1 kind: PersistentVolume metadata: name: local-pv spec: capacity: storage: 1Gi volumeMode: Filesystem storageClassName: local-storage accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain local: path: /home/txl/data nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: pv operator: In values: - local --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: ngx-pvc spec: accessModes: - ReadWriteOnce storageClassName: local-storage resources: requests: storage: 1Gi --- apiVersion: apps/v1 kind: Deployment metadata: name: ngx-sc spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: ngx1 image: nginx:1.18-alpine imagePullPolicy: IfNotPresent volumeMounts: - name: mydata mountPath: /data ports: - containerPort: 80 volumes: - name: mydata persistentVolumeClaim: claimName: ngx-pvc ","date":"2022-12-11","objectID":"/posts/pv_pvc/:3:2","tags":null,"title":"PV 和 PVC","uri":"/posts/pv_pvc/"},{"categories":["kubernetes"],"content":"关键配置参数 绑定模式 WaitForFirstConsumer：控制卷绑定和动态制备应该发生在什么时候 Immediate：一旦创建 PVC 就绑定 WaitForFirstConsumer：延迟绑定，直到使用该 PVC 的 Pod 被创建 ","date":"2022-12-11","objectID":"/posts/pv_pvc/:3:3","tags":null,"title":"PV 和 PVC","uri":"/posts/pv_pvc/"},{"categories":["kubernetes"],"content":"挂载到 Pod apiVersion: apps/v1 kind: Deployment metadata: name: ngx-pv spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: ngx1 image: nginx:1.18-alpine imagePullPolicy: IfNotPresent volumeMounts: - name: mydata mountPath: /data ports: - containerPort: 80 volumes: - name: mydata persistentVolumeClaim: claimName: ngx-pvc ","date":"2022-12-11","objectID":"/posts/pv_pvc/:4:0","tags":null,"title":"PV 和 PVC","uri":"/posts/pv_pvc/"},{"categories":["kubernetes"],"content":"在 Kubernetes 中，pod 是应用程序的载体，我们可以通过 pod 的 ip 来访问应用程序，但是 pod 的 ip 地址不是固定的，这也就意味着不方便直接采用 pod 的 ip 对服务进行访问 为了解决这个问题，Kubernetes 提供了 service 资源，service 会对提供同一个服务的多个 pod 进行聚合，并且提供一个统一的入口地址，通过访问 service 的入口地址就能访问到后面的 pod 服务。 通过 service 可以提供负载均衡和服务自动发现 ","date":"2022-12-11","objectID":"/posts/service/:0:0","tags":null,"title":"Service","uri":"/posts/service/"},{"categories":["kubernetes"],"content":"服务类型 ClusterIP：k8s 默认的 ServiceType，通过集群的内部 IP 暴露服务，选择该值时服务只能够在集群内部访问 NodePort：用来对集群外暴露 Service，你可以通过访问集群内的每个 NodeIP:NodePort 的方式，访问到对应 Service 后端的 Endpoint LoadBalancer: 这也是用来对集群外暴露服务的，不同的是这需要外部负载均衡器的云提供商，比如 AWS 等 ExternalName：这个也是在集群内发布服务用的，需要借助 KubeDNS(version \u003e= 1.7) 的支持，就是用KubeDNS 将该 service 和 ExternalName 做一个 Map，KubeDNS 返回一个 CNAME 记录。 每种服务类型都是会指定一个 clusterIP 的，由 clusterIP 进入对应代理模式实现负载均衡，如果强制 spec.clusterIP: \"None\"（即 headless service），集群无法为它们实现负载均衡，直接通过 pod 域名访问pod，典型是应用是 StatefulSet。 ","date":"2022-12-11","objectID":"/posts/service/:1:0","tags":null,"title":"Service","uri":"/posts/service/"},{"categories":["kubernetes"],"content":"Service 使用 ","date":"2022-12-11","objectID":"/posts/service/:2:0","tags":null,"title":"Service","uri":"/posts/service/"},{"categories":["kubernetes"],"content":"创建 创建 deployment 信息，设置 app=nginx 的标签 apiVersion: apps/v1 kind: Deployment metadata: name: myngx spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: ngx image: \"nginx:1.18-alpine\" 创建一个名为 nginx-svc 的 service 对象，它会将请求代理到80端口且具有标签 app: nginx 的 pod 上 apiVersion: v1 kind: Service metadata: name: nginx-svc spec: type: ClusterIP selector: # 通过selector和pod建立关联 app: nginx ports: - port: 80 targetPort: 80 ","date":"2022-12-11","objectID":"/posts/service/:2:1","tags":null,"title":"Service","uri":"/posts/service/"},{"categories":["kubernetes"],"content":"EndPoint Endpoint 是 k8s 中的一个资源对象，存储在 etcd 中，用来记录一个 service 对应的所有 pod 的访问地址，它是根据 service 配置文件中的 selector 描述产生的 一个 service 由一组 pod 组成，这些 pod 通过 endpoints 暴露出来，endpoints 是实现实际服务的端点集合。换句话说，service 和 pod 之间的联系是通过 endpoints 实现的。 $ kubectl get endpoints NAME ENDPOINTS AGE kubernetes 192.168.0.111:6443 12d nginx-svc 10.244.0.181:80,10.244.3.32:80 59m ","date":"2022-12-11","objectID":"/posts/service/:3:0","tags":null,"title":"Service","uri":"/posts/service/"},{"categories":["kubernetes"],"content":"kube-proxy 主要负责 pod 网络代理，维护网络规则和四层负载均衡工作 service 在很多情况下只是一个概念，真正起作用的其实是 kube-proxy 服务进程，每个 node 节点上都运行一个kube-proxy 服务进程，当创建 service 的时候会通过 api-server 向 etcd 写入创建的 service 信息，而 kube-proxy 会基于监听的机制发现这种 service 的变动，然后它会将最新的 service 信息转换成对应的访问规则 kube-proxy 监听 10249 和 10256 端口，对外提供 /metrics 和 /healthy 的访问 # 查看配置 $ kubectl describe cm kube-proxy -n kube-system # 查看 kube-proxy pod $ kubectl get pods -n kube-system | grep kube-proxy kube-proxy-bxk96 1/1 Running 2 11d kube-proxy-xbv75 1/1 Running 4 12d ","date":"2022-12-11","objectID":"/posts/service/:4:0","tags":null,"title":"Service","uri":"/posts/service/"},{"categories":["kubernetes"],"content":"userspace 模式(废弃) ","date":"2022-12-11","objectID":"/posts/service/:4:1","tags":null,"title":"Service","uri":"/posts/service/"},{"categories":["kubernetes"],"content":"iptables 模式(默认模式) iptables 模式下，节点上 kube-proxy 持续监听 Service 以及 Endpoints 对象的变化，为 service 后端的每个 pod 创建对应的 iptables 规则，当捕获到 Service 的 clusterIP 和端口请求，利用注入的 iptables，将请求重定向到 Service 的对应的 Pod ","date":"2022-12-11","objectID":"/posts/service/:4:2","tags":null,"title":"Service","uri":"/posts/service/"},{"categories":["kubernetes"],"content":"IPVS 模式 IPVS 模式是利用 linux 的 IPVS 模块实现，同样是由 kube-proxy 实时监视集群的 service 和 endpoint。基于内核内哈希表，有更高的网络流量吞吐量（iptables 模式在大规模集群，比如10000 个服务中性能下降显著），并且具有更复杂的负载均衡算法（最小连接、局部性、 加权、持久性） 当 kube-proxy 以 IPVS 代理模式启动时，它将验证 IPVS 内核模块是否可用。 如果未检测到 IPVS 内核模块，则 kube-proxy 将退回到以 iptables 代理模式运行。 ","date":"2022-12-11","objectID":"/posts/service/:4:3","tags":null,"title":"Service","uri":"/posts/service/"},{"categories":["kubernetes"],"content":"无头(HeadLiness) 类型的 Service 在某些场景中，开发人员可能不想使用 Service 提供的负载均衡功能，而希望自己来控制负载均衡策略，针对这种情况，k8s 提供了 HeadLiness Service，这类 Service 不会分配 ClusterIP，如果想要访问 Service，只能通过service 的域名进行查询 apiVersion: v1 kind: Service metadata: name: nginx-svc spec: clusterIP: \"None\" # 将clusterIP设置为None，即可创建headliness Service type: ClusterIP selector: app: nginx ports: - port: 80 targetPort: 80 ","date":"2022-12-11","objectID":"/posts/service/:5:0","tags":null,"title":"Service","uri":"/posts/service/"},{"categories":["kubernetes"],"content":"宿主机访问 Service ","date":"2022-12-11","objectID":"/posts/service/:6:0","tags":null,"title":"Service","uri":"/posts/service/"},{"categories":["kubernetes"],"content":"安装 bind-utils $ sudo yum install bind-utils -y # 无法直接查询到service对应的ip $ nslookup nginx-svc ** server can't find nginx-svc: NXDOMAIN ","date":"2022-12-11","objectID":"/posts/service/:6:1","tags":null,"title":"Service","uri":"/posts/service/"},{"categories":["kubernetes"],"content":"设置解析 查看 kube-dns clusterIp $ kubectl get svc -n kube-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kube-dns ClusterIP 10.96.0.10 \u003cnone\u003e 53/UDP,53/TCP,9153/TCP 11d 修改 /etc/resolv.conf 文件，设置DNS服务器IP地址、DNS域名和设置主机的域名搜索顺序。加入内容 nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local 访问 $ curl nginx-svc ","date":"2022-12-11","objectID":"/posts/service/:6:2","tags":null,"title":"Service","uri":"/posts/service/"},{"categories":["kubernetes"],"content":"Secret 是一种包含少量敏感信息例如密码、OAuth 令牌和 SSH 密钥。 将这些信息放在 secret 中比放在 Pod 的定义或者 容器镜像中来说更加安全和灵活。 Kubernetes 提供若干种内置的类型，用于一些常见的使用场景。 针对这些类型，Kubernetes 所执行的合法性检查操作以及对其所实施的限制各不相同。 内置类型 用法 Opaque 用户定义的任意数据 kubernetes.io/service-account-token 服务账号令牌 kubernetes.io/dockercfg ~/.dockercfg 文件的序列化形式 kubernetes.io/dockerconfigjson ~/.docker/config.json 文件的序列化形式 kubernetes.io/basic-auth 用于基本身份认证的凭据 kubernetes.io/ssh-auth 用于 SSH 身份认证的凭据 kubernetes.io/tls 用于 TLS 客户端或者服务器端的数据 bootstrap.kubernetes.io/token 启动引导令牌数据 ","date":"2022-12-10","objectID":"/posts/secret/:0:0","tags":null,"title":"Secret","uri":"/posts/secret/"},{"categories":["kubernetes"],"content":"基本用法 ","date":"2022-12-10","objectID":"/posts/secret/:1:0","tags":null,"title":"Secret","uri":"/posts/secret/"},{"categories":["kubernetes"],"content":"创建 首先将字符串转换为 base64 $ echo -n 'admin' | base64 $ echo -n '1f2d1e2e67df' | base64 apiVersion: v1 kind: Secret metadata: name: mysecret type: Opaque data: username: YWRtaW4= password: MWYyZDFlMmU2N2Rm 如果希望使用非 base64 编码的字符串直接放入 Secret 中，应当使用 stringData 字段 stringData: config.yaml: | apiUrl: \"https://my.api.com/api/v1\" username: \"admin\" password: \"1f2d1e2e67df\" ","date":"2022-12-10","objectID":"/posts/secret/:1:1","tags":null,"title":"Secret","uri":"/posts/secret/"},{"categories":["kubernetes"],"content":"解码 输出 yaml 内容 $ kubectl get secret mysecret -o yaml $ echo -n 'YWRtaW4=' | base64 -d 使用 JSONPath 模板输出特定字段，文档：JSONPath 支持 | Kubernetes $ kubectl get secret mysecret -o jsonpath={.data.username} | base64 -d ","date":"2022-12-10","objectID":"/posts/secret/:1:2","tags":null,"title":"Secret","uri":"/posts/secret/"},{"categories":["kubernetes"],"content":"以环境变量的方式使用 apiVersion: apps/v1 kind: Deployment metadata: name: myngx spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: ngx image: nginx:1.18-alpine imagePullPolicy: IfNotPresent env: - name: SECRET_USERNAME valueFrom: secretKeyRef: name: mysecret key: username ","date":"2022-12-10","objectID":"/posts/secret/:1:3","tags":null,"title":"Secret","uri":"/posts/secret/"},{"categories":["kubernetes"],"content":"挂载到文件 apiVersion: apps/v1 kind: Deployment metadata: name: myngx spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: ngx image: nginx:1.18-alpine imagePullPolicy: IfNotPresent volumeMounts: - name: foo mountPath: /etc/foo readOnly: true volumes: - name: foo secret: secretName: mysecret items: - key: username path: my-username 如果省略 items 节点，会映射 secret 所有的 key ","date":"2022-12-10","objectID":"/posts/secret/:1:4","tags":null,"title":"Secret","uri":"/posts/secret/"},{"categories":["kubernetes"],"content":"手工配置 basic-auth 认证 ","date":"2022-12-10","objectID":"/posts/secret/:2:0","tags":null,"title":"Secret","uri":"/posts/secret/"},{"categories":["kubernetes"],"content":"生成密码文件 可以使用 htpasswd 或者 openssl passwd 命令生成 安装 $ sudo yum -y install httpd-tools 生成认证密码 创建一个用户名和密码的认证到auth文件中 $ htpasswd -c auth txl ","date":"2022-12-10","objectID":"/posts/secret/:2:1","tags":null,"title":"Secret","uri":"/posts/secret/"},{"categories":["kubernetes"],"content":"导入 Secret $ kubectl create secret generic basic-auth --from-file=auth ","date":"2022-12-10","objectID":"/posts/secret/:2:2","tags":null,"title":"Secret","uri":"/posts/secret/"},{"categories":["kubernetes"],"content":"创建 nginx Configmap apiVersion: v1 kind: ConfigMap metadata: name: ngx data: default: | server { listen 80; server_name localhost; location / { auth_basic \"test auth\"; auth_basic_user_file /etc/nginx/basicauth; # 指向生成的密码文件 root /usr/share/nginx/html; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } } ","date":"2022-12-10","objectID":"/posts/secret/:2:3","tags":null,"title":"Secret","uri":"/posts/secret/"},{"categories":["kubernetes"],"content":"创建 Deployment apiVersion: apps/v1 kind: Deployment metadata: name: ngx spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: ngx image: nginx:1.18-alpine imagePullPolicy: IfNotPresent volumeMounts: - name: nginx-default mountPath: /etc/nginx/conf.d/default.conf # 覆盖nginx默认配置 subPath: default - name: basic-auth mountPath: /etc/nginx/basicauth subPath: auth volumes: - name: nginx-default configMap: name: ngx defaultMode: 0655 - name: basic-auth secret: secretName: basic-auth defaultMode: 0655 ","date":"2022-12-10","objectID":"/posts/secret/:2:4","tags":null,"title":"Secret","uri":"/posts/secret/"},{"categories":["kubernetes"],"content":"访问 $ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES ngx-5597699bdf-ntbjx 1/1 Running 0 31m 10.244.3.29 lain2 \u003cnone\u003e \u003cnone\u003e $ curl --basic -u txl:123 http://10.244.3.29 ","date":"2022-12-10","objectID":"/posts/secret/:2:5","tags":null,"title":"Secret","uri":"/posts/secret/"},{"categories":["kubernetes"],"content":"从私有仓库拉取镜像 使用 Docker Hub 镜像仓库 ","date":"2022-12-10","objectID":"/posts/secret/:3:0","tags":null,"title":"Secret","uri":"/posts/secret/"},{"categories":["kubernetes"],"content":"登录 Docker 镜像仓库 $ docker login --username=\u003c用户名\u003e # 发布 $ docker push \u003c镜像名\u003e ","date":"2022-12-10","objectID":"/posts/secret/:3:1","tags":null,"title":"Secret","uri":"/posts/secret/"},{"categories":["kubernetes"],"content":"创建 DockerHub Secret 创建一个名为 docker-regcred 的 docker registry secret $ kubectl create secret docker-registry docker-regcred \\ --docker-server=https://index.docker.io/v1/ \\ --docker-username=\u003c用户名\u003e \\ --docker-password=\u003c密码\u003e \\ --docker-email=\u003c邮箱地址\u003e 解码 $ kubectl get secret docker-registry -o jsonpath={.data.*} | base64 -d ","date":"2022-12-10","objectID":"/posts/secret/:3:2","tags":null,"title":"Secret","uri":"/posts/secret/"},{"categories":["kubernetes"],"content":"使用 apiVersion: apps/v1 kind: Deployment metadata: name: myalpine spec: selector: matchLabels: app: myalpine replicas: 1 template: metadata: labels: app: myalpine spec: imagePullSecrets: - name: docker-regcred containers: - name: alpine image: lains3/alpine:3.12 imagePullPolicy: IfNotPresent command: [\"sh\",\"-c\",\"echo this is alpine \u0026\u0026 sleep 36000\"] ","date":"2022-12-10","objectID":"/posts/secret/:3:3","tags":null,"title":"Secret","uri":"/posts/secret/"},{"categories":["kubernetes"],"content":"ConfigMap 是一种 API 对象，用来将非机密性的数据保存到键值对中。使用时， Pod 可以将其用作环境变量、命令行参数或者存储卷中的配置文件。 ConfigMap 将你的环境配置信息和容器镜像解耦，便于应用配置的修改。 使用场景： 容器 entrypoint 的命令行参数 容器的环境变量 映射成文件 编写代码在 Pod 中运行，使用 Kubernetes API 来读取 ConfigMap ","date":"2022-12-09","objectID":"/posts/configmap/:0:0","tags":null,"title":"ConfigMap","uri":"/posts/configmap/"},{"categories":["kubernetes"],"content":"使用 apiVersion: v1 kind: ConfigMap metadata: name: mycm data: host: \"0.0.0.0\" port: \"9999\" user.properties: | user.name=txl user.age=18 查看 $ kubectl get cm -n default NAME DATA AGE mycm 3 45m ","date":"2022-12-09","objectID":"/posts/configmap/:1:0","tags":null,"title":"ConfigMap","uri":"/posts/configmap/"},{"categories":["kubernetes"],"content":"在环境变量中使用 apiVersion: apps/v1 kind: Deployment metadata: name: myngx spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: ngx image: nginx:1.18-alpine imagePullPolicy: IfNotPresent env: - name: HOST valueFrom: configMapKeyRef: name: mycm # ConfigMap 名称 key: host # 需要取值的键 ","date":"2022-12-09","objectID":"/posts/configmap/:1:1","tags":null,"title":"ConfigMap","uri":"/posts/configmap/"},{"categories":["kubernetes"],"content":"映射成文件 apiVersion: apps/v1 kind: Deployment metadata: name: myngx spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: ngx image: nginx:1.18-alpine imagePullPolicy: IfNotPresent volumeMounts: - name: cmconfig mountPath: \"/config\" readOnly: true volumes: - name: cmconfig configMap: name: mycm # ConfigMap 名称 items: # 来自 ConfigMap 的一组键，将被创建为文件 - key: \"user.properties\" path: \"userinfo.conf\" 如果省略 items 节点，会映射 ConfigMap 全部的 key $ cd /config \u0026\u0026 ls -l total 0 lrwxrwxrwx 1 root root 11 Dec 6 14:00 host -\u003e ..data/host lrwxrwxrwx 1 root root 11 Dec 6 14:00 port -\u003e ..data/port lrwxrwxrwx 1 root root 22 Dec 6 14:00 user.properties -\u003e ..data/user.properties 使用 subPath 可用于指定所引用的卷内的子路径，而不是其根路径。 apiVersion: apps/v1 kind: Deployment metadata: name: myngx spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: ngx image: nginx:1.18-alpine imagePullPolicy: IfNotPresent env: - name: HOST valueFrom: configMapKeyRef: name: mycm key: host volumeMounts: - name: cmconfig mountPath: /config/userinfo.conf subPath: user.properties volumes: - name: cmconfig configMap: defaultMode: 0655 name: mycm ","date":"2022-12-09","objectID":"/posts/configmap/:1:2","tags":null,"title":"ConfigMap","uri":"/posts/configmap/"},{"categories":["kubernetes"],"content":"使用 client-go 调用 GitHub: kubernetes/client-go: Go client for Kubernetes ","date":"2022-12-09","objectID":"/posts/configmap/:2:0","tags":null,"title":"ConfigMap","uri":"/posts/configmap/"},{"categories":["kubernetes"],"content":"集群外调用 使用 api 代理 $ kubectl proxy --address='0.0.0.0' --accept-hosts='^*$' --port=8009 获取 ConfigMap func getClient() *kubernetes.Clientset { config := \u0026rest.Config{ Host: \"http://ip:8009\", } c, err := kubernetes.NewForConfig(config) if err != nil { log.Fatalln(err) } return c } func main() { client := getClient() cm, err := client.CoreV1().ConfigMaps(\"default\").Get(context.Background(), \"mycm\", v1.GetOptions{}) if err != nil { log.Fatalln(err) } fmt.Println(cm.Data) } 执行结果 map[host:0.0.0.0 port:9999 user.properties:user.name=txl user.age=18 ] ","date":"2022-12-09","objectID":"/posts/configmap/:2:1","tags":null,"title":"ConfigMap","uri":"/posts/configmap/"},{"categories":["kubernetes"],"content":"集群内调用 创建 ServiceAccouont 拥有 default 空间内对 ConfigMap 的查看权限 apiVersion: v1 kind: ServiceAccount metadata: name: sa-cm --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: clusterrole-cm rules: - apiGroups: [\"\"] resources: [\"configmaps\"] verbs: [\"get\", \"watch\", \"list\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: clusterrolebinding-cm namespace: default roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: clusterrole-cm subjects: - kind: ServiceAccount name: sa-cm namespace: default 调用 API token 路径：/var/run/secrets/kubernetes.io/serviceaccount/token api server 地址：https://$KUBERNETES_SERVICE_HOST:$KUBERNETES_PORT_443_TCP_PORT 证书路径：/var/run/secrets/kubernetes.io/serviceaccount/ca.crt var apiServer string var token string func init() { apiServer = fmt.Sprintf(\"https://%s:%s\", os.Getenv(\"KUBERNETES_SERVICE_HOST\"), os.Getenv(\"KUBERNETES_PORT_443_TCP_PORT\")) f, err := os.Open(\"/var/run/secrets/kubernetes.io/serviceaccount/token\") if err != nil { log.Fatal(err) } b, _ := io.ReadAll(f) token = string(b) } func getClient() *kubernetes.Clientset { config := \u0026rest.Config{ Host: apiServer, BearerToken: token, TLSClientConfig: rest.TLSClientConfig{CAFile: \"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\"}, } c, err := kubernetes.NewForConfig(config) if err != nil { log.Fatalln(err) } return c } func main() { client := getClient() cm, err := client.CoreV1().ConfigMaps(\"default\").Get(context.Background(), \"mycm\", v1.GetOptions{}) if err != nil { log.Fatalln(err) } fmt.Println(cm.Data) select {} } 交叉编译 set GOOS=linux set GOARCH=amd64 go build -o cmtest main.go 创建 Deployment apiVersion: apps/v1 kind: Deployment metadata: name: cmtest spec: selector: matchLabels: app: cmtest replicas: 1 template: metadata: labels: app: cmtest spec: serviceAccount: sa-cm # 指定 ServiceAccount nodeName: lain1 # 指定 node containers: - name: cmtest image: alpine:3.12 imagePullPolicy: IfNotPresent command: [\"/app/cmtest\"] volumeMounts: - name: app mountPath: /app volumes: - name: app hostPath: path: /home/txl/goapi type: Directory 查看 $ kubectl get pods NAME READY STATUS RESTARTS AGE cmtest-96b96c458-szxhk 1/1 Running 0 2m46s $ kubectl logs cmtest-96b96c458-szxhk map[host:0.0.0.0 port:9999 user.properties:user.name=txl user.age=18 ] ","date":"2022-12-09","objectID":"/posts/configmap/:2:2","tags":null,"title":"ConfigMap","uri":"/posts/configmap/"},{"categories":["kubernetes"],"content":"监控 ConfigMap 变化 type CmHandler struct{} func(this *CmHandler) OnAdd(obj interface{}){} func(this *CmHandler) OnUpdate(oldObj, newObj interface{}){ if newObj.(*v1.ConfigMap).Name==\"mycm\"{ log.Println(\"mycm发生了变化\") } } func(this *CmHandler) OnDelete(obj interface{}){} func main() { fact:=informers.NewSharedInformerFactory(getClient(), 0) cmInformer:=fact.Core().V1().ConfigMaps() cmInformer.Informer().AddEventHandler(\u0026CmHandler{}) fact.Start(wait.NeverStop) select {} } ","date":"2022-12-09","objectID":"/posts/configmap/:2:3","tags":null,"title":"ConfigMap","uri":"/posts/configmap/"},{"categories":["kubernetes"],"content":"文档：Pod | Kubernetes Pod 是可以在 Kubernetes 中创建和管理的、最小的可部署的计算单元。它是一个或多个容器的组合。这些容器共享存储、网络和命名空间，以及如何运行的规范。其它的资源对象都是用来支撑或者扩展 Pod 对象功能的，比如控制器对象是用来管控 Pod 对象的，Service 或者 Ingress 资源对象是用来暴露 Pod 引用对象的，PersistentVolume 资源对象是用来为 Pod 提供存储等等，K8S 不会直接处理容器，而是 Pod，Pod 是由一个或多个 container 组成。基本的好处有： 方便部署、扩展和收缩、方便调度等 Pod中的容器共享数据和网络空间，统一的资源管理与分配 在Pod中，所有容器都被同一安排和调度，并运行在共享的上下文中。对于具体应用而言，Pod是它们的逻辑主机，Pod包含业务相关的多个应用容器。 每一个 Pod 都有一个特殊的被称为 “根容器” 的 Pause 容器。Pause 容器对应的镜像属于 Kubernetes 平台的一部分，除了 Pause 容器，每个 Pod 还包含一个或多个紧密相关的用户业务容器。Pause 容器的作用： 扮演 Pid=1 的，回收僵尸进程 基于 Linux 的 namespace 的共享 ","date":"2022-12-04","objectID":"/posts/pod/:0:0","tags":null,"title":"Pod、ReplicaSet 和 Deployment","uri":"/posts/pod/"},{"categories":["kubernetes"],"content":"Pod 基本使用 Pod 通常不是直接创建的，而是使用工作负载资源创建的 ","date":"2022-12-04","objectID":"/posts/pod/:1:0","tags":null,"title":"Pod、ReplicaSet 和 Deployment","uri":"/posts/pod/"},{"categories":["kubernetes"],"content":"创建 apiVersion: v1 kind: Pod metadata: name: myngx spec: containers: - name: ngx image: \"nginx:1.18-alpine\" 展示详细信息 $ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES myngx 1/1 Running 0 13m 10.244.3.7 lain2 \u003cnone\u003e \u003cnone\u003e ","date":"2022-12-04","objectID":"/posts/pod/:1:1","tags":null,"title":"Pod、ReplicaSet 和 Deployment","uri":"/posts/pod/"},{"categories":["kubernetes"],"content":"ReplicaSet ReplicaSet 是为了保持维护的期待 Pod 副本数量与现时 Pod 副本数量一致。如在由于 Pod 异常退出导致期待的副本数量不足时，会自动创建新的 Pod 保证到与期望的 Pod 副本数量一致 ","date":"2022-12-04","objectID":"/posts/pod/:2:0","tags":null,"title":"Pod、ReplicaSet 和 Deployment","uri":"/posts/pod/"},{"categories":["kubernetes"],"content":"使用 Deployment Deployment 运行一组相同的 Pod（副本水平扩展）、滚动更新。通过副本集管理和创建POD。我们往往不会直接在集群中使用 ReplicaSet 部署一个新的微服务，一方面是因为 ReplicaSet 的功能其实不够强大，一些常见的更新、扩容和缩容运维操作都不支持，Deployment 的引入就是为了就是为了支持这些复杂的操作 ","date":"2022-12-04","objectID":"/posts/pod/:3:0","tags":null,"title":"Pod、ReplicaSet 和 Deployment","uri":"/posts/pod/"},{"categories":["kubernetes"],"content":"挂载 挂载 hostPath 主机目录卷实例 apiVersion: apps/v1 kind: Deployment metadata: name: myngx spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: ngx image: \"nginx:1.18-alpine\" volumeMounts: # 声明容器中的挂载位置 - name: mydata mountPath: /data - name: alpine # 测试多容器 command: [\"sh\",\"-c\",\"echo this is alpine \u0026\u0026 sleep 3600\"] image: \"alpine:3.12\" volumes: - name: mydata hostPath: path: /home/txl/yaml/data # 声明主机节点目录 type: Directory # 指定type hostPath 支持的 type 值如下： ","date":"2022-12-04","objectID":"/posts/pod/:3:1","tags":null,"title":"Pod、ReplicaSet 和 Deployment","uri":"/posts/pod/"},{"categories":["kubernetes"],"content":"共享文件夹 同一个 pod 内的容器都能读写 EmptyDir 中的文件。常用于临时空间、多容器共享，如日志或者tmp文件需要的临时目录 apiVersion: apps/v1 kind: Deployment metadata: name: myngx spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: ngx image: nginx:1.18-alpine imagePullPolicy: IfNotPresent volumeMounts: - name: sharedata mountPath: /data - name: alpine image: alpine:3.12 imagePullPolicy: IfNotPresent command: [\"sh\",\"-c\",\"echo this is alpine \u0026\u0026 sleep 36000\"] volumeMounts: - name: sharedata mountPath: /data volumes: - name: sharedata emptyDir: {} ","date":"2022-12-04","objectID":"/posts/pod/:3:2","tags":null,"title":"Pod、ReplicaSet 和 Deployment","uri":"/posts/pod/"},{"categories":["kubernetes"],"content":"Init 容器 文档：Init 容器 | Kubernetes Init 容器是一种特殊容器，在 Pod 内的应用容器启动之前运行。Init 容器可以包括一些应用镜像中不存在的实用工具和安装脚本。 Init 容器与普通的容器非常像，除了如下两点： 它们总是运行到完成。 每个都必须在下一个启动之前成功完成。 如果 Pod 的 Init 容器失败，kubelet 会不断地重启该 Init 容器直到该容器成功为止。 然而，如果 Pod 对应的 restartPolicy 值为 “Never”，Kubernetes 不会重新启动 Pod。 原理 在 Pod 启动过程中，每个 Init 容器会在网络和数据卷初始化之后按顺序启动。 依据 Init 容器在 Pod spec 配置中的出现顺序依次运行。由于 Pod 可能各种原因多次重启，所以 Init 容器中的操作，须具备幂等性。 应用场景 环境检查：例如确保应用容器依赖的服务启动后再启动应用容器 初始化配置：例如给应用容器准备配置文件 基本配置 apiVersion: apps/v1 kind: Deployment metadata: name: myngx spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: initContainers: - name: init-mydb image: alpine:3.12 command: ['sh', '-c', 'echo wait for db \u0026\u0026 sleep 35 \u0026\u0026 echo done'] # 模拟等待35s containers: - name: ngx image: nginx:1.18-alpine imagePullPolicy: IfNotPresent volumeMounts: - name: sharedata mountPath: /data - name: alpine image: alpine:3.12 imagePullPolicy: IfNotPresent command: [\"sh\",\"-c\",\"echo this is alpine \u0026\u0026 sleep 36000\"] volumeMounts: - name: sharedata mountPath: /data volumes: - name: sharedata emptyDir: {} 查看 init 容器状态 $ kubectl get pod NAME READY STATUS RESTARTS AGE myngx-89dd8586b-k59pl 0/2 Init:0/1 0 8s 状态 含义 Init:N/M Pod 包含 M 个 Init 容器，其中 N 个已经运行完成。 Init:Error Init 容器已执行失败。 Init:CrashLoopBackOff Init 容器执行总是失败。 Pending Pod 还没有开始执行 Init 容器。 PodInitializing or Running Pod 已经完成执行 Init 容器。 ","date":"2022-12-04","objectID":"/posts/pod/:3:3","tags":null,"title":"Pod、ReplicaSet 和 Deployment","uri":"/posts/pod/"},{"categories":["kubernetes"],"content":"文档：使用 RBAC 鉴权 | Kubernetes ","date":"2022-12-03","objectID":"/posts/rbac/:0:0","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"用户 UserAccount（普通用户）：集群外部访问时使用的用户账号，最常见的就是 kubectl 命令就是作为 kubernetes-admin 用户来执行，k8s本身不记录这些账号 ServiceAccount（服务账户）：它们被绑定到特定的名字空间，服务账号与一组以 Secret 保存的凭据相关，这些凭据会被挂载到 Pod 中，从而允许集群内的进程访问 Kubernetes API ","date":"2022-12-03","objectID":"/posts/rbac/:1:0","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"用户认证 ","date":"2022-12-03","objectID":"/posts/rbac/:2:0","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"A. 使用 X509 客户证书 生成证书 安装 OpenSSL $ sudo yum install openssl openssl-devel 生成一个名称为txl的普通用户的客户端证书 $ mkdir ua/txl $ cd ua/txl # 生成客户端私钥 $ openssl genrsa -out client.key 2048 # 根据私钥生成csr, 指定用户名txl $ openssl req -new -key client.key -out client.csr -subj \"/CN=txl\" # 根据k8s的CA证书生成客户端证书 $ sudo openssl x509 -req -in client.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out client.crt -days 365 证书反解 获取证书设置的CN(Common name) $ openssl x509 -noout -subject -in client.crt 使用证书初步请求API $ kubectl get endpoints NAME ENDPOINTS AGE kubernetes 192.168.0.111:6443 4d5h $ curl --cert ./client.crt --key ./client.key --cacert /etc/kubernetes/pki/ca.crt -s https://192.168.0.111:6443/api { \"kind\": \"APIVersions\", \"versions\": [ \"v1\" ], \"serverAddressByClientCIDRs\": [ { \"clientCIDR\": \"0.0.0.0/0\", \"serverAddress\": \"192.168.0.111:6443\" } ] } 可以使用 --insecure 代替 --cacert /etc/kubernetes/pki/ca.crt 忽略服务端证书验证 证书加入 kube config 把 client.crt 加入到 ~/.kube/config $ kubectl config --kubeconfig=/home/txl/.kube/config set-credentials txl --client-certificate=/home/txl/ua/txl/client.crt --client-key=/home/txl/ua/txl/client.key 创建一个名为 user_context 的 context $ kubectl config --kubeconfig=/home/txl/.kube/config set-context user_context --cluster=kubernetes --user=txl 切换当前上下文为 user_context $ kubectl config use-context user_context # 查看 $ kubectl config current-context # 重新切回默认管理员 $ kubectl config use-context kubernetes-admin@kubernetes ","date":"2022-12-03","objectID":"/posts/rbac/:2:1","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"B. 使用静态令牌文件(Token) token 和证书只能配一个 生成 Token $ head -c 16 /dev/urandom | od -An -t x | tr -d ' ' 加入 kube config $ kubectl config set-credentials txl --token=fdb72d94a1c2c2cfbf82341d1f98c68c 修改 api-server 启动参数 $ sudo vi /etc/kubernetes/pki/token_auth # 加入 fdb72d94a1c2c2cfbf82341d1f98c68c,txl,1001 $ sudo vi /etc/kubernetes/manifests/kube-apiserver.yaml # 加入 --token-auth-file=/etc/kubernetes/pki/token_auth 查看 $ curl -H \"Authorization: Bearer fdb72d94a1c2c2cfbf82341d1f98c68c\" https://192.168.0.111:6443/api/v1/namespaces/default/pods --insecure { \"kind\": \"PodList\", \"apiVersion\": \"v1\", \"metadata\": { \"selfLink\": \"/api/v1/namespaces/default/pods\", \"resourceVersion\": \"1586027\" }, \"items\": [] } ","date":"2022-12-03","objectID":"/posts/rbac/:2:2","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"资源 查看所有资源 $ kubectl api-resources -o wide 其中 VERBS 列展示了该资源对应的操作，比如 role create 创建 delete 删除 deletecollection 批量删除 get 获取 list 列表 patch 合并变更 update 更新 watch 监听 ","date":"2022-12-03","objectID":"/posts/rbac/:3:0","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"Role 和 RoleBinding Role（角色）：包含一组代表相关权限的规则，用于授予对单个命名空间的资源访问 RoleBinding（角色绑定）：将角色中定义的权限赋予一个或者一组用户 ","date":"2022-12-03","objectID":"/posts/rbac/:4:0","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"创建 Role 下面是一个位于default的role，拥有对pod的读访问权限 $ vi role_mypod.yaml kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: namespace: default name: mypod rules: - apiGroups: [\"*\"] resources: [\"pods\"] verbs: [\"get\", \"watch\", \"list\"] 执行创建 $ kubectl apply -f role_mypod.yaml # 查看default空间所有role $ kubectl get role -n default 删除 $ kubectl delete role mypod -n default ","date":"2022-12-03","objectID":"/posts/rbac/:4:1","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"绑定 RoleBinding 创建一个名为mypodbinding的 rolebinding，关联创建的用户txl和创建的角色mypod 1. 使用命令 $ kubectl create rolebinding mypodbinding -n default --role mypod --user txl 2. 使用 yaml apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: creationTimestamp: null name: mypodrolebinding roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: mypod subjects: - apiGroup: rbac.authorization.k8s.io kind: User name: txl - kind: ServiceAccount name: txl 查看 $ kubectl get rolebinding -n default NAME ROLE mypodrolebinding Role/mypod 删除 $ kubectl delete rolebinding mypodbinding -n default ","date":"2022-12-03","objectID":"/posts/rbac/:4:2","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"ClusterRole 和 ClusterRoleBinding ClusterRole 同样可以用于授予 Role 能够授予的权限。 因为 ClusterRole 属于集群范围，所以它也可以为以下资源授予访问权限： 集群范围资源（如节点 Node） 非资源端点（如 /healthz） 跨名字空间访问的名字空间作用域的资源（如 Pod） clusterRole不限定命名空间，绑定既可以使用 RoleBinding，也可以使用 ClusterRoleBinding ","date":"2022-12-03","objectID":"/posts/rbac/:5:0","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"创建 ClusterRole kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: mypod-cluster rules: - apiGroups: [\"*\"] resources: [\"pods\"] verbs: [\"get\", \"watch\", \"list\"] 查看 $ kubectl get clusterrole ","date":"2022-12-03","objectID":"/posts/rbac/:5:1","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"绑定 RoleBinding 需要指定命名空间 apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: mypodrolebinding-cluster namespace: kube-system roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: mypod-cluster subjects: - apiGroup: rbac.authorization.k8s.io kind: User name: txl 查看 $ kubectl get rolebinding -n kube-system NAME ROLE mypodrolebinding-cluster ClusterRole/mypod-cluster ","date":"2022-12-03","objectID":"/posts/rbac/:5:2","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"绑定 ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: mypod-clusterrolebinding roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: mypod-cluster subjects: - apiGroup: rbac.authorization.k8s.io kind: User name: txl - kind: ServiceAccount name: txl namespace: default ","date":"2022-12-03","objectID":"/posts/rbac/:5:3","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"ServiceAccount UserAccount 是可以跨 namespace 的，而 ServiceAccount 只能局限在自己所属的 namespace 中，每个 namespace 都会有一个默认的 default 账号 。 ","date":"2022-12-03","objectID":"/posts/rbac/:6:0","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"创建 SA $ kubectl create sa mysa 也可以导出到 yaml kubectl create sa mysa -o yaml --dry-run=client \u003e mysa.yaml 查看 $ kubectl get sa -n default NAME SECRETS AGE default 1 4d7h mysa 1 5m21s # 查看令牌 $ kubectl describe sa mysa Name: mysa Namespace: default Labels: \u003cnone\u003e Annotations: \u003cnone\u003e Image pull secrets: \u003cnone\u003e Mountable secrets: mysa-token-5bnbm Tokens: mysa-token-5bnbm Events: \u003cnone\u003e $ kubectl describe secret mysa-token-5bnbm ","date":"2022-12-03","objectID":"/posts/rbac/:6:1","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"绑定 ClusterRoleBinding 使用命令的方式 $ kubectl create clusterrolebinding mysa-clusterrolebinding --clusterrole=mypod-cluster --serviceaccount=default:mysa 查看 $ kubectl get clusterrolebinding NAME ROLE mypod-clusterrolebinding ClusterRole/mypod-cluster mysa-clusterrolebinding ClusterRole/mypod-cluster ","date":"2022-12-03","objectID":"/posts/rbac/:6:2","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"外部访问 API 安装 jq 轻量级的 json 处理命令。可以对 json 数据进行分片、过滤、映射、转换和格式化输出 $ sudo yum install jq -y 获取 SA Token 保存到临时变量 mysatoken 中 $ mysatoken=$(kubectl get secret $(kubectl get sa mysa -o json | jq -Mr '.secrets[0].name') -o json | jq -Mr '.data.token' | base64 -d) # 查看token $ echo $mysatoken 请求 $ curl -H \"Authorization: Bearer $mysatoken\" --insecure https://192.168.0.111:6443/api/v1/namespaces/default/pods ","date":"2022-12-03","objectID":"/posts/rbac/:6:3","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"在 Pod 里访问 API 创建一个测试 pod apiVersion: apps/v1 kind: Deployment metadata: name: myngx spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: serviceAccountName: mysa # 指定SA，否则使用的default SA containers: - name: nginxtest image: nginx:1.18-alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80 查看 pod $ kubectl get pod NAME READY STATUS RESTARTS AGE myngx-74748c5956-5rfrs 1/1 Running 0 4m21s 进入容器 $ kubectl exec -it myngx-74748c5956-5rfrs -- sh 设置临时变量 # SA token $ TOKEN=`cat /var/run/secrets/kubernetes.io/serviceaccount/token` # api server地址 $ APISERVER=\"https://$KUBERNETES_SERVICE_HOST:$KUBERNETES_PORT_443_TCP_PORT\" 请求（跳过服务器证书检查） $ curl --header \"Authorization: Bearer $TOKEN\" --insecure -s $APISERVER/api/v1/namespaces/default/pods 使用证书请求 $ curl --header \"Authorization: Bearer $TOKEN\" --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt $APISERVER/api/v1/namespaces/default/pods ","date":"2022-12-03","objectID":"/posts/rbac/:6:4","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"kubeadm是官方社区推出的一个用于快速部署kubernetes集群的工具 ","date":"2022-12-03","objectID":"/posts/kubeadm/:0:0","tags":null,"title":"使用 kubeadm 部署 k8s","uri":"/posts/kubeadm/"},{"categories":["kubernetes"],"content":"部署 文档地址：安装 kubeadm | Kubernetes ","date":"2022-12-03","objectID":"/posts/kubeadm/:1:0","tags":null,"title":"使用 kubeadm 部署 k8s","uri":"/posts/kubeadm/"},{"categories":["kubernetes"],"content":"添加 kubenetes 的 yum 源 在每个节点上分别执行 $ su - $ cat \u003c\u003cEOF \u003e /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF $ yum makecache ","date":"2022-12-03","objectID":"/posts/kubeadm/:1:1","tags":null,"title":"使用 kubeadm 部署 k8s","uri":"/posts/kubeadm/"},{"categories":["kubernetes"],"content":"安装 kubeadm、kubelet 和 kubectl 在每个节点上分别执行 $ sudo yum install -y kubelet-1.21.0 kubeadm-1.21.0 kubectl-1.21.0 安装后查看列表 $ rpm -aq kubelet kubectl kubeadm 把kubelet设置为开机启动 $ sudo systemctl enable kubelet kubeadm init 集群的快速初始化，部署Master节点的各个组件 kubeadm join 节点加入到指定集群中 kubeadm token 管理用于加入集群时使用的认证令牌 (如list，create) kubeadm reset 重置集群，如删除构建文件以回到初始状态 ","date":"2022-12-03","objectID":"/posts/kubeadm/:1:2","tags":null,"title":"使用 kubeadm 部署 k8s","uri":"/posts/kubeadm/"},{"categories":["kubernetes"],"content":"使用 systemd 作为 docker 的 cgroup driver 在每个节点上执行 $ sudo vi /etc/docker/daemon.json 加入内容 { \"exec-opts\": [\"native.cgroupdriver=systemd\"] } 重启docker $ systemctl daemon-reload \u0026\u0026 systemctl restart docker 验证结果 $ docker info |grep Cgroup Cgroup Driver: systemd Cgroup Version: 1 ","date":"2022-12-03","objectID":"/posts/kubeadm/:1:3","tags":null,"title":"使用 kubeadm 部署 k8s","uri":"/posts/kubeadm/"},{"categories":["kubernetes"],"content":"关闭 swap 临时关闭 $ swapoff -a 永久关闭 $ sudo vi /etc/fstab # 注释掉SWAP分区项 # swap was on /dev/sda11 during installation # UUID=0xxxxxxxxxxxxxx4f69 none swap sw 0 0 ","date":"2022-12-03","objectID":"/posts/kubeadm/:1:4","tags":null,"title":"使用 kubeadm 部署 k8s","uri":"/posts/kubeadm/"},{"categories":["kubernetes"],"content":"初始化集群 $ sudo kubeadm init --kubernetes-version=v1.21.0 --image-repository registry.aliyuncs.com/google_containers --pod-network-cidr=10.244.0.0/16 根据输出提示操作 $ mkdir -p $HOME/.kube $ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config $ sudo chown $(id -u):$(id -g) $HOME/.kube/config 默认token的有效期为24小时，当过期之后，该token就不可用了。 查看token列表： $ sudo kubeadm token list 重新生成token： $ sudo kubeadm token create --print-join-command ","date":"2022-12-03","objectID":"/posts/kubeadm/:1:5","tags":null,"title":"使用 kubeadm 部署 k8s","uri":"/posts/kubeadm/"},{"categories":["kubernetes"],"content":"安装网络组件 CNI (Container Network Interface) 容器网络接口，为了让用户在容器创建或销毁时都能够更容易地配置容器网络。常见的组件有： Flannel: 最基本的网络组件 Calico: 支持网络策略 Canal: 前两者的合体 Weave: 同样支持策略机制，还支持加密 使用 flannel Github地址：https://github.com/coreos/flannel 在每个节点执行 $ sudo sysctl net.bridge.bridge-nf-call-iptables=1 在master节点执行 $ kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml # 去污点 $ kubectl taint nodes --all node-role.kubernetes.io/master- 查看 kubectl get pods --all-namespaces 可能出现的错误 基本排查命令 $ kubectl describe $ journalctl -f -u kubelet $ for p in $(kubectl get pods --namespace=kube-system -l k8s-app=kube-dns -o name); do kubectl logs --namespace=kube-system $p; done coredns $ kubectl describe pod -n kube-system coredns-xxx network: open /run/flannel/subnet.env: no such file or directory 手动在每个节点上创建 $ sudo vi /run/flannel/subnet.env # 加入内容 FLANNEL_NETWORK=10.244.0.0/16 FLANNEL_SUBNET=10.244.0.1/24 FLANNEL_MTU=1450 FLANNEL_IPMASQ=true “cni0” already has an IP address different from 10.244.1.1/24 $ sudo ip link delete cni0 # 重启pod $ kubectl delete pod xxx Readiness probe failed: HTTP probe failed with statuscode: 503 $ systemctl stop kubelet $ systemctl stop docker $ iptables --flush $ iptables -tnat --flush $ systemctl start kubelet $ systemctl start docker 节点 not ready $ kubectl describe node lain2 failed to find plugin “xxx” in path [/opt/cni/bin] 把master节点的 /opt/cni/bin 拷贝过来，在master节点执行 $ cd /opt/cni $ sudo scp -r bin root@192.168.0.105:/opt/cni/bin 原因可能是重装k8s的时候没有删除 /etc/cni/net.d 目录 ","date":"2022-12-03","objectID":"/posts/kubeadm/:1:6","tags":null,"title":"使用 kubeadm 部署 k8s","uri":"/posts/kubeadm/"},{"categories":["kubernetes"],"content":"加入子节点 在每个node节点执行刚刚初始化集群时生成的token $ sudo kubeadm join 192.168.0.111:6443 --token fnq8dx.doxwr7sctdm57p0t \\ --discovery-token-ca-cert-hash sha256:114acfe6e30bc0181a93b9135296af62c5f946fa590691577071a4ebf21fc3ee ","date":"2022-12-03","objectID":"/posts/kubeadm/:1:7","tags":null,"title":"使用 kubeadm 部署 k8s","uri":"/posts/kubeadm/"},{"categories":["kubernetes"],"content":"查看集群健康状况 $ kubectl get cs controller-manager \u0026\u0026 scheduler Unhealthy: dial tcp 127.0.0.1:10252 connection refuse 在master节点执行 $ sudo vi /etc/kubernetes/manifests/kube-scheduler.yaml $ sudo vi /etc/kubernetes/manifests/kube-controller-manager.yaml # 注释 # - --port=0 # 重启kublet $ systemctl restart kubelet ","date":"2022-12-03","objectID":"/posts/kubeadm/:1:8","tags":null,"title":"使用 kubeadm 部署 k8s","uri":"/posts/kubeadm/"},{"categories":["kubernetes"],"content":"将集群导入 Rancher 找一个节点执行下载v2.6 $ sudo docker run --privileged -d --restart=unless-stopped -p 80:80 -p 443:443 rancher/rancher:v2.6-head 根据指引执行命令 ","date":"2022-12-03","objectID":"/posts/kubeadm/:2:0","tags":null,"title":"使用 kubeadm 部署 k8s","uri":"/posts/kubeadm/"}]