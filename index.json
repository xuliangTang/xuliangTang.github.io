[{"categories":["go"],"content":"数字证书是一个经证书授权中心数字签名的包含公开密钥拥有者信息以及公开密钥的文件 ","date":"2023-01-24","objectID":"/posts/go-ca-cert/:0:0","tags":null,"title":"使用 Go 生成自签 CA 证书","uri":"/posts/go-ca-cert/"},{"categories":["go"],"content":"使用 Go 自签发证书 go 的 x509 标准库下有个 Certificate 结构，这个结构就是证书解析后对应的实体，新证书需要先生成秘钥对，然后使用根证书的私钥进行签名，证书和私钥以及公钥这里使用的是pem编码方式 package main import ( \"crypto/rand\" \"crypto/rsa\" \"crypto/x509\" \"crypto/x509/pkix\" \"encoding/pem\" \"log\" \"math/big\" mathRand \"math/rand\" \"os\" \"time\" ) const ( CAFile = \"./test/certs/ca.crt\" // CA证书 CAKey = \"./test/certs/ca.key\" // CA私钥 ClientFile = \"./test/certs/lisi.pem\" // 客户端证书 ClientKey = \"./test/certs/lisi_key.pem\" // 客户端私钥 ) func main() { // 解析根证书 caFile, err := os.ReadFile(CAFile) if err != nil { log.Fatal(err) } caBlock, _ := pem.Decode(caFile) caCert, err := x509.ParseCertificate(caBlock.Bytes) // CA证书对象 if err != nil { log.Fatal(err) } // 解析私钥 keyFile, err := os.ReadFile(CAKey) if err != nil { log.Fatal(err) } keyBlock, _ := pem.Decode(keyFile) caPriKey, err := x509.ParsePKCS1PrivateKey(keyBlock.Bytes) // 私钥对象 if err != nil { log.Fatal(err) } // Go 提供了标准库 crypto/x509 给我们提供了 x509 签证的能力，我们可以先通过 x509.Certificate 构建证书签名请求 CSR 然后再进行签证 // 构建新的证书模板，里面的字段可以根据自己需求填写 certTemplate := \u0026x509.Certificate{ SerialNumber: big.NewInt(mathRand.Int63()), // 证书序列号 Subject: pkix.Name{ Country: []string{\"CN\"}, //Organization: []string{\"填的话这里可以用作用户组\"}, //OrganizationalUnit: []string{\"可填课不填\"}, Province: []string{\"beijing\"}, CommonName: \"lisi\", // CN Locality: []string{\"beijing\"}, }, NotBefore: time.Now(), // 证书有效期开始时间 NotAfter: time.Now().AddDate(1, 0, 0), // 证书有效期 BasicConstraintsValid: true, // 基本的有效性约束 IsCA: false, // 是否是根证书 ExtKeyUsage: []x509.ExtKeyUsage{x509.ExtKeyUsageClientAuth, x509.ExtKeyUsageServerAuth}, // 证书用途(客户端认证，数据加密) KeyUsage: x509.KeyUsageDigitalSignature | x509.KeyUsageDataEncipherment, EmailAddresses: []string{\"UserAccount@jtthink.com\"}, } // 生成公私钥秘钥对 priKey, err := rsa.GenerateKey(rand.Reader, 2048) if err != nil { log.Fatal(err) } // 创建证书对象 clientCert, err := x509.CreateCertificate(rand.Reader, certTemplate, caCert, \u0026priKey.PublicKey, caPriKey) if err != nil { log.Fatal(err) } // 编码证书文件和私钥文件 clientCertPem := \u0026pem.Block{ Type: \"CERTIFICATE\", Bytes: clientCert, } clientCertFile, err := os.OpenFile(ClientFile, os.O_WRONLY|os.O_CREATE|os.O_TRUNC, 0600) if err != nil { log.Fatal(err) } err = pem.Encode(clientCertFile, clientCertPem) if err != nil { log.Fatal(err) } buf := x509.MarshalPKCS1PrivateKey(priKey) keyPem := \u0026pem.Block{ Type: \"PRIVATE KEY\", Bytes: buf, } clientKeyFile, _ := os.OpenFile(ClientKey, os.O_WRONLY|os.O_CREATE|os.O_TRUNC, 0600) err = pem.Encode(clientKeyFile, keyPem) if err != nil { log.Fatal(err) } } ","date":"2023-01-24","objectID":"/posts/go-ca-cert/:1:0","tags":null,"title":"使用 Go 生成自签 CA 证书","uri":"/posts/go-ca-cert/"},{"categories":["go"],"content":"使用证书请求k8s api 关联 roleBinding 后测试一下 curl --cert ./lisi.pem --key ./lisi_key.pem --cacert /etc/kubernetes/pki/ca.crt -s https://192.168.0.111:6443/api/v1/namespaces/default/pods ","date":"2023-01-24","objectID":"/posts/go-ca-cert/:2:0","tags":null,"title":"使用 Go 生成自签 CA 证书","uri":"/posts/go-ca-cert/"},{"categories":["k8s-go"],"content":"k8s 实现的“进入某个容器”的功能，底层本质是 Docker 容器通过 exec 进入容器的扩展。本质是新建了一个“与目标容器，共享 namespace 的”新的 shell 进程。所以该 shell 进程，看到的世界，就是容器内的世界了。 通过 client-go 提供的方法，实现通过网页进入 kubernetes 任意容器的终端操作 ","date":"2023-01-05","objectID":"/posts/k8s-pod-shell/:0:0","tags":null,"title":"k8s 远程进入容器 terminal","uri":"/posts/k8s-pod-shell/"},{"categories":["k8s-go"],"content":"remotecommand http://k8s.io/client-go/tools/remotecommand 是 kubernetes client-go 提供的 remotecommand 包，提供了方法与集群中的容器建立长连接，并设置容器的 stdin，stdout 等。 remotecommand 包提供基于 SPDY 协议的 Executor interface，进行和 pod 终端的流的传输。初始化一个 Executor 很简单，只需要调用 remotecommand 的 NewSPDYExecutor 并传入对应参数。 func main() { config, err := clientcmd.BuildConfigFromFlags(\"\", \"kubeconfig\") if err != nil { log.Fatal(err) } client, err := kubernetes.NewForConfig(config) if err != nil { log.Fatal(err) } option := \u0026coreV1.PodExecOptions{ Container: \"nginxtest\", // 容器名称 Command: []string{\"sh\", \"-c\", \"ls\"}, // 命令 Stdin: true, Stdout: true, Stderr: true, } req := client.CoreV1().RESTClient().Post().Resource(\"pods\"). Namespace(\"default\"). Name(\"myngx-79bdb4ccf8-nbln7\"). // pod名称 SubResource(\"exec\"). VersionedParams(option, scheme.ParameterCodec) // 这里初始化了一个 remote-cmd 的对象 exec, err := remotecommand.NewSPDYExecutor(config, \"POST\", req.URL()) if err != nil { log.Fatal(err) } // 这里开始，将输入输出，进行实时传递（Stream） err = exec.StreamWithContext(context.Background(), remotecommand.StreamOptions{ Stdin: os.Stdin, Stdout: os.Stdout, Stderr: os.Stderr, Tty: true, }) if err != nil { log.Fatal(err) } } 将 TTY 设置为 true，命令设置为 sh 进入容器交互式执行 option := \u0026coreV1.PodExecOptions{ Container: \"nginxtest\", Command: []string{\"sh\"}, Stdin: true, Stdout: true, Stderr: true, TTY: true, } ","date":"2023-01-05","objectID":"/posts/k8s-pod-shell/:1:0","tags":null,"title":"k8s 远程进入容器 terminal","uri":"/posts/k8s-pod-shell/"},{"categories":["k8s-go"],"content":"websocket Executor 的 StreamWithContext 方法，会建立一个流传输的连接，直到服务端和调用端一端关闭连接，才会停止传输。常用的做法是定义一个你想用的客户端，实现 Read(p []byte) (int, error) 和 Write(p []byte) (int, error) 方法即可，调用 Stream 方法时，只要将 StreamOptions 的 Stdin Stdout 都设置为该客户端，Executor 就会通过你定义的 write 和 read 方法来传输数据。 var Upgrader websocket.Upgrader func init() { Upgrader = websocket.Upgrader{ CheckOrigin: func(r *http.Request) bool { return true }, } } type WsShellClient struct { client *websocket.Conn } func NewWsShellClient(client *websocket.Conn) *WsShellClient { return \u0026WsShellClient{client: client} } // 实现 io.Writer func (this *WsShellClient) Write(p []byte) (n int, err error) { err = this.client.WriteMessage(websocket.TextMessage, p) if err != nil { return 0, err } return len(p), nil } // 实现 io.Reader func (this *WsShellClient) Read(p []byte) (n int, err error) { _, b, err := this.client.ReadMessage() if err != nil { return 0, err } return copy(p, string(b)+\"\\n\"), nil } func main() { r := gin.New() r.GET(\"/\", func(c *gin.Context) { wsClient, err := ws.Upgrader.Upgrade(c.Writer, c.Request, nil) if err != nil { log.Println(err) return } shellClient := ws.NewWsShellClient(wsClient) option := \u0026coreV1.PodExecOptions{ Container: \"nginxtest\", Command: []string{\"sh\"}, Stdin: true, Stdout: true, Stderr: true, TTY: true, } req := client.CoreV1().RESTClient().Post().Resource(\"pods\"). Namespace(\"default\"). Name(\"myngx-79bdb4ccf8-nbln7\"). SubResource(\"exec\"). VersionedParams(option, scheme.ParameterCodec) exec, err := remotecommand.NewSPDYExecutor(config, \"POST\", req.URL()) if err != nil { log.Println(err) } err = exec.StreamWithContext(c, remotecommand.StreamOptions{ Stdin: shellClient, Stdout: shellClient, Stderr: shellClient, Tty: true, }) if err != nil { log.Println(err) } }) r.Run(\":8080\") } 测试html示例 \u003c!DOCTYPE html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"UTF-8\"\u003e \u003ctitle\u003eTitle\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003cdiv\u003e \u003cdiv id=\"message\" style=\"width: 500px;height:300px;border:solid 1px gray;overflow:auto\"\u003e \u003c/div\u003e \u003cdiv\u003e \u003cinput type=\"type\" id=\"txtCmd\"/\u003e \u003cinput type=\"button\" id=\"cmdBtn\" value=\"发送\"/\u003e \u003cinput type=\"button\" onclick=\"document.getElementById('message').innerHTML=''\" value=\"清空\"/\u003e \u003c/div\u003e \u003c/div\u003e \u003cscript\u003e var ws = new WebSocket(\"ws://localhost:8080/\"); ws.onopen = function(){ console.log(\"open\"); } ws.onmessage = function(e){ let html=document.getElementById(\"message\").innerHTML; html+='\u003cp\u003e服务端消息:' + e.data + '\u003c/p\u003e' document.getElementById(\"message\").innerHTML=html } ws.onclose = function(e){ console.log(\"close\"); } ws.onerror = function(e){ console.log(e); } document.getElementById(\"cmdBtn\").onclick= ()=\u003e{ console.log(document.getElementById(\"txtCmd\").value) ws.send(document.getElementById(\"txtCmd\").value) } \u003c/script\u003e \u003c/body\u003e \u003c/html\u003e ","date":"2023-01-05","objectID":"/posts/k8s-pod-shell/:2:0","tags":null,"title":"k8s 远程进入容器 terminal","uri":"/posts/k8s-pod-shell/"},{"categories":["k8s-go"],"content":"xterm.js 前端页面使用 xterm.js 进行模拟terminal展示，只要 javascript 监听 Terminal 对象的对应事件及 websocket 连接的事件，进行对应的页面展示和消息推送就可以了。 ","date":"2023-01-05","objectID":"/posts/k8s-pod-shell/:3:0","tags":null,"title":"k8s 远程进入容器 terminal","uri":"/posts/k8s-pod-shell/"},{"categories":["k8s-go"],"content":"在 Kubernetes 中，有5个主要的组件，分别是 master 节点上的 kube-api-server、kube-controller-manager 和 kube-scheduler，node 节点上的 kubelet 和kube-proxy 。这其中 kube-apiserver 是对外和对内提供资源的声明式 API 的组件，其它4个组件都需要和它交互。为了保证消息的实时性，有两种方式： 客户端组件 (kubelet, scheduler, controller-manager 等) 轮询 apiserver apiserver 通知客户端 为了降低 kube-apiserver 的压力，有一个非常关键的机制就是 list-watch。list-watch 本质上也是 client 端监听 k8s 资源变化并作出相应处理的生产者消费者框架 list-watach 机制需要满足以下需求： 实时性 (即数据变化时，相关组件越快感知越好) 保证消息的顺序性 (即消息要按发生先后顺序送达目的组件。很难想象在Pod创建消息前收到该Pod删除消息时组件应该怎么处理) 保证消息不丢失或者有可靠的重新获取机制 (比如 kubelet 和 kube-apiserver 间网络闪断，需要保证网络恢复后kubelet可以收到网络闪断期间产生的消息) ","date":"2022-12-18","objectID":"/posts/k8s-list-watch/:0:0","tags":null,"title":"K8s list-watch 机制和 Informer 模块","uri":"/posts/k8s-list-watch/"},{"categories":["k8s-go"],"content":"list-watch 机制 list-watch 由两部分组成，分别是 list 和 watch。list 非常好理解，就是调用资源的 list API 罗列资源 ，基于 HTTP 短链接实现，watch 则是调用资源的 watch API 监听资源变更事件，基于 HTTP 长链接实现 etcd 存储集群的数据信息，apiserver 作为统一入口，任何对数据的操作都必须经过 apiserver。客户端通过 list-watch 监听 apiserver 中资源的 create, update 和 delete 事件，并针对事件类型调用相应的事件处理函数 ","date":"2022-12-18","objectID":"/posts/k8s-list-watch/:1:0","tags":null,"title":"K8s list-watch 机制和 Informer 模块","uri":"/posts/k8s-list-watch/"},{"categories":["k8s-go"],"content":"informer 机制 k8s 的 informer 模块封装 list-watch API，用户只需要指定资源，编写事件处理函数，AddFunc, UpdateFunc 和 DeleteFunc 等。如下图所示，informer 首先通过 list API 罗列资源，然后调用 watch API 监听资源的变更事件，并将结果放入到一个 FIFO 队列，队列的另一头有协程从中取出事件，并调用对应的注册函数处理事件。Informer 还维护了一个只读的 Map Store 缓存，主要为了提升查询的效率，降低 apiserver 的负载 ","date":"2022-12-18","objectID":"/posts/k8s-list-watch/:2:0","tags":null,"title":"K8s list-watch 机制和 Informer 模块","uri":"/posts/k8s-list-watch/"},{"categories":["k8s-go"],"content":"在 client-go 中的应用 client-go 使用 k8s.io/client-go/tools/cache 包里的 informer 对象进行 list-watch 机制的封装 最粗暴的解释： 初始化时，调 List API 获得全量 list，缓存起来(本地缓存)，这样就不需要每次请求都去请求 ApiServer 调用 Watch API 去 watch 资源，发生变更后会通过一定机制维护缓存 type DepHandler struct{} func (this *DepHandler) OnAdd(obj interface{}) {} func (this *DepHandler) OnUpdate(oldObj, newObj interface{}) { if dep, ok := newObj.(*v1.Deployment); ok { fmt.Println(dep.Name) } } func (this *DepHandler) OnDelete(obj interface{}) {} func main() { _, c := cache.NewInformer( // 监听 default 命名空间中 deployment 的变化 cache.NewListWatchFromClient(K8sClient.AppsV1().RESTClient(), \"deployments\", \"default\", fields.Everything()), \u0026v1.Deployment{}, 0, // 重新同步时间 \u0026DepHandler{}, // 实现类 ) c.Run(wait.NeverStop) select {} } ","date":"2022-12-18","objectID":"/posts/k8s-list-watch/:3:0","tags":null,"title":"K8s list-watch 机制和 Informer 模块","uri":"/posts/k8s-list-watch/"},{"categories":["k8s-go"],"content":"SharedInformerFactory sharedInformerFactory 用来构造各种 Informer 的工厂对象，它可以共享多个 informer 资源 informerFactory := informers.NewSharedInformerFactory(K8sClient, 0) // 构建一个 deployment informer depInformer := informerFactory.Apps().V1().Deployments() depInformer.Informer().AddEventHandler(\u0026DepHandler{}) informerFactory.Start(wait.NeverStop) select {} ","date":"2022-12-18","objectID":"/posts/k8s-list-watch/:3:1","tags":null,"title":"K8s list-watch 机制和 Informer 模块","uri":"/posts/k8s-list-watch/"},{"categories":["k8s-go"],"content":"示例 ","date":"2022-12-18","objectID":"/posts/k8s-list-watch/:4:0","tags":null,"title":"K8s list-watch 机制和 Informer 模块","uri":"/posts/k8s-list-watch/"},{"categories":["k8s-go"],"content":"监听 deployment // 全局对象，存储所有deployments var DepMapImpl *DeploymentMap func init() { DepMapImpl = \u0026DeploymentMap{Data: new(sync.Map)} } type DeploymentMap struct { Data *sync.Map // key:namespace value:[]*v1.Deployments } // 添加 func (this *DeploymentMap) Add(deployment *v1.Deployment) { if depList, ok := this.Data.Load(deployment.Namespace); ok { depList = append(depList.([]*v1.Deployment), deployment) this.Data.Store(deployment.Namespace, depList) } else { this.Data.Store(deployment.Namespace, []*v1.Deployment{deployment}) } } // 获取列表 func (this *DeploymentMap) ListByNs(namespace string) ([]*v1.Deployment, error) { if depList, ok := this.Data.Load(namespace); ok { return depList.([]*v1.Deployment), nil } return nil, fmt.Errorf(\"record not found\") } // 更新 func (this *DeploymentMap) Update(deployment *v1.Deployment) error { if depList, ok := this.Data.Load(deployment.Namespace); ok { depList := depList.([]*v1.Deployment) for i, dep := range depList { if dep.Name == deployment.Name { depList[i] = deployment break } } return nil } return fmt.Errorf(\"deployment [%s] not found\", deployment.Name) } // 删除 func (this *DeploymentMap) Delete(deployment *v1.Deployment) { if depList, ok := this.Data.Load(deployment.Namespace); ok { depList := depList.([]*v1.Deployment) for i, dep := range depList { if dep.Name == deployment.Name { newDepList := append(depList[:i], depList[i+1:]...) this.Data.Store(deployment.Namespace, newDepList) break } } } } // informer实现 type DepHandler struct{} func (this *DepHandler) OnAdd(obj interface{}) { DepMapImpl.Add(obj.(*v1.Deployment)) } func (this *DepHandler) OnUpdate(oldObj, newObj interface{}) { err := DepMapImpl.Update(newObj.(*v1.Deployment)) if err != nil { log.Println(err) } } func (this *DepHandler) OnDelete(obj interface{}) { DepMapImpl.Delete(obj.(*v1.Deployment)) } // 执行监听 func InitDeployments() { informerFactory := informers.NewSharedInformerFactory(K8sClient, 0) depInformer := informerFactory.Apps().V1().Deployments() depInformer.Informer().AddEventHandler(\u0026DepHandler{}) informerFactory.Start(wait.NeverStop) } ","date":"2022-12-18","objectID":"/posts/k8s-list-watch/:4:1","tags":null,"title":"K8s list-watch 机制和 Informer 模块","uri":"/posts/k8s-list-watch/"},{"categories":["k8s-go"],"content":"获取 deployment 的关联 pod 之前做过利用 Deployment 的 MatchLabels 去匹配 pod 的 labels 的方式。这次我们利用 ReplicaSet 的标签去匹配 Pod，这种方式可以区分当多个 Deployment 的 Pod 设置为相同标签的场景 当创建完 Deployment 后，k8s 会创建对应的 ReplicaSet，它会根据 template 里的内容进行 hash，然后自动设置一个标签 pod-template-hash，且与它管理的所有 Pod 标签相对应 Labels: app=xnginx pod-template-hash=767447889d 我们只需要通过 Deployment 获取它的 ReplicaSet，再拿 labels 去匹配 Pod 第一步：监听 Deployment、ReplicaSet 和 Pod，分别实现对应的 informer 方法，将数据缓存到本地 第二步：通过 Deployment 获取对应的 ReplicaSet，拿到 labels 关键代码： // 从本地缓存中取出所有的rs rsList, err := RSMapImpl.ListByNs(namespace) // 获取 labels labels, err := GetListWatchRsLabelByDeployment(deployment, rsList) // list-watch方式 根据deployment获取当前ReplicaSet的标签 func GetListWatchRsLabelByDeployment(deployment *v1.Deployment, rsList []*v1.ReplicaSet) (map[string]string, error) { for _, rs := range rsList { if IsCurrentRsByDeployment(rs, deployment) { selector, err := metaV1.LabelSelectorAsMap(rs.Spec.Selector) if err != nil { return nil, err } return selector, nil } } return nil, nil } // 判断rs是否对应当前deployment func IsCurrentRsByDeployment(set *v1.ReplicaSet, deployment *v1.Deployment) bool { if set.ObjectMeta.Annotations[\"deployment.kubernetes.io/revision\"] != deployment.ObjectMeta.Annotations[\"deployment.kubernetes.io/revision\"] { return false } for _, rf := range set.OwnerReferences { if rf.Kind == \"Deployment\" \u0026\u0026 rf.Name == deployment.Name { return true } } return false } 第三步：通过 labels 去匹配 pods 关键代码： // 根据标签获取Pod列表 func (this *PodMap) ListByLabels(ns string, labels map[string]string) ([]*v1.Pod, error) { ret := make([]*v1.Pod, 0) if podList, ok := this.Data.Load(ns); ok { podList := podList.([]*v1.Pod) for _, p := range podList { // 判断标签完全匹配 if reflect.DeepEqual(p.Labels, labels) { ret = append(ret, p) } } return ret, nil } return nil, fmt.Errorf(\"pods not found\") } ","date":"2022-12-18","objectID":"/posts/k8s-list-watch/:4:2","tags":null,"title":"K8s list-watch 机制和 Informer 模块","uri":"/posts/k8s-list-watch/"},{"categories":["k8s-go"],"content":"获取 Pod 状态和 Event Pod 状态信息包含： 阶段：Pod 的 status 字段是一个 PodStatus 对象，其中包含一个 phase 字段 取值 描述 Pending（悬决） Pod 已被 Kubernetes 系统接受，但有一个或者多个容器尚未创建亦未运行。此阶段包括等待 Pod 被调度的时间和通过网络下载镜像的时间。 Running（运行中） Pod 已经绑定到了某个节点，Pod 中所有的容器都已被创建。至少有一个容器仍在运行，或者正处于启动或重启状态。 Succeeded（成功） Pod 中的所有容器都已成功终止，并且不会再重启。 Failed（失败） Pod 中的所有容器都已终止，并且至少有一个容器是因为失败终止。也就是说，容器以非 0 状态退出或者被系统终止。 Unknown（未知） 因为某些原因无法取得 Pod 的状态。这种情况通常是因为与 Pod 所在主机通信失败。 状况：PodStatus 对象包含一个 PodConditions 数组 字段名称 描述 type Pod 状况的名称 status 表明该状况是否适用，可能的取值有 “True\"、\"False” 或 “Unknown” lastProbeTime 上次探测 Pod 状况时的时间戳 lastTransitionTime Pod 上次从一种状态转换到另一种状态时的时间戳 reason 机器可读的、驼峰编码（UpperCamelCase）的文字，表述上次状况变化的原因 message 人类可读的消息，给出上次状态转换的详细信息 PodScheduled：Pod 已经被调度到某节点 PodHasNetwork：Pod 沙箱被成功创建并且配置了网络（Alpha 特性，必须被显式启用） ContainersReady：Pod 中所有容器都已就绪 Initialized：所有的 Init 容器都已成功完成 Ready：Pod 可以为请求提供服务，并且应该被添加到对应服务的负载均衡池中 事件对象：为用户提供了洞察集群内发生的事情的能力。为了避免主节点磁盘空间被填满，将强制执行保留策略：事件在最后一次发生的一小时后将会被删除 关键代码： // EventMapImpl 全局对象，存储所有Event var EventMapImpl *EventMap func init() { EventMapImpl = \u0026EventMap{Data: new(sync.Map)} } type EventMap struct { Data *sync.Map // key:namespace_kind_name value: *v1.Event } func (this *EventMap) GetKey(event *v1.Event) string { key := fmt.Sprintf(\"%s_%s_%s\", event.Namespace, event.InvolvedObject.Kind, event.InvolvedObject.Name) return key } // Add 添加 func (this *EventMap) Add(event *v1.Event) { EventMapImpl.Data.Store(this.GetKey(event), event) } // Delete 删除 func (this *EventMap) Delete(event *v1.Event) { EventMapImpl.Data.Delete(this.GetKey(event)) } // 获取最新一条event message func (this *EventMap) GetMessage(ns string, kind string, name string) string { key := fmt.Sprintf(\"%s_%s_%s\", ns, kind, name) if v, ok := this.Data.Load(key); ok { return v.(*v1.Event).Message } return \"\" } // EventHandler informer实现 type EventHandler struct{} func (this *EventHandler) OnAdd(obj interface{}) { EventMapImpl.Add(obj.(*v1.Event)) } func (this *EventHandler) OnUpdate(oldObj, newObj interface{}) { EventMapImpl.Add(newObj.(*v1.Event)) } func (this *EventHandler) OnDelete(obj interface{}) { EventMapImpl.Delete(obj.(*v1.Event)) } // 评估Pod是否就绪 func GetPodIsReady(pod *coreV1.Pod) bool { for _, condition := range pod.Status.Conditions { if condition.Type == \"ContainersReady\" \u0026\u0026 condition.Status != \"True\" { return false } } for _, rg := range pod.Spec.ReadinessGates { for _, condition := range pod.Status.Conditions { if condition.Type == rg.ConditionType \u0026\u0026 condition.Status != \"True\" { return false } } } return true } // 获取pods DTO 把原生的 pod 对象转换为自己的实体对象 func GetPodsByLabels(ns string, labels []map[string]string) (pods []*model.PodModel) { podList, err := PodMapImpl.ListByLabels(ns, labels) lib.CheckError(err) pods = make([]*model.PodModel, len(podList)) for i, pod := range podList { pods[i] = \u0026model.PodModel{ Name: pod.Name, NodeName: pod.Spec.NodeName, Images: GetPodImages(pod.Spec.Containers), Phase: string(pod.Status.Phase), IsReady: GetPodIsReady(pod), Message: EventMapImpl.GetMessage(pod.Namespace, \"Pod\", pod.Name), CreatedAt: pod.CreationTimestamp.Format(\"2006-01-02 15:04:05\"), } } return } ","date":"2022-12-18","objectID":"/posts/k8s-list-watch/:4:3","tags":null,"title":"K8s list-watch 机制和 Informer 模块","uri":"/posts/k8s-list-watch/"},{"categories":["k8s-go"],"content":"client-go 是负责与 Kubernetes APIServer 服务进行交互的客户端库，利用 Client-Go 与 Kubernetes APIServer 进行的交互访问，来对 Kubernetes 中的各类资源对象进行管理操作，包括内置的资源对象及 CRD ","date":"2022-12-18","objectID":"/posts/k8s-go/:0:0","tags":null,"title":"K8s client-go","uri":"/posts/k8s-go/"},{"categories":["k8s-go"],"content":"client-go 客户端 Client-Go 共提供了 4 种与 Kubernetes APIServer 交互的客户端 RESTClient：最基础的客户端，主要是对 HTTP 请求进行了封装，支持 Json 和 Protobuf 格式的数据。 DiscoveryClient：发现客户端，负责发现 APIServer 支持的资源组、资源版本和资源信息的。 ClientSet：负责操作 Kubernetes 内置的资源对象，例如：Pod、Service等。 DynamicClient：动态客户端，可以对任意的 Kubernetes 资源对象进行通用操作，包括 CRD。 ","date":"2022-12-18","objectID":"/posts/k8s-go/:1:0","tags":null,"title":"K8s client-go","uri":"/posts/k8s-go/"},{"categories":["k8s-go"],"content":"基本使用 参考 API文档 的 group 和 apiVersion 等信息 ","date":"2022-12-18","objectID":"/posts/k8s-go/:2:0","tags":null,"title":"K8s client-go","uri":"/posts/k8s-go/"},{"categories":["k8s-go"],"content":"创建 admin ServiceAccount kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: admin annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\" roleRef: kind: ClusterRole name: cluster-admin apiGroup: rbac.authorization.k8s.io subjects: - kind: ServiceAccount name: admin namespace: kube-system --- apiVersion: v1 kind: ServiceAccount metadata: name: admin namespace: kube-system labels: kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcile 查看 token $ kubectl describe sa admin -n kube-system Name: admin Namespace: kube-system Tokens: admin-token-nzxlb $ kubectl describe secret admin-token-nzxlb -n kube-system ","date":"2022-12-18","objectID":"/posts/k8s-go/:2:1","tags":null,"title":"K8s client-go","uri":"/posts/k8s-go/"},{"categories":["k8s-go"],"content":"建立连接 先使用反代的方式，在 master 节点执行 $ kubectl proxy --address='0.0.0.0' --accept-hosts='^*$' --port=8009 安装客户端库 (版本要对应 Github)，连接 API Server var K8sClient *kubernetes.Clientset func init() { config := \u0026rest.Config{ Host: \"ip:8009\", BearerToken: \"\", } client, err := kubernetes.NewForConfig(config) if err != nil { log.Fatalln(err) } K8sClient = client } ","date":"2022-12-18","objectID":"/posts/k8s-go/:2:2","tags":null,"title":"K8s client-go","uri":"/posts/k8s-go/"},{"categories":["k8s-go"],"content":"示例：获取资源列表 ctx := context.Background() // 查询 kube-system 命名空间下的 service svs, _ := K8sClient.CoreV1().Services(\"kube-system\").List(ctx, v1.ListOptions{}) // 查询 kube-system 命名空间下的 deployment deps, _ := K8sClient.AppsV1().Deployments(\"kube-system\").List(ctx, v1.ListOptions{}) ","date":"2022-12-18","objectID":"/posts/k8s-go/:2:3","tags":null,"title":"K8s client-go","uri":"/posts/k8s-go/"},{"categories":["k8s-go"],"content":"示例：获取资源详情 ctx := context.Background() // 获取名称为 ngx 的 deployment 资源 dep, _ := K8sClient.AppsV1().Deployments(namespace).Get(ctx, \"ngx\", metav1.GetOptions{}) dep.Name // 名称 dep.Namespace // 命名空间 dep.Status.Replicas // 副本数量 dep.CreationTimestamp // 创建时间 dep.Spec.Template.Spec.Containers[0].Image // 第一个镜像名称 ","date":"2022-12-18","objectID":"/posts/k8s-go/:2:4","tags":null,"title":"K8s client-go","uri":"/posts/k8s-go/"},{"categories":["k8s-go"],"content":"示例：获取 deployment 的关联 pod 最简单的方式，利用 Deployment 的 MatchLabels 去匹配 pod 的 labels type PodModel struct { Name string // pod名称 NodeName string // 节点 Images string // 镜像名称 CreatedAt string // 创建时间 } // 拼接labels字符串 func GetLabels(labels map[string]string) string { var labelStr strings.Builder for k, v := range labels { if labelStr.Len() != 0 { labelStr.WriteString(\",\") } labelStr.WriteString(fmt.Sprintf(\"%s=%s\", k, v)) } return labelStr.String() } // 根据deployment获取关联的pods集合 func GetPodsByDep(namespace string, dep *v1.Deployment) (pods []*PodModel) { ctx := context.Background() // 通过LabelSelector去匹配对应的pods listOpt := metav1.ListOptions{ LabelSelector: GetLabels(dep.Spec.Selector.MatchLabels)， } podList, _ := K8sClient.CoreV1().Pods(namespace).List(ctx, listOpt) pods = make([]*PodModel, len(podList.Items)) for i, pod := range podList.Items { pods[i] = \u0026PodModel{ Name: pod.Name, NodeName: pod.Spec.NodeName, Images: GetPodImages(pod.Spec.Containers), CreatedAt: pod.CreationTimestamp.Format(\"2006-01-02 15:04:05\"), } } return } dep, _ := K8sClient.AppsV1().Deployments(namespace).Get(context.Background(), \"ngx\", metav1.GetOptions{}) Pods = GetPodsByDep(\"default\", dep) ","date":"2022-12-18","objectID":"/posts/k8s-go/:2:5","tags":null,"title":"K8s client-go","uri":"/posts/k8s-go/"},{"categories":["k8s-go"],"content":"示例：修改 deployment 副本数量 // 获取 deployment 副本数量 scale, _ := K8sClient.AppsV1().Deployments(\"default\").GetScale(ctx, \"ngx\", v1.GetOptions{}) // 修改副本数量 scale.Spec.Replicas++ K8sClient.AppsV1().Deployments(\"default\").UpdateScale(ctx, \"ngx\", scale, v1.UpdateOptions{}) ","date":"2022-12-18","objectID":"/posts/k8s-go/:2:6","tags":null,"title":"K8s client-go","uri":"/posts/k8s-go/"},{"categories":["k8s-go"],"content":"示例：创建资源 根据 yaml 创建一个 nginx deployment apiVersion: apps/v1 kind: Deployment metadata: name: myngx namespace: default spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: nginxtest image: nginx:1.18-alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80 ngxDep := \u0026appV1.Deployment{} // 读取yaml内容 b, _ := os.ReadFile(\"nginx.yaml\") ngxJson, _ := yaml.ToJSON(b) json.Unmarshal(ngxJson, ngxDep) dep, _ := K8sClient.AppsV1().Deployments(\"default\").Create(context.Background(), ngxDep, v1.CreateOptions{}) ","date":"2022-12-18","objectID":"/posts/k8s-go/:2:7","tags":null,"title":"K8s client-go","uri":"/posts/k8s-go/"},{"categories":["kubernetes"],"content":"Kube-scheduler 是 Kubernetes 集群默认的调度器，并且是控制面中一个核心组件。scheduler 通过 kubernetes 的监测（Watch）机制来发现集群中新创建且尚未被调度到 Node 上的 Pod。 scheduler 会将发现的每一个未调度的 Pod 调度到一个合适的 Node 上来运行。 scheduler会依据下文的调度原则来做出调度选择。 对于新创建的 pod 或其他未调度的 pod来讲，kube-scheduler 选择一个最佳节点供它们运行。但是，Pod 中的每个容器对资源的要求都不同，每个 Pod 也有不同的要求。因此，需要根据具体的调度要求对现有节点进行过滤。 在Kubernetes集群中，满足 Pod 调度要求的节点称为可行节点 （feasible nodes FN） 。如果没有合适的节点，则 pod 将保持未调度状态，直到调度程序能够放置它。也就是说，当我们创建 Pod 时，如果长期处于 Pending 状态，这个时候应该看你的集群调度器是否因为某些问题没有合适的节点了 调度器为 Pod 找到 FN 后，然后运行一组函数对 FN 进行评分，并在 FN 中找到得分最高的节点来运行 Pod。 调度策略在决策时需要考虑的因素包括个人和集体资源需求、硬件/软件/策略约束 （constraints）、亲和性 (affinity) 和反亲和性（ anti-affinity ）规范、数据局部性、工作负载间干扰等。 基本调度流程： 发布 Pod ControllerManager 会把 Pod 加入待调度队列 kube-scheduler 决定调度到哪个 node，然后写入 etcd 被选中节点中的 kubelet 开始工作（pull image、启动 Pod） ","date":"2022-12-13","objectID":"/posts/kube-schedule/:0:0","tags":null,"title":"调度器 kube-schedule","uri":"/posts/kube-schedule/"},{"categories":["kubernetes"],"content":"如何为 Pod 选择节点？ kube-scheduler 给一个 Pod 做调度选择时包含两个步骤： 过滤 (Filtering) 打分 (Scoring) 过滤也被称为预选 （Predicates），该步骤会找到可调度的节点集，然后通过是否满足特定资源的请求，例如通过 PodFitsResources 过滤器检查候选节点是否有足够的资源来满足 Pod 资源的请求。这个步骤完成后会得到一个包含合适的节点的列表（通常为多个），如果列表为空，则Pod不可调度。 打分也被称为优选（Priorities），在该步骤中，会对上一个步骤的输出进行打分，Scheduer 通过打分的规则为每个通过 Filtering 步骤的节点计算出一个分数。 完成上述两个步骤之后，kube-scheduler 会将Pod分配给分数最高的 Node，如果存在多个相同分数的节点，会随机选择一个。 ","date":"2022-12-13","objectID":"/posts/kube-schedule/:1:0","tags":null,"title":"调度器 kube-schedule","uri":"/posts/kube-schedule/"},{"categories":["kubernetes"],"content":"将 Pod 指派给节点 你可以约束一个 Pod 以便限制其只能在特定的节点上运行，或优先在特定的节点上运行。有几种方法可以实现这点，推荐的方法都是用标签选择算符来进行选择。 通常这样的约束不是必须的，因为调度器将自动进行合理的放置，但在某些情况下，你可能需要进一步控制 Pod 被部署到哪个节点。 给节点 lain1 添加一个标签 disktype=ssd $ kubectl label nodes lain1 disktype=ssd # 删除标签 $ kubectl label nodes lain1 disktype- ","date":"2022-12-13","objectID":"/posts/kube-schedule/:2:0","tags":null,"title":"调度器 kube-schedule","uri":"/posts/kube-schedule/"},{"categories":["kubernetes"],"content":"nodeSelector 设置你希望目标节点所具有的节点标签。 apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent nodeSelector: disktype: ssd # 该Pod将被调度到有disktype=ssd标签的节点 ","date":"2022-12-13","objectID":"/posts/kube-schedule/:2:1","tags":null,"title":"调度器 kube-schedule","uri":"/posts/kube-schedule/"},{"categories":["kubernetes"],"content":"节点亲和性 nodeAffinity 节点亲和性有两种： requiredDuringSchedulingIgnoredDuringExecution：调度器只有在规则被满足的时候才能执行调度 preferredDuringSchedulingIgnoredDuringExecution：调度器会尝试寻找满足对应规则的节点。如果找不到匹配的节点，调度器仍然会调度该 Pod 强制的节点亲和性调度 下面的 pod 只会调度到具有 disktype=ssd 标签的节点上 apiVersion: v1 kind: Pod metadata: name: nginx spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: disktype operator: In values: - ssd containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent 首选的节点亲和性调度 apiVersion: v1 kind: Pod metadata: name: nginx spec: affinity: nodeAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 # 权重 preference: matchExpressions: - key: disktype operator: In values: - ssd containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent ","date":"2022-12-13","objectID":"/posts/kube-schedule/:2:2","tags":null,"title":"调度器 kube-schedule","uri":"/posts/kube-schedule/"},{"categories":["kubernetes"],"content":"节点污点和容忍度 节点亲和性使 Pod 被吸引到一类特定的节点，污点 (Taint) 则相反，它使节点能够排斥一类特定的 Pod。污点有三种类型： NoSchedule：不会将 Pod 调度到该节点 PreferNoSchedule：尽量避免将 Pod 调度到该节点上 NoExecute：任何不能忍受这个污点的 Pod 都会马上被驱逐 一些内置的污点： node.kubernetes.io/not-ready：节点未准备好。这相当于节点状况 Ready 的值为 False node.kubernetes.io/unreachable：节点控制器访问不到节点. 这相当于节点状况 Ready 的值为 Unknown node.kubernetes.io/memory-pressure：节点存在内存压力 node.kubernetes.io/disk-pressure：节点存在磁盘压力 node.kubernetes.io/pid-pressure: 节点的 PID 压力 node.kubernetes.io/network-unavailable：节点网络不可用 node.kubernetes.io/unschedulable: 节点不可调度 node.cloudprovider.kubernetes.io/uninitialized：如果 kubelet 启动时指定了一个“外部”云平台驱动， 它将给当前节点添加一个污点将其标志为不可用。在 cloud-controller-manager 的一个控制器初始化这个节点后，kubelet 将删除这个污点 容忍度 (Toleration) 是应用于 Pod 上的。容忍度允许调度器调度带有对应污点的 Pod。 容忍度允许调度但并不保证调度：作为其功能的一部分， 调度器也会评估其他参数。 # 查看节点的污点 $ kubectl describe node lain1 | grep Taints # 给节点打一个污点 $ kubectl taint nodes lain1 key1=value1:NoSchedule # 删除污点 $ kubectl taint node lain1 key1:NoSchedule- 使用容忍 apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent tolerations: - key: \"key1\" # 对应污点key operator: \"Equal\" value: \"value1\" effect: \"NoSchedule\" ","date":"2022-12-13","objectID":"/posts/kube-schedule/:2:3","tags":null,"title":"调度器 kube-schedule","uri":"/posts/kube-schedule/"},{"categories":["kubernetes"],"content":"Pod 亲和性 Pod 间亲和性与反亲和性使你可以基于已经在节点上运行的 Pod 的标签来约束 Pod 可以调度到的节点，而不是基于节点上的标签。 Pod 亲和性和反亲和性都需要相当的计算量，因此会在大规模集群中显著降低调度速度。 不建议在包含数百个节点的集群中使用这类设置 与节点亲和性类似，Pod 的亲和性与反亲和性也有两种类型： requiredDuringSchedulingIgnoredDuringExecution preferredDuringSchedulingIgnoredDuringExecution 实例资源清单 apiVersion: apps/v1 kind: Deployment metadata: name: ngx1 spec: selector: matchLabels: app: ngx1 replicas: 1 template: metadata: labels: app: ngx1 spec: nodeName: lain1 containers: - name: ngx1 image: nginx:1.18-alpine imagePullPolicy: IfNotPresent 下面这个 Pod 必须调度到具有 disktype 标签的节点上，并且集群中至少有一个位于该可用区的节点上运行着带有 app=ngx1 标签的 Pod apiVersion: apps/v1 kind: Deployment metadata: name: ngx2 spec: selector: matchLabels: app: ngx2 replicas: 1 template: metadata: labels: app: ngx2 spec: affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - ngx1 topologyKey: disktype containers: - name: ngx2 image: nginx:1.18-alpine imagePullPolicy: IfNotPresent ","date":"2022-12-13","objectID":"/posts/kube-schedule/:2:4","tags":null,"title":"调度器 kube-schedule","uri":"/posts/kube-schedule/"},{"categories":["kubernetes"],"content":"Horizontal Pod Autoscaling（Pod 水平自动伸缩），简称HPA。它可以基于 CPU 利用率或其他指标自动扩缩 ReplicationController、Deployment 和 ReplicaSet 中的 Pod 数量，它不适用于无法扩缩的对象，比如 DaemonSet。除了 CPU 利用率，也可以基于其他应程序提供的自定义度量指标来执行自动扩缩 文档：Pod 水平自动扩缩 | Kubernetes 我们可以简单的通过 kubectl autoscale 命令来创建一个 HPA 资源对象，HPA Controller 默认 30s 轮询一次（可通过 kube-controller-manager 的 --horizontal-pod-autoscaler-sync-period 参数进行设置)，查询指定的资源中的 Pod 资源使用率，并且与创建时设定的值和指标做对比，从而实现自动伸缩的功能。 ","date":"2022-12-12","objectID":"/posts/hpa/:0:0","tags":null,"title":"Pod 水平自动扩缩 — HPA","uri":"/posts/hpa/"},{"categories":["kubernetes"],"content":"Metrics Server Metrics Server 可以通过标准的 Kubernetes Summary API 把监控数据暴露出来，有了 Metrics Server 之后，就可以采集节点和 Pod 的内存、磁盘、CPU 和网络的使用率等。Metrics API URI 为 /apis/metrics.k8s.io/ ","date":"2022-12-12","objectID":"/posts/hpa/:1:0","tags":null,"title":"Pod 水平自动扩缩 — HPA","uri":"/posts/hpa/"},{"categories":["kubernetes"],"content":"安装 可以通过官方仓库的资源清单安装： Github 部署之前，需要修改 k8s.gcr.io/metrics-server/metrics-server 镜像的地址 # image: k8s.gcr.io/metrics-server/metrics-server:v0.4.1 image: bitnami/metrics-server:0.4.1 等待部署完成后，可以查看 pod 日志是否正常 $ kubectl get pods -n kube-system -l k8s-app=metrics-server NAME READY STATUS RESTARTS AGE metrics-server-7d8467779f-vgtzb 1/1 Running 0 18m $ kubectl logs -f metrics-server-7d8467779f-vgtzb -n kube-system ","date":"2022-12-12","objectID":"/posts/hpa/:1:1","tags":null,"title":"Pod 水平自动扩缩 — HPA","uri":"/posts/hpa/"},{"categories":["kubernetes"],"content":"查看 $ kubectl top nodes NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% lain1 149m 3% 3526Mi 45% lain2 208m 10% 2786Mi 75% $ kubectl top pod etcd-lain1 -n kube-system NAME CPU(cores) MEMORY(bytes) etcd-lain1 20m 249Mi ","date":"2022-12-12","objectID":"/posts/hpa/:1:2","tags":null,"title":"Pod 水平自动扩缩 — HPA","uri":"/posts/hpa/"},{"categories":["kubernetes"],"content":"使用 HPA 的 API 有三个版本，在当前稳定版本 autoscaling/v1 中只支持基于 CPU 指标的缩放。在 Beta 版本 autoscaling/v2beta2，引入了基于内存和自定义指标的缩放。 $ kubectl api-versions | grep autoscal autoscaling/v1 # 只支持通过cpu伸缩 autoscaling/v2beta1 # 支持通过cpu、内存和自定义数据来进行伸缩 autoscaling/v2beta2 我们部署一个测试 api，执行一些 CUP 密集型计算，然后利用 HAP 来进行自动伸缩容 test := map[string]string{ \"str\": \"requests来设置各容器需要的最小资源\", } r := gin.New() r.GET(\"/\", func(context *gin.Context) { ret := 0 for i := 0; i \u003c= 1000000; i++ { t := map[string]string{} b, _ := json.Marshal(test) _ = json.Unmarshal(b, t) ret++ } context.JSON(200, gin.H{\"message\": ret}) }) r.Run(\":8080\") 资源清单如下 apiVersion: apps/v1 kind: Deployment metadata: name: web1 spec: selector: matchLabels: app: myweb replicas: 1 template: metadata: labels: app: myweb spec: nodeName: lain1 containers: - name: web1test image: alpine:3.12 imagePullPolicy: IfNotPresent command: [\"/app/stress\"] volumeMounts: - name: app mountPath: /app resources: requests: cpu: \"200m\" memory: \"256Mi\" limits: cpu: \"400m\" # 1物理核=1000个微核(millicores) 1000m=1CPU memory: \"512Mi\" ports: - containerPort: 8080 volumes: - name: app hostPath: path: /home/txl/goapi --- apiVersion: v1 kind: Service metadata: name: web1 spec: type: ClusterIP ports: - port: 80 targetPort: 8080 selector: app: myweb requests 节点用来设置各容器需要的最小资源 limits 节点用于限制运行时容器占用的资源 ","date":"2022-12-12","objectID":"/posts/hpa/:2:0","tags":null,"title":"Pod 水平自动扩缩 — HPA","uri":"/posts/hpa/"},{"categories":["kubernetes"],"content":"创建 现在创建一个 HPA 资源对象，可以使用命令创建 $ kubectl autoscale deployment web1 --min=1 --max=5 --cpu-percent=20 $ kubectl get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE web1 Deployment/web1 0%/20% 1 5 1 12m 此命令创建了一个关联资源 web1 的 HPA，最小的 Pod 副本数为1，最大为5。HPA 会根据设定的 cpu 使用率（20%）动态的增加或者减少 Pod 数量。 也可以使用 yaml 来创建 apiVersion: autoscaling/v2beta2 kind: HorizontalPodAutoscaler metadata: name: web1hpa namespace: default spec: minReplicas: 1 maxReplicas: 5 scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: web1 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 50 # 使用率 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 50 你还可以指定资源度量指标使用绝对数值，而不是百分比，你需要将 target.type 从 Utilization 替换成 AverageValue，同时设置 target.averageValue 而非 target.averageUtilization 的值 metrics: - type: Resource resource: name: cpu target: type: AverageValue averageValue: 230m # 使用量 - type: Resource resource: name: memory target: type: AverageValue averageValue: 400m ","date":"2022-12-12","objectID":"/posts/hpa/:2:1","tags":null,"title":"Pod 水平自动扩缩 — HPA","uri":"/posts/hpa/"},{"categories":["kubernetes"],"content":"压测 $ sudo yum -y install httpd-tools $ ab -n 10000 -c 10 http://web1/ 可以看到，HPA 已经开始工作，副本数量已经从原来的1变成了4个 $ kubectl get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE web1 Deployment/web1 192%/20% 1 5 4 107s 查看 HPA 资源工作过程 $ kubectl describe hpa web1 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulRescale 20m horizontal-pod-autoscaler New size: 4; reason: cpu resource utilization (percentage of request) above target Normal SuccessfulRescale 20m horizontal-pod-autoscaler New size: 5; reason: cpu resource utilization (percentage of request) above target Normal SuccessfulRescale 14m horizontal-pod-autoscaler New size: 1; reason: All metrics below target ","date":"2022-12-12","objectID":"/posts/hpa/:2:2","tags":null,"title":"Pod 水平自动扩缩 — HPA","uri":"/posts/hpa/"},{"categories":["kubernetes"],"content":"可能出现的错误 ","date":"2022-12-12","objectID":"/posts/hpa/:3:0","tags":null,"title":"Pod 水平自动扩缩 — HPA","uri":"/posts/hpa/"},{"categories":["kubernetes"],"content":"Pod 启动异常 x509: cannot validate certificate for 192.168.0.111 because it doesn’t contain any ip sans node=“lain1” 因为 Kubelet 证书需要由群集证书颁发机构签名 ，或者给 Metrics Server 增加配置参数 –Kubelet-insecure-tls 来禁用证书验证 解决这个问题的方法是使用 APIServer 签署 Kubelet 证书。 首先编辑 kube-system namespace 中的 kubelet-config ConfigMap，在 kind: KubeletConfiguration 下方增加内容 serverTLSBootstrap: true 然后分别为每个节点上修改 kubelet-config configmap $ sudo vi /var/lib/kubelet/config.yaml # 在 kind: KubeletConfiguration 下方增加内容 serverTLSBootstrap: true 然后重启 kubelet $ systemctl restart kubelet Kubelet 都会生成一个 CSR 并将其提交给 APIServer，您需要为集群上的每个 Kubelet 批准 CSR $ kubectl get csr NAME AGE SIGNERNAME REQUESTOR CONDITION csr-clwz7 44m kubernetes.io/kubelet-serving system:node:lain1 Approved,Issued csr-w6kpf 34m kubernetes.io/kubelet-serving system:node:lain2 Approved,Issued $ kubectl certificate approve csr-clwz7 csr-w6kpf 默认情况下，这些服务证书将在一年后过期。因此，一年后，Kubelet 将生成一个新的 CSR，您需要批准它 参考：https://particule.io/en/blog/kubeadm-metrics-server ","date":"2022-12-12","objectID":"/posts/hpa/:3:1","tags":null,"title":"Pod 水平自动扩缩 — HPA","uri":"/posts/hpa/"},{"categories":["kubernetes"],"content":"为了能够屏蔽底层存储实现的细节，方便用户使用，k8s 引入 PV 和 PVC 两种资源对象。Persistent Volume 提供存储资源（并实现），Persistent Volume Claim 描述需要的存储标准，然后从现有 PV 中匹配或者动态建立新的资源，最后将两者进行绑定。 ","date":"2022-12-11","objectID":"/posts/pv_pvc/:0:0","tags":null,"title":"PV 和 PVC","uri":"/posts/pv_pvc/"},{"categories":["kubernetes"],"content":"持久卷（Persistent Volume） PV 是持久化卷的意思，是对底层的共享存储的一种抽象。一般情况下 PV 由 k8s 管理员进行创建和配置，它与底层具体的共享存储技术有关，并通过插件（如：local、NFS）等具体的底层技术来实现完成与共享存储的对接。 ","date":"2022-12-11","objectID":"/posts/pv_pvc/:1:0","tags":null,"title":"PV 和 PVC","uri":"/posts/pv_pvc/"},{"categories":["kubernetes"],"content":"持久卷的类型 cephfs - CephFS volume csi - 容器存储接口 (CSI) fc - Fibre Channel (FC) 存储 hostPath - HostPath 卷 （仅供单节点测试使用；不适用于多节点集群；请尝试使用 local 卷作为替代） iscsi - iSCSI (SCSI over IP) 存储 local - 节点上挂载的本地存储设备 nfs - 网络文件系统 (NFS) 存储 rbd - Rados 块设备 (RBD) 卷 ","date":"2022-12-11","objectID":"/posts/pv_pvc/:1:1","tags":null,"title":"PV 和 PVC","uri":"/posts/pv_pvc/"},{"categories":["kubernetes"],"content":"创建 使用 local 卷的资源清单 apiVersion: v1 kind: PersistentVolume metadata: name: local-pv spec: capacity: storage: 1Gi volumeMode: Filesystem accessModes: - ReadWriteOnce storageClassName: \"\" # 存储类别 persistentVolumeReclaimPolicy: Retain local: path: /home/txl/data nodeAffinity: required: # 指定必须满足的硬性节点约束 nodeSelectorTerms: # 节点选择器条件的列表 - matchExpressions: # 基于节点标签所设置的节点选择器要求的列表 - key: pv # 适用的标签主键 operator: In # 代表主键与值集之间的关系 values: - local ","date":"2022-12-11","objectID":"/posts/pv_pvc/:1:2","tags":null,"title":"PV 和 PVC","uri":"/posts/pv_pvc/"},{"categories":["kubernetes"],"content":"关键配置参数 储存能力 capacity：目前只支持存储空间的设置（storage=1Gi） 卷模式 volumeMode：设置为 Filesystem 的卷会被 Pod 挂载（Mount）到某个目录。 如果卷的存储来自某块设备而该设备目前为空，Kuberneretes 会在第一次挂载卷之前在设备上创建文件系统 访问模式 accessModes：用于描述用户应用对存储资源的访问权限，访问权限包括下面几种方式： ReadWriteOnce（RWO）：读写权限，但是只能被单个节点挂载 ReadOnlyMany（ROX）：只读权限，可以被多个节点挂载 ReadWriteMany（RWX）：读写权限，可以被多个节点挂载 存储类别 storageClassName：PV 可以通过 storageClassName 参数指定一个存储类别： 具有特定类别的 PV 只能与请求了该类别的 PVC 进行绑定 未设定类别的 PV 只能与不请求任何类别的 PVC 进行绑定 回收策略 persistentVolumeReclaimPolicy：当 PV 不再被使用了之后，对其的处理方式。目前支持三种策略： Retain（保留）：保留数据，需要管理员手动清理数据 Recycle（回收）：清除PV中的数据，效果相当于执行 rm -rf /thevolume/* Delete（删除）：与PV相连的后端存储完成volume的删除操作，当然这常见于云服务商的存储服务 节点亲和性 NodeAffinity：定义一些约束，进而限制从哪些节点上可以访问此卷。matchExpressions 的 operator包括： In：label 的值在某个列表中 NotIn：label 的值不在某个列表中 Exists：某个 label 存在 DoesNotExist：某个 label 不存在 Gt：label 的值大于某个值（字符串比较） Lt：label 的值小于某个值（字符串比较） 状态 status：一个 PV 的生命周期中，可能会处于4种不同的阶段 Available（可用）：表示可用状态，还未被任何PVC绑定 Bound（已绑定）：表示PV已经被PVC绑定 Released（已释放）：表示PVC被删除，但是资源还未被集群重新声明 Failed（失败）：表示该PV的自动回收失败 ","date":"2022-12-11","objectID":"/posts/pv_pvc/:1:3","tags":null,"title":"PV 和 PVC","uri":"/posts/pv_pvc/"},{"categories":["kubernetes"],"content":"标签 查看标签 $ kubectl get node --show-labels=true 给 node lain1 打一个标签 pv=local $ kubectl label nodes lain1 pv=local 删除标签 $ kubectl label nodes lain1 pv- ","date":"2022-12-11","objectID":"/posts/pv_pvc/:1:4","tags":null,"title":"PV 和 PVC","uri":"/posts/pv_pvc/"},{"categories":["kubernetes"],"content":"持久卷声明（PersistentVolumeClaim） PVC 是资源的申请，用来声明对存储空间、访问模式、存储类别需求信息 ","date":"2022-12-11","objectID":"/posts/pv_pvc/:2:0","tags":null,"title":"PV 和 PVC","uri":"/posts/pv_pvc/"},{"categories":["kubernetes"],"content":"创建 绑定：spec 关键字段要匹配，storageClassName 字段必须一致 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: ngx-pvc spec: accessModes: - ReadWriteOnce storageClassName: \"\" resources: requests: storage: 1Gi ","date":"2022-12-11","objectID":"/posts/pv_pvc/:2:1","tags":null,"title":"PV 和 PVC","uri":"/posts/pv_pvc/"},{"categories":["kubernetes"],"content":"存储类（StorageClass） Kubernetes 提供了一套可以自动创建 PV 的机制，即：Dynamic Provisioning。而这个机制的核心在于StorageClass 这个 API 对象。StorageClass 对象会定义下面两部分内容: PV 的属性，如存储类型，Volume 的大小等。 创建这种 PV 需要用到的存储插件，即存储制备器。 有了这两个信息之后，Kubernetes 就能够根据用户提交的 PVC，找到一个对应的 StorageClass，之后Kubernetes 就会调用该 StorageClass 声明的存储插件，进而创建出需要的 PV。 ","date":"2022-12-11","objectID":"/posts/pv_pvc/:3:0","tags":null,"title":"PV 和 PVC","uri":"/posts/pv_pvc/"},{"categories":["kubernetes"],"content":"为什么需要 StorageClass 在一个大规模的 Kubernetes 集群里，可能有成千上万个 PVC，这就意味着运维人员必须实现创建出这个多个 PV，此外，随着项目的需要，会有新的 PVC 不断被提交，那么运维人员就需要不断的添加新的，满足要求的 PV，否则新的 Pod 就会因为 PVC 绑定不到 PV 而导致创建失败。而且通过 PVC 请求到一定的存储空间也很有可能不足以满足应用对于存储设备的各种需求。 而且不同的应用程序对于存储性能的要求可能也不尽相同，比如读写速度、并发性能等，为了解决这一问题，Kubernetes 又为我们引入了一个新的资源对象：StorageClass，通过 StorageClass 的定义，管理员可以将存储资源定义为某种类型的资源，比如快速存储、慢速存储等，用户根据 StorageClass 的描述就可以非常直观的知道各种存储资源的具体特性了，这样就可以根据应用的特性去申请合适的存储资源了。 ","date":"2022-12-11","objectID":"/posts/pv_pvc/:3:1","tags":null,"title":"PV 和 PVC","uri":"/posts/pv_pvc/"},{"categories":["kubernetes"],"content":"创建 资源清单如下 kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: local-storage provisioner: Local # 卷插件（如：local NFS） reclaimPolicy: Retain # 回收策略 volumeBindingMode: Immediate # 绑定模式 --- apiVersion: v1 kind: PersistentVolume metadata: name: local-pv spec: capacity: storage: 1Gi volumeMode: Filesystem storageClassName: local-storage accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain local: path: /home/txl/data nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: pv operator: In values: - local --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: ngx-pvc spec: accessModes: - ReadWriteOnce storageClassName: local-storage resources: requests: storage: 1Gi --- apiVersion: apps/v1 kind: Deployment metadata: name: ngx-sc spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: ngx1 image: nginx:1.18-alpine imagePullPolicy: IfNotPresent volumeMounts: - name: mydata mountPath: /data ports: - containerPort: 80 volumes: - name: mydata persistentVolumeClaim: claimName: ngx-pvc ","date":"2022-12-11","objectID":"/posts/pv_pvc/:3:2","tags":null,"title":"PV 和 PVC","uri":"/posts/pv_pvc/"},{"categories":["kubernetes"],"content":"关键配置参数 绑定模式 WaitForFirstConsumer：控制卷绑定和动态制备应该发生在什么时候 Immediate：一旦创建 PVC 就绑定 WaitForFirstConsumer：延迟绑定，直到使用该 PVC 的 Pod 被创建 ","date":"2022-12-11","objectID":"/posts/pv_pvc/:3:3","tags":null,"title":"PV 和 PVC","uri":"/posts/pv_pvc/"},{"categories":["kubernetes"],"content":"挂载到 Pod apiVersion: apps/v1 kind: Deployment metadata: name: ngx-pv spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: ngx1 image: nginx:1.18-alpine imagePullPolicy: IfNotPresent volumeMounts: - name: mydata mountPath: /data ports: - containerPort: 80 volumes: - name: mydata persistentVolumeClaim: claimName: ngx-pvc ","date":"2022-12-11","objectID":"/posts/pv_pvc/:4:0","tags":null,"title":"PV 和 PVC","uri":"/posts/pv_pvc/"},{"categories":["kubernetes"],"content":"在 Kubernetes 中，pod 是应用程序的载体，我们可以通过 pod 的 ip 来访问应用程序，但是 pod 的 ip 地址不是固定的，这也就意味着不方便直接采用 pod 的 ip 对服务进行访问 为了解决这个问题，Kubernetes 提供了 service 资源，service 会对提供同一个服务的多个 pod 进行聚合，并且提供一个统一的入口地址，通过访问 service 的入口地址就能访问到后面的 pod 服务。 通过 service 可以提供负载均衡和服务自动发现 ","date":"2022-12-11","objectID":"/posts/service/:0:0","tags":null,"title":"Service","uri":"/posts/service/"},{"categories":["kubernetes"],"content":"服务类型 ClusterIP：k8s 默认的 ServiceType，通过集群的内部 IP 暴露服务，选择该值时服务只能够在集群内部访问 NodePort：用来对集群外暴露 Service，你可以通过访问集群内的每个 NodeIP:NodePort 的方式，访问到对应 Service 后端的 Endpoint LoadBalancer: 这也是用来对集群外暴露服务的，不同的是这需要外部负载均衡器的云提供商，比如 AWS 等 ExternalName：这个也是在集群内发布服务用的，需要借助 KubeDNS(version \u003e= 1.7) 的支持，就是用KubeDNS 将该 service 和 ExternalName 做一个 Map，KubeDNS 返回一个 CNAME 记录。 每种服务类型都是会指定一个 clusterIP 的，由 clusterIP 进入对应代理模式实现负载均衡，如果强制 spec.clusterIP: \"None\"（即 headless service），集群无法为它们实现负载均衡，直接通过 pod 域名访问pod，典型是应用是 StatefulSet。 ","date":"2022-12-11","objectID":"/posts/service/:1:0","tags":null,"title":"Service","uri":"/posts/service/"},{"categories":["kubernetes"],"content":"Service 使用 ","date":"2022-12-11","objectID":"/posts/service/:2:0","tags":null,"title":"Service","uri":"/posts/service/"},{"categories":["kubernetes"],"content":"创建 创建 deployment 信息，设置 app=nginx 的标签 apiVersion: apps/v1 kind: Deployment metadata: name: myngx spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: ngx image: \"nginx:1.18-alpine\" 创建一个名为 nginx-svc 的 service 对象，它会将请求代理到80端口且具有标签 app: nginx 的 pod 上 apiVersion: v1 kind: Service metadata: name: nginx-svc spec: type: ClusterIP selector: # 通过selector和pod建立关联 app: nginx ports: - port: 80 targetPort: 80 ","date":"2022-12-11","objectID":"/posts/service/:2:1","tags":null,"title":"Service","uri":"/posts/service/"},{"categories":["kubernetes"],"content":"EndPoint Endpoint 是 k8s 中的一个资源对象，存储在 etcd 中，用来记录一个 service 对应的所有 pod 的访问地址，它是根据 service 配置文件中的 selector 描述产生的 一个 service 由一组 pod 组成，这些 pod 通过 endpoints 暴露出来，endpoints 是实现实际服务的端点集合。换句话说，service 和 pod 之间的联系是通过 endpoints 实现的。 $ kubectl get endpoints NAME ENDPOINTS AGE kubernetes 192.168.0.111:6443 12d nginx-svc 10.244.0.181:80,10.244.3.32:80 59m ","date":"2022-12-11","objectID":"/posts/service/:3:0","tags":null,"title":"Service","uri":"/posts/service/"},{"categories":["kubernetes"],"content":"kube-proxy 主要负责 pod 网络代理，维护网络规则和四层负载均衡工作 service 在很多情况下只是一个概念，真正起作用的其实是 kube-proxy 服务进程，每个 node 节点上都运行一个kube-proxy 服务进程，当创建 service 的时候会通过 api-server 向 etcd 写入创建的 service 信息，而 kube-proxy 会基于监听的机制发现这种 service 的变动，然后它会将最新的 service 信息转换成对应的访问规则 kube-proxy 监听 10249 和 10256 端口，对外提供 /metrics 和 /healthy 的访问 # 查看配置 $ kubectl describe cm kube-proxy -n kube-system # 查看 kube-proxy pod $ kubectl get pods -n kube-system | grep kube-proxy kube-proxy-bxk96 1/1 Running 2 11d kube-proxy-xbv75 1/1 Running 4 12d ","date":"2022-12-11","objectID":"/posts/service/:4:0","tags":null,"title":"Service","uri":"/posts/service/"},{"categories":["kubernetes"],"content":"userspace 模式(废弃) ","date":"2022-12-11","objectID":"/posts/service/:4:1","tags":null,"title":"Service","uri":"/posts/service/"},{"categories":["kubernetes"],"content":"iptables 模式(默认模式) iptables 模式下，节点上 kube-proxy 持续监听 Service 以及 Endpoints 对象的变化，为 service 后端的每个 pod 创建对应的 iptables 规则，当捕获到 Service 的 clusterIP 和端口请求，利用注入的 iptables，将请求重定向到 Service 的对应的 Pod ","date":"2022-12-11","objectID":"/posts/service/:4:2","tags":null,"title":"Service","uri":"/posts/service/"},{"categories":["kubernetes"],"content":"IPVS 模式 IPVS 模式是利用 linux 的 IPVS 模块实现，同样是由 kube-proxy 实时监视集群的 service 和 endpoint。基于内核内哈希表，有更高的网络流量吞吐量（iptables 模式在大规模集群，比如10000 个服务中性能下降显著），并且具有更复杂的负载均衡算法（最小连接、局部性、 加权、持久性） 当 kube-proxy 以 IPVS 代理模式启动时，它将验证 IPVS 内核模块是否可用。 如果未检测到 IPVS 内核模块，则 kube-proxy 将退回到以 iptables 代理模式运行。 ","date":"2022-12-11","objectID":"/posts/service/:4:3","tags":null,"title":"Service","uri":"/posts/service/"},{"categories":["kubernetes"],"content":"无头(HeadLiness) 类型的 Service 在某些场景中，开发人员可能不想使用 Service 提供的负载均衡功能，而希望自己来控制负载均衡策略，针对这种情况，k8s 提供了 HeadLiness Service，这类 Service 不会分配 ClusterIP，如果想要访问 Service，只能通过service 的域名进行查询 apiVersion: v1 kind: Service metadata: name: nginx-svc spec: clusterIP: \"None\" # 将clusterIP设置为None，即可创建headliness Service type: ClusterIP selector: app: nginx ports: - port: 80 targetPort: 80 ","date":"2022-12-11","objectID":"/posts/service/:5:0","tags":null,"title":"Service","uri":"/posts/service/"},{"categories":["kubernetes"],"content":"宿主机访问 Service ","date":"2022-12-11","objectID":"/posts/service/:6:0","tags":null,"title":"Service","uri":"/posts/service/"},{"categories":["kubernetes"],"content":"安装 bind-utils $ sudo yum install bind-utils -y # 无法直接查询到service对应的ip $ nslookup nginx-svc ** server can't find nginx-svc: NXDOMAIN ","date":"2022-12-11","objectID":"/posts/service/:6:1","tags":null,"title":"Service","uri":"/posts/service/"},{"categories":["kubernetes"],"content":"设置解析 查看 kube-dns clusterIp $ kubectl get svc -n kube-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kube-dns ClusterIP 10.96.0.10 \u003cnone\u003e 53/UDP,53/TCP,9153/TCP 11d 修改 /etc/resolv.conf 文件，设置DNS服务器IP地址、DNS域名和设置主机的域名搜索顺序。加入内容 nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local 访问 $ curl nginx-svc ","date":"2022-12-11","objectID":"/posts/service/:6:2","tags":null,"title":"Service","uri":"/posts/service/"},{"categories":["kubernetes"],"content":"Secret 是一种包含少量敏感信息例如密码、OAuth 令牌和 SSH 密钥。 将这些信息放在 secret 中比放在 Pod 的定义或者 容器镜像中来说更加安全和灵活。 Kubernetes 提供若干种内置的类型，用于一些常见的使用场景。 针对这些类型，Kubernetes 所执行的合法性检查操作以及对其所实施的限制各不相同。 内置类型 用法 Opaque 用户定义的任意数据 kubernetes.io/service-account-token 服务账号令牌 kubernetes.io/dockercfg ~/.dockercfg 文件的序列化形式 kubernetes.io/dockerconfigjson ~/.docker/config.json 文件的序列化形式 kubernetes.io/basic-auth 用于基本身份认证的凭据 kubernetes.io/ssh-auth 用于 SSH 身份认证的凭据 kubernetes.io/tls 用于 TLS 客户端或者服务器端的数据 bootstrap.kubernetes.io/token 启动引导令牌数据 ","date":"2022-12-10","objectID":"/posts/secret/:0:0","tags":null,"title":"Secret","uri":"/posts/secret/"},{"categories":["kubernetes"],"content":"基本用法 ","date":"2022-12-10","objectID":"/posts/secret/:1:0","tags":null,"title":"Secret","uri":"/posts/secret/"},{"categories":["kubernetes"],"content":"创建 首先将字符串转换为 base64 $ echo -n 'admin' | base64 $ echo -n '1f2d1e2e67df' | base64 apiVersion: v1 kind: Secret metadata: name: mysecret type: Opaque data: username: YWRtaW4= password: MWYyZDFlMmU2N2Rm 如果希望使用非 base64 编码的字符串直接放入 Secret 中，应当使用 stringData 字段 stringData: config.yaml: | apiUrl: \"https://my.api.com/api/v1\" username: \"admin\" password: \"1f2d1e2e67df\" ","date":"2022-12-10","objectID":"/posts/secret/:1:1","tags":null,"title":"Secret","uri":"/posts/secret/"},{"categories":["kubernetes"],"content":"解码 输出 yaml 内容 $ kubectl get secret mysecret -o yaml $ echo -n 'YWRtaW4=' | base64 -d 使用 JSONPath 模板输出特定字段，文档：JSONPath 支持 | Kubernetes $ kubectl get secret mysecret -o jsonpath={.data.username} | base64 -d ","date":"2022-12-10","objectID":"/posts/secret/:1:2","tags":null,"title":"Secret","uri":"/posts/secret/"},{"categories":["kubernetes"],"content":"以环境变量的方式使用 apiVersion: apps/v1 kind: Deployment metadata: name: myngx spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: ngx image: nginx:1.18-alpine imagePullPolicy: IfNotPresent env: - name: SECRET_USERNAME valueFrom: secretKeyRef: name: mysecret key: username ","date":"2022-12-10","objectID":"/posts/secret/:1:3","tags":null,"title":"Secret","uri":"/posts/secret/"},{"categories":["kubernetes"],"content":"挂载到文件 apiVersion: apps/v1 kind: Deployment metadata: name: myngx spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: ngx image: nginx:1.18-alpine imagePullPolicy: IfNotPresent volumeMounts: - name: foo mountPath: /etc/foo readOnly: true volumes: - name: foo secret: secretName: mysecret items: - key: username path: my-username 如果省略 items 节点，会映射 secret 所有的 key ","date":"2022-12-10","objectID":"/posts/secret/:1:4","tags":null,"title":"Secret","uri":"/posts/secret/"},{"categories":["kubernetes"],"content":"手工配置 basic-auth 认证 ","date":"2022-12-10","objectID":"/posts/secret/:2:0","tags":null,"title":"Secret","uri":"/posts/secret/"},{"categories":["kubernetes"],"content":"生成密码文件 可以使用 htpasswd 或者 openssl passwd 命令生成 安装 $ sudo yum -y install httpd-tools 生成认证密码 创建一个用户名和密码的认证到auth文件中 $ htpasswd -c auth txl ","date":"2022-12-10","objectID":"/posts/secret/:2:1","tags":null,"title":"Secret","uri":"/posts/secret/"},{"categories":["kubernetes"],"content":"导入 Secret $ kubectl create secret generic basic-auth --from-file=auth ","date":"2022-12-10","objectID":"/posts/secret/:2:2","tags":null,"title":"Secret","uri":"/posts/secret/"},{"categories":["kubernetes"],"content":"创建 nginx Configmap apiVersion: v1 kind: ConfigMap metadata: name: ngx data: default: | server { listen 80; server_name localhost; location / { auth_basic \"test auth\"; auth_basic_user_file /etc/nginx/basicauth; # 指向生成的密码文件 root /usr/share/nginx/html; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } } ","date":"2022-12-10","objectID":"/posts/secret/:2:3","tags":null,"title":"Secret","uri":"/posts/secret/"},{"categories":["kubernetes"],"content":"创建 Deployment apiVersion: apps/v1 kind: Deployment metadata: name: ngx spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: ngx image: nginx:1.18-alpine imagePullPolicy: IfNotPresent volumeMounts: - name: nginx-default mountPath: /etc/nginx/conf.d/default.conf # 覆盖nginx默认配置 subPath: default - name: basic-auth mountPath: /etc/nginx/basicauth subPath: auth volumes: - name: nginx-default configMap: name: ngx defaultMode: 0655 - name: basic-auth secret: secretName: basic-auth defaultMode: 0655 ","date":"2022-12-10","objectID":"/posts/secret/:2:4","tags":null,"title":"Secret","uri":"/posts/secret/"},{"categories":["kubernetes"],"content":"访问 $ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES ngx-5597699bdf-ntbjx 1/1 Running 0 31m 10.244.3.29 lain2 \u003cnone\u003e \u003cnone\u003e $ curl --basic -u txl:123 http://10.244.3.29 ","date":"2022-12-10","objectID":"/posts/secret/:2:5","tags":null,"title":"Secret","uri":"/posts/secret/"},{"categories":["kubernetes"],"content":"从私有仓库拉取镜像 使用 Docker Hub 镜像仓库 ","date":"2022-12-10","objectID":"/posts/secret/:3:0","tags":null,"title":"Secret","uri":"/posts/secret/"},{"categories":["kubernetes"],"content":"登录 Docker 镜像仓库 $ docker login --username=\u003c用户名\u003e # 发布 $ docker push \u003c镜像名\u003e ","date":"2022-12-10","objectID":"/posts/secret/:3:1","tags":null,"title":"Secret","uri":"/posts/secret/"},{"categories":["kubernetes"],"content":"创建 DockerHub Secret 创建一个名为 docker-regcred 的 docker registry secret $ kubectl create secret docker-registry docker-regcred \\ --docker-server=https://index.docker.io/v1/ \\ --docker-username=\u003c用户名\u003e \\ --docker-password=\u003c密码\u003e \\ --docker-email=\u003c邮箱地址\u003e 解码 $ kubectl get secret docker-registry -o jsonpath={.data.*} | base64 -d ","date":"2022-12-10","objectID":"/posts/secret/:3:2","tags":null,"title":"Secret","uri":"/posts/secret/"},{"categories":["kubernetes"],"content":"使用 apiVersion: apps/v1 kind: Deployment metadata: name: myalpine spec: selector: matchLabels: app: myalpine replicas: 1 template: metadata: labels: app: myalpine spec: imagePullSecrets: - name: docker-regcred containers: - name: alpine image: lains3/alpine:3.12 imagePullPolicy: IfNotPresent command: [\"sh\",\"-c\",\"echo this is alpine \u0026\u0026 sleep 36000\"] ","date":"2022-12-10","objectID":"/posts/secret/:3:3","tags":null,"title":"Secret","uri":"/posts/secret/"},{"categories":["kubernetes"],"content":"ConfigMap 是一种 API 对象，用来将非机密性的数据保存到键值对中。使用时， Pod 可以将其用作环境变量、命令行参数或者存储卷中的配置文件。 ConfigMap 将你的环境配置信息和容器镜像解耦，便于应用配置的修改。 使用场景： 容器 entrypoint 的命令行参数 容器的环境变量 映射成文件 编写代码在 Pod 中运行，使用 Kubernetes API 来读取 ConfigMap ","date":"2022-12-09","objectID":"/posts/configmap/:0:0","tags":null,"title":"ConfigMap","uri":"/posts/configmap/"},{"categories":["kubernetes"],"content":"使用 apiVersion: v1 kind: ConfigMap metadata: name: mycm data: host: \"0.0.0.0\" port: \"9999\" user.properties: | user.name=txl user.age=18 查看 $ kubectl get cm -n default NAME DATA AGE mycm 3 45m ","date":"2022-12-09","objectID":"/posts/configmap/:1:0","tags":null,"title":"ConfigMap","uri":"/posts/configmap/"},{"categories":["kubernetes"],"content":"在环境变量中使用 apiVersion: apps/v1 kind: Deployment metadata: name: myngx spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: ngx image: nginx:1.18-alpine imagePullPolicy: IfNotPresent env: - name: HOST valueFrom: configMapKeyRef: name: mycm # ConfigMap 名称 key: host # 需要取值的键 ","date":"2022-12-09","objectID":"/posts/configmap/:1:1","tags":null,"title":"ConfigMap","uri":"/posts/configmap/"},{"categories":["kubernetes"],"content":"映射成文件 apiVersion: apps/v1 kind: Deployment metadata: name: myngx spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: ngx image: nginx:1.18-alpine imagePullPolicy: IfNotPresent volumeMounts: - name: cmconfig mountPath: \"/config\" readOnly: true volumes: - name: cmconfig configMap: name: mycm # ConfigMap 名称 items: # 来自 ConfigMap 的一组键，将被创建为文件 - key: \"user.properties\" path: \"userinfo.conf\" 如果省略 items 节点，会映射 ConfigMap 全部的 key $ cd /config \u0026\u0026 ls -l total 0 lrwxrwxrwx 1 root root 11 Dec 6 14:00 host -\u003e ..data/host lrwxrwxrwx 1 root root 11 Dec 6 14:00 port -\u003e ..data/port lrwxrwxrwx 1 root root 22 Dec 6 14:00 user.properties -\u003e ..data/user.properties 使用 subPath 可用于指定所引用的卷内的子路径，而不是其根路径。 apiVersion: apps/v1 kind: Deployment metadata: name: myngx spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: ngx image: nginx:1.18-alpine imagePullPolicy: IfNotPresent env: - name: HOST valueFrom: configMapKeyRef: name: mycm key: host volumeMounts: - name: cmconfig mountPath: /config/userinfo.conf subPath: user.properties volumes: - name: cmconfig configMap: defaultMode: 0655 name: mycm ","date":"2022-12-09","objectID":"/posts/configmap/:1:2","tags":null,"title":"ConfigMap","uri":"/posts/configmap/"},{"categories":["kubernetes"],"content":"使用 client-go 调用 GitHub: kubernetes/client-go: Go client for Kubernetes ","date":"2022-12-09","objectID":"/posts/configmap/:2:0","tags":null,"title":"ConfigMap","uri":"/posts/configmap/"},{"categories":["kubernetes"],"content":"集群外调用 使用 api 代理 $ kubectl proxy --address='0.0.0.0' --accept-hosts='^*$' --port=8009 获取 ConfigMap func getClient() *kubernetes.Clientset { config := \u0026rest.Config{ Host: \"http://ip:8009\", } c, err := kubernetes.NewForConfig(config) if err != nil { log.Fatalln(err) } return c } func main() { client := getClient() cm, err := client.CoreV1().ConfigMaps(\"default\").Get(context.Background(), \"mycm\", v1.GetOptions{}) if err != nil { log.Fatalln(err) } fmt.Println(cm.Data) } 执行结果 map[host:0.0.0.0 port:9999 user.properties:user.name=txl user.age=18 ] ","date":"2022-12-09","objectID":"/posts/configmap/:2:1","tags":null,"title":"ConfigMap","uri":"/posts/configmap/"},{"categories":["kubernetes"],"content":"集群内调用 创建 ServiceAccouont 拥有 default 空间内对 ConfigMap 的查看权限 apiVersion: v1 kind: ServiceAccount metadata: name: sa-cm --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: clusterrole-cm rules: - apiGroups: [\"\"] resources: [\"configmaps\"] verbs: [\"get\", \"watch\", \"list\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: clusterrolebinding-cm namespace: default roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: clusterrole-cm subjects: - kind: ServiceAccount name: sa-cm namespace: default 调用 API token 路径：/var/run/secrets/kubernetes.io/serviceaccount/token api server 地址：https://$KUBERNETES_SERVICE_HOST:$KUBERNETES_PORT_443_TCP_PORT 证书路径：/var/run/secrets/kubernetes.io/serviceaccount/ca.crt var apiServer string var token string func init() { apiServer = fmt.Sprintf(\"https://%s:%s\", os.Getenv(\"KUBERNETES_SERVICE_HOST\"), os.Getenv(\"KUBERNETES_PORT_443_TCP_PORT\")) f, err := os.Open(\"/var/run/secrets/kubernetes.io/serviceaccount/token\") if err != nil { log.Fatal(err) } b, _ := io.ReadAll(f) token = string(b) } func getClient() *kubernetes.Clientset { config := \u0026rest.Config{ Host: apiServer, BearerToken: token, TLSClientConfig: rest.TLSClientConfig{CAFile: \"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\"}, } c, err := kubernetes.NewForConfig(config) if err != nil { log.Fatalln(err) } return c } func main() { client := getClient() cm, err := client.CoreV1().ConfigMaps(\"default\").Get(context.Background(), \"mycm\", v1.GetOptions{}) if err != nil { log.Fatalln(err) } fmt.Println(cm.Data) select {} } 交叉编译 set GOOS=linux set GOARCH=amd64 go build -o cmtest main.go 创建 Deployment apiVersion: apps/v1 kind: Deployment metadata: name: cmtest spec: selector: matchLabels: app: cmtest replicas: 1 template: metadata: labels: app: cmtest spec: serviceAccount: sa-cm # 指定 ServiceAccount nodeName: lain1 # 指定 node containers: - name: cmtest image: alpine:3.12 imagePullPolicy: IfNotPresent command: [\"/app/cmtest\"] volumeMounts: - name: app mountPath: /app volumes: - name: app hostPath: path: /home/txl/goapi type: Directory 查看 $ kubectl get pods NAME READY STATUS RESTARTS AGE cmtest-96b96c458-szxhk 1/1 Running 0 2m46s $ kubectl logs cmtest-96b96c458-szxhk map[host:0.0.0.0 port:9999 user.properties:user.name=txl user.age=18 ] ","date":"2022-12-09","objectID":"/posts/configmap/:2:2","tags":null,"title":"ConfigMap","uri":"/posts/configmap/"},{"categories":["kubernetes"],"content":"监控 ConfigMap 变化 type CmHandler struct{} func(this *CmHandler) OnAdd(obj interface{}){} func(this *CmHandler) OnUpdate(oldObj, newObj interface{}){ if newObj.(*v1.ConfigMap).Name==\"mycm\"{ log.Println(\"mycm发生了变化\") } } func(this *CmHandler) OnDelete(obj interface{}){} func main() { fact:=informers.NewSharedInformerFactory(getClient(), 0) cmInformer:=fact.Core().V1().ConfigMaps() cmInformer.Informer().AddEventHandler(\u0026CmHandler{}) fact.Start(wait.NeverStop) select {} } ","date":"2022-12-09","objectID":"/posts/configmap/:2:3","tags":null,"title":"ConfigMap","uri":"/posts/configmap/"},{"categories":["kubernetes"],"content":"文档：Pod | Kubernetes Pod 是可以在 Kubernetes 中创建和管理的、最小的可部署的计算单元。它是一个或多个容器的组合。这些容器共享存储、网络和命名空间，以及如何运行的规范。其它的资源对象都是用来支撑或者扩展 Pod 对象功能的，比如控制器对象是用来管控 Pod 对象的，Service 或者 Ingress 资源对象是用来暴露 Pod 引用对象的，PersistentVolume 资源对象是用来为 Pod 提供存储等等，K8S 不会直接处理容器，而是 Pod，Pod 是由一个或多个 container 组成。基本的好处有： 方便部署、扩展和收缩、方便调度等 Pod中的容器共享数据和网络空间，统一的资源管理与分配 在Pod中，所有容器都被同一安排和调度，并运行在共享的上下文中。对于具体应用而言，Pod是它们的逻辑主机，Pod包含业务相关的多个应用容器。 每一个 Pod 都有一个特殊的被称为 “根容器” 的 Pause 容器。Pause 容器对应的镜像属于 Kubernetes 平台的一部分，除了 Pause 容器，每个 Pod 还包含一个或多个紧密相关的用户业务容器。Pause 容器的作用： 扮演 Pid=1 的，回收僵尸进程 基于 Linux 的 namespace 的共享 ","date":"2022-12-04","objectID":"/posts/pod/:0:0","tags":null,"title":"Pod、ReplicaSet 和 Deployment","uri":"/posts/pod/"},{"categories":["kubernetes"],"content":"Pod 基本使用 Pod 通常不是直接创建的，而是使用工作负载资源创建的 ","date":"2022-12-04","objectID":"/posts/pod/:1:0","tags":null,"title":"Pod、ReplicaSet 和 Deployment","uri":"/posts/pod/"},{"categories":["kubernetes"],"content":"创建 apiVersion: v1 kind: Pod metadata: name: myngx spec: containers: - name: ngx image: \"nginx:1.18-alpine\" 展示详细信息 $ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES myngx 1/1 Running 0 13m 10.244.3.7 lain2 \u003cnone\u003e \u003cnone\u003e ","date":"2022-12-04","objectID":"/posts/pod/:1:1","tags":null,"title":"Pod、ReplicaSet 和 Deployment","uri":"/posts/pod/"},{"categories":["kubernetes"],"content":"ReplicaSet ReplicaSet 是为了保持维护的期待 Pod 副本数量与现时 Pod 副本数量一致。如在由于 Pod 异常退出导致期待的副本数量不足时，会自动创建新的 Pod 保证到与期望的 Pod 副本数量一致 ","date":"2022-12-04","objectID":"/posts/pod/:2:0","tags":null,"title":"Pod、ReplicaSet 和 Deployment","uri":"/posts/pod/"},{"categories":["kubernetes"],"content":"使用 Deployment Deployment 运行一组相同的 Pod（副本水平扩展）、滚动更新。通过副本集管理和创建POD。我们往往不会直接在集群中使用 ReplicaSet 部署一个新的微服务，一方面是因为 ReplicaSet 的功能其实不够强大，一些常见的更新、扩容和缩容运维操作都不支持，Deployment 的引入就是为了就是为了支持这些复杂的操作 ","date":"2022-12-04","objectID":"/posts/pod/:3:0","tags":null,"title":"Pod、ReplicaSet 和 Deployment","uri":"/posts/pod/"},{"categories":["kubernetes"],"content":"挂载 挂载 hostPath 主机目录卷实例 apiVersion: apps/v1 kind: Deployment metadata: name: myngx spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: ngx image: \"nginx:1.18-alpine\" volumeMounts: # 声明容器中的挂载位置 - name: mydata mountPath: /data - name: alpine # 测试多容器 command: [\"sh\",\"-c\",\"echo this is alpine \u0026\u0026 sleep 3600\"] image: \"alpine:3.12\" volumes: - name: mydata hostPath: path: /home/txl/yaml/data # 声明主机节点目录 type: Directory # 指定type hostPath 支持的 type 值如下： ","date":"2022-12-04","objectID":"/posts/pod/:3:1","tags":null,"title":"Pod、ReplicaSet 和 Deployment","uri":"/posts/pod/"},{"categories":["kubernetes"],"content":"共享文件夹 同一个 pod 内的容器都能读写 EmptyDir 中的文件。常用于临时空间、多容器共享，如日志或者tmp文件需要的临时目录 apiVersion: apps/v1 kind: Deployment metadata: name: myngx spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: ngx image: nginx:1.18-alpine imagePullPolicy: IfNotPresent volumeMounts: - name: sharedata mountPath: /data - name: alpine image: alpine:3.12 imagePullPolicy: IfNotPresent command: [\"sh\",\"-c\",\"echo this is alpine \u0026\u0026 sleep 36000\"] volumeMounts: - name: sharedata mountPath: /data volumes: - name: sharedata emptyDir: {} ","date":"2022-12-04","objectID":"/posts/pod/:3:2","tags":null,"title":"Pod、ReplicaSet 和 Deployment","uri":"/posts/pod/"},{"categories":["kubernetes"],"content":"Init 容器 文档：Init 容器 | Kubernetes Init 容器是一种特殊容器，在 Pod 内的应用容器启动之前运行。Init 容器可以包括一些应用镜像中不存在的实用工具和安装脚本。 Init 容器与普通的容器非常像，除了如下两点： 它们总是运行到完成。 每个都必须在下一个启动之前成功完成。 如果 Pod 的 Init 容器失败，kubelet 会不断地重启该 Init 容器直到该容器成功为止。 然而，如果 Pod 对应的 restartPolicy 值为 “Never”，Kubernetes 不会重新启动 Pod。 原理 在 Pod 启动过程中，每个 Init 容器会在网络和数据卷初始化之后按顺序启动。 依据 Init 容器在 Pod spec 配置中的出现顺序依次运行。由于 Pod 可能各种原因多次重启，所以 Init 容器中的操作，须具备幂等性。 应用场景 环境检查：例如确保应用容器依赖的服务启动后再启动应用容器 初始化配置：例如给应用容器准备配置文件 基本配置 apiVersion: apps/v1 kind: Deployment metadata: name: myngx spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: initContainers: - name: init-mydb image: alpine:3.12 command: ['sh', '-c', 'echo wait for db \u0026\u0026 sleep 35 \u0026\u0026 echo done'] # 模拟等待35s containers: - name: ngx image: nginx:1.18-alpine imagePullPolicy: IfNotPresent volumeMounts: - name: sharedata mountPath: /data - name: alpine image: alpine:3.12 imagePullPolicy: IfNotPresent command: [\"sh\",\"-c\",\"echo this is alpine \u0026\u0026 sleep 36000\"] volumeMounts: - name: sharedata mountPath: /data volumes: - name: sharedata emptyDir: {} 查看 init 容器状态 $ kubectl get pod NAME READY STATUS RESTARTS AGE myngx-89dd8586b-k59pl 0/2 Init:0/1 0 8s 状态 含义 Init:N/M Pod 包含 M 个 Init 容器，其中 N 个已经运行完成。 Init:Error Init 容器已执行失败。 Init:CrashLoopBackOff Init 容器执行总是失败。 Pending Pod 还没有开始执行 Init 容器。 PodInitializing or Running Pod 已经完成执行 Init 容器。 ","date":"2022-12-04","objectID":"/posts/pod/:3:3","tags":null,"title":"Pod、ReplicaSet 和 Deployment","uri":"/posts/pod/"},{"categories":["kubernetes"],"content":"文档：使用 RBAC 鉴权 | Kubernetes ","date":"2022-12-03","objectID":"/posts/rbac/:0:0","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"用户 UserAccount（普通用户）：集群外部访问时使用的用户账号，最常见的就是 kubectl 命令就是作为 kubernetes-admin 用户来执行，k8s本身不记录这些账号 ServiceAccount（服务账户）：它们被绑定到特定的名字空间，服务账号与一组以 Secret 保存的凭据相关，这些凭据会被挂载到 Pod 中，从而允许集群内的进程访问 Kubernetes API ","date":"2022-12-03","objectID":"/posts/rbac/:1:0","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"用户认证 ","date":"2022-12-03","objectID":"/posts/rbac/:2:0","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"A. 使用 X509 客户证书 生成证书 安装 OpenSSL $ sudo yum install openssl openssl-devel 生成一个名称为txl的普通用户的客户端证书 $ mkdir ua/txl $ cd ua/txl # 生成客户端私钥 $ openssl genrsa -out client.key 2048 # 根据私钥生成csr, 指定用户名txl $ openssl req -new -key client.key -out client.csr -subj \"/CN=txl\" # 根据k8s的CA证书生成客户端证书 $ sudo openssl x509 -req -in client.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out client.crt -days 365 证书反解 获取证书设置的CN(Common name) $ openssl x509 -noout -subject -in client.crt 使用证书初步请求API $ kubectl get endpoints NAME ENDPOINTS AGE kubernetes 192.168.0.111:6443 4d5h $ curl --cert ./client.crt --key ./client.key --cacert /etc/kubernetes/pki/ca.crt -s https://192.168.0.111:6443/api { \"kind\": \"APIVersions\", \"versions\": [ \"v1\" ], \"serverAddressByClientCIDRs\": [ { \"clientCIDR\": \"0.0.0.0/0\", \"serverAddress\": \"192.168.0.111:6443\" } ] } 可以使用 --insecure 代替 --cacert /etc/kubernetes/pki/ca.crt 忽略服务端证书验证 证书加入 kube config 把 client.crt 加入到 ~/.kube/config $ kubectl config --kubeconfig=/home/txl/.kube/config set-credentials txl --client-certificate=/home/txl/ua/txl/client.crt --client-key=/home/txl/ua/txl/client.key 创建一个名为 user_context 的 context $ kubectl config --kubeconfig=/home/txl/.kube/config set-context user_context --cluster=kubernetes --user=txl 切换当前上下文为 user_context $ kubectl config use-context user_context # 查看 $ kubectl config current-context # 重新切回默认管理员 $ kubectl config use-context kubernetes-admin@kubernetes ","date":"2022-12-03","objectID":"/posts/rbac/:2:1","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"B. 使用静态令牌文件(Token) token 和证书只能配一个 生成 Token $ head -c 16 /dev/urandom | od -An -t x | tr -d ' ' 加入 kube config $ kubectl config set-credentials txl --token=fdb72d94a1c2c2cfbf82341d1f98c68c 修改 api-server 启动参数 $ sudo vi /etc/kubernetes/pki/token_auth # 加入 fdb72d94a1c2c2cfbf82341d1f98c68c,txl,1001 $ sudo vi /etc/kubernetes/manifests/kube-apiserver.yaml # 加入 --token-auth-file=/etc/kubernetes/pki/token_auth 查看 $ curl -H \"Authorization: Bearer fdb72d94a1c2c2cfbf82341d1f98c68c\" https://192.168.0.111:6443/api/v1/namespaces/default/pods --insecure { \"kind\": \"PodList\", \"apiVersion\": \"v1\", \"metadata\": { \"selfLink\": \"/api/v1/namespaces/default/pods\", \"resourceVersion\": \"1586027\" }, \"items\": [] } ","date":"2022-12-03","objectID":"/posts/rbac/:2:2","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"资源 查看所有资源 $ kubectl api-resources -o wide 其中 VERBS 列展示了该资源对应的操作，比如 role create 创建 delete 删除 deletecollection 批量删除 get 获取 list 列表 patch 合并变更 update 更新 watch 监听 ","date":"2022-12-03","objectID":"/posts/rbac/:3:0","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"Role 和 RoleBinding Role（角色）：包含一组代表相关权限的规则，用于授予对单个命名空间的资源访问 RoleBinding（角色绑定）：将角色中定义的权限赋予一个或者一组用户 ","date":"2022-12-03","objectID":"/posts/rbac/:4:0","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"创建 Role 下面是一个位于default的role，拥有对pod的读访问权限 $ vi role_mypod.yaml kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: namespace: default name: mypod rules: - apiGroups: [\"*\"] resources: [\"pods\"] verbs: [\"get\", \"watch\", \"list\"] 执行创建 $ kubectl apply -f role_mypod.yaml # 查看default空间所有role $ kubectl get role -n default 删除 $ kubectl delete role mypod -n default ","date":"2022-12-03","objectID":"/posts/rbac/:4:1","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"绑定 RoleBinding 创建一个名为mypodbinding的 rolebinding，关联创建的用户txl和创建的角色mypod 1. 使用命令 $ kubectl create rolebinding mypodbinding -n default --role mypod --user txl 2. 使用 yaml apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: creationTimestamp: null name: mypodrolebinding roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: mypod subjects: - apiGroup: rbac.authorization.k8s.io kind: User name: txl - kind: ServiceAccount name: txl 查看 $ kubectl get rolebinding -n default NAME ROLE mypodrolebinding Role/mypod 删除 $ kubectl delete rolebinding mypodbinding -n default ","date":"2022-12-03","objectID":"/posts/rbac/:4:2","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"ClusterRole 和 ClusterRoleBinding ClusterRole 同样可以用于授予 Role 能够授予的权限。 因为 ClusterRole 属于集群范围，所以它也可以为以下资源授予访问权限： 集群范围资源（如节点 Node） 非资源端点（如 /healthz） 跨名字空间访问的名字空间作用域的资源（如 Pod） clusterRole不限定命名空间，绑定既可以使用 RoleBinding，也可以使用 ClusterRoleBinding ","date":"2022-12-03","objectID":"/posts/rbac/:5:0","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"创建 ClusterRole kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: mypod-cluster rules: - apiGroups: [\"*\"] resources: [\"pods\"] verbs: [\"get\", \"watch\", \"list\"] 查看 $ kubectl get clusterrole ","date":"2022-12-03","objectID":"/posts/rbac/:5:1","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"绑定 RoleBinding 需要指定命名空间 apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: mypodrolebinding-cluster namespace: kube-system roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: mypod-cluster subjects: - apiGroup: rbac.authorization.k8s.io kind: User name: txl 查看 $ kubectl get rolebinding -n kube-system NAME ROLE mypodrolebinding-cluster ClusterRole/mypod-cluster ","date":"2022-12-03","objectID":"/posts/rbac/:5:2","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"绑定 ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: mypod-clusterrolebinding roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: mypod-cluster subjects: - apiGroup: rbac.authorization.k8s.io kind: User name: txl - kind: ServiceAccount name: txl namespace: default ","date":"2022-12-03","objectID":"/posts/rbac/:5:3","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"ServiceAccount UserAccount 是可以跨 namespace 的，而 ServiceAccount 只能局限在自己所属的 namespace 中，每个 namespace 都会有一个默认的 default 账号 。 ","date":"2022-12-03","objectID":"/posts/rbac/:6:0","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"创建 SA $ kubectl create sa mysa 也可以导出到 yaml kubectl create sa mysa -o yaml --dry-run=client \u003e mysa.yaml 查看 $ kubectl get sa -n default NAME SECRETS AGE default 1 4d7h mysa 1 5m21s # 查看令牌 $ kubectl describe sa mysa Name: mysa Namespace: default Labels: \u003cnone\u003e Annotations: \u003cnone\u003e Image pull secrets: \u003cnone\u003e Mountable secrets: mysa-token-5bnbm Tokens: mysa-token-5bnbm Events: \u003cnone\u003e $ kubectl describe secret mysa-token-5bnbm ","date":"2022-12-03","objectID":"/posts/rbac/:6:1","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"绑定 ClusterRoleBinding 使用命令的方式 $ kubectl create clusterrolebinding mysa-clusterrolebinding --clusterrole=mypod-cluster --serviceaccount=default:mysa 查看 $ kubectl get clusterrolebinding NAME ROLE mypod-clusterrolebinding ClusterRole/mypod-cluster mysa-clusterrolebinding ClusterRole/mypod-cluster ","date":"2022-12-03","objectID":"/posts/rbac/:6:2","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"外部访问 API 安装 jq 轻量级的 json 处理命令。可以对 json 数据进行分片、过滤、映射、转换和格式化输出 $ sudo yum install jq -y 获取 SA Token 保存到临时变量 mysatoken 中 $ mysatoken=$(kubectl get secret $(kubectl get sa mysa -o json | jq -Mr '.secrets[0].name') -o json | jq -Mr '.data.token' | base64 -d) # 查看token $ echo $mysatoken 请求 $ curl -H \"Authorization: Bearer $mysatoken\" --insecure https://192.168.0.111:6443/api/v1/namespaces/default/pods ","date":"2022-12-03","objectID":"/posts/rbac/:6:3","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"在 Pod 里访问 API 创建一个测试 pod apiVersion: apps/v1 kind: Deployment metadata: name: myngx spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: serviceAccountName: mysa # 指定SA，否则使用的default SA containers: - name: nginxtest image: nginx:1.18-alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80 查看 pod $ kubectl get pod NAME READY STATUS RESTARTS AGE myngx-74748c5956-5rfrs 1/1 Running 0 4m21s 进入容器 $ kubectl exec -it myngx-74748c5956-5rfrs -- sh 设置临时变量 # SA token $ TOKEN=`cat /var/run/secrets/kubernetes.io/serviceaccount/token` # api server地址 $ APISERVER=\"https://$KUBERNETES_SERVICE_HOST:$KUBERNETES_PORT_443_TCP_PORT\" 请求（跳过服务器证书检查） $ curl --header \"Authorization: Bearer $TOKEN\" --insecure -s $APISERVER/api/v1/namespaces/default/pods 使用证书请求 $ curl --header \"Authorization: Bearer $TOKEN\" --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt $APISERVER/api/v1/namespaces/default/pods ","date":"2022-12-03","objectID":"/posts/rbac/:6:4","tags":null,"title":"K8s 认证和授权","uri":"/posts/rbac/"},{"categories":["kubernetes"],"content":"kubeadm是官方社区推出的一个用于快速部署kubernetes集群的工具 ","date":"2022-12-03","objectID":"/posts/kubeadm/:0:0","tags":null,"title":"使用 kubeadm 部署 k8s","uri":"/posts/kubeadm/"},{"categories":["kubernetes"],"content":"部署 文档地址：安装 kubeadm | Kubernetes ","date":"2022-12-03","objectID":"/posts/kubeadm/:1:0","tags":null,"title":"使用 kubeadm 部署 k8s","uri":"/posts/kubeadm/"},{"categories":["kubernetes"],"content":"添加 kubenetes 的 yum 源 在每个节点上分别执行 $ su - $ cat \u003c\u003cEOF \u003e /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF $ yum makecache ","date":"2022-12-03","objectID":"/posts/kubeadm/:1:1","tags":null,"title":"使用 kubeadm 部署 k8s","uri":"/posts/kubeadm/"},{"categories":["kubernetes"],"content":"安装 kubeadm、kubelet 和 kubectl 在每个节点上分别执行 $ sudo yum install -y kubelet-1.21.0 kubeadm-1.21.0 kubectl-1.21.0 安装后查看列表 $ rpm -aq kubelet kubectl kubeadm 把kubelet设置为开机启动 $ sudo systemctl enable kubelet kubeadm init 集群的快速初始化，部署Master节点的各个组件 kubeadm join 节点加入到指定集群中 kubeadm token 管理用于加入集群时使用的认证令牌 (如list，create) kubeadm reset 重置集群，如删除构建文件以回到初始状态 ","date":"2022-12-03","objectID":"/posts/kubeadm/:1:2","tags":null,"title":"使用 kubeadm 部署 k8s","uri":"/posts/kubeadm/"},{"categories":["kubernetes"],"content":"使用 systemd 作为 docker 的 cgroup driver 在每个节点上执行 $ sudo vi /etc/docker/daemon.json 加入内容 { \"exec-opts\": [\"native.cgroupdriver=systemd\"] } 重启docker $ systemctl daemon-reload \u0026\u0026 systemctl restart docker 验证结果 $ docker info |grep Cgroup Cgroup Driver: systemd Cgroup Version: 1 ","date":"2022-12-03","objectID":"/posts/kubeadm/:1:3","tags":null,"title":"使用 kubeadm 部署 k8s","uri":"/posts/kubeadm/"},{"categories":["kubernetes"],"content":"关闭 swap 临时关闭 $ swapoff -a 永久关闭 $ sudo vi /etc/fstab # 注释掉SWAP分区项 # swap was on /dev/sda11 during installation # UUID=0xxxxxxxxxxxxxx4f69 none swap sw 0 0 ","date":"2022-12-03","objectID":"/posts/kubeadm/:1:4","tags":null,"title":"使用 kubeadm 部署 k8s","uri":"/posts/kubeadm/"},{"categories":["kubernetes"],"content":"初始化集群 $ sudo kubeadm init --kubernetes-version=v1.21.0 --image-repository registry.aliyuncs.com/google_containers --pod-network-cidr=10.244.0.0/16 根据输出提示操作 $ mkdir -p $HOME/.kube $ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config $ sudo chown $(id -u):$(id -g) $HOME/.kube/config 默认token的有效期为24小时，当过期之后，该token就不可用了。 查看token列表： $ sudo kubeadm token list 重新生成token： $ sudo kubeadm token create --print-join-command ","date":"2022-12-03","objectID":"/posts/kubeadm/:1:5","tags":null,"title":"使用 kubeadm 部署 k8s","uri":"/posts/kubeadm/"},{"categories":["kubernetes"],"content":"安装网络组件 CNI (Container Network Interface) 容器网络接口，为了让用户在容器创建或销毁时都能够更容易地配置容器网络。常见的组件有： Flannel: 最基本的网络组件 Calico: 支持网络策略 Canal: 前两者的合体 Weave: 同样支持策略机制，还支持加密 使用 flannel Github地址：https://github.com/coreos/flannel 在每个节点执行 $ sudo sysctl net.bridge.bridge-nf-call-iptables=1 在master节点执行 $ kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml # 去污点 $ kubectl taint nodes --all node-role.kubernetes.io/master- 查看 kubectl get pods --all-namespaces 可能出现的错误 基本排查命令 $ kubectl describe $ journalctl -f -u kubelet $ for p in $(kubectl get pods --namespace=kube-system -l k8s-app=kube-dns -o name); do kubectl logs --namespace=kube-system $p; done coredns $ kubectl describe pod -n kube-system coredns-xxx network: open /run/flannel/subnet.env: no such file or directory 手动在每个节点上创建 $ sudo vi /run/flannel/subnet.env # 加入内容 FLANNEL_NETWORK=10.244.0.0/16 FLANNEL_SUBNET=10.244.0.1/24 FLANNEL_MTU=1450 FLANNEL_IPMASQ=true “cni0” already has an IP address different from 10.244.1.1/24 $ sudo ip link delete cni0 # 重启pod $ kubectl delete pod xxx Readiness probe failed: HTTP probe failed with statuscode: 503 $ systemctl stop kubelet $ systemctl stop docker $ iptables --flush $ iptables -tnat --flush $ systemctl start kubelet $ systemctl start docker 节点 not ready $ kubectl describe node lain2 failed to find plugin “xxx” in path [/opt/cni/bin] 把master节点的 /opt/cni/bin 拷贝过来，在master节点执行 $ cd /opt/cni $ sudo scp -r bin root@192.168.0.105:/opt/cni/bin 原因可能是重装k8s的时候没有删除 /etc/cni/net.d 目录 ","date":"2022-12-03","objectID":"/posts/kubeadm/:1:6","tags":null,"title":"使用 kubeadm 部署 k8s","uri":"/posts/kubeadm/"},{"categories":["kubernetes"],"content":"加入子节点 在每个node节点执行刚刚初始化集群时生成的token $ sudo kubeadm join 192.168.0.111:6443 --token fnq8dx.doxwr7sctdm57p0t \\ --discovery-token-ca-cert-hash sha256:114acfe6e30bc0181a93b9135296af62c5f946fa590691577071a4ebf21fc3ee ","date":"2022-12-03","objectID":"/posts/kubeadm/:1:7","tags":null,"title":"使用 kubeadm 部署 k8s","uri":"/posts/kubeadm/"},{"categories":["kubernetes"],"content":"查看集群健康状况 $ kubectl get cs controller-manager \u0026\u0026 scheduler Unhealthy: dial tcp 127.0.0.1:10252 connection refuse 在master节点执行 $ sudo vi /etc/kubernetes/manifests/kube-scheduler.yaml $ sudo vi /etc/kubernetes/manifests/kube-controller-manager.yaml # 注释 # - --port=0 # 重启kublet $ systemctl restart kubelet ","date":"2022-12-03","objectID":"/posts/kubeadm/:1:8","tags":null,"title":"使用 kubeadm 部署 k8s","uri":"/posts/kubeadm/"},{"categories":["kubernetes"],"content":"将集群导入 Rancher 找一个节点执行下载v2.6 $ sudo docker run --privileged -d --restart=unless-stopped -p 80:80 -p 443:443 rancher/rancher:v2.6-head 根据指引执行命令 ","date":"2022-12-03","objectID":"/posts/kubeadm/:2:0","tags":null,"title":"使用 kubeadm 部署 k8s","uri":"/posts/kubeadm/"}]